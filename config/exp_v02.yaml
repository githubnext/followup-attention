# This folder describes the parameters to run an attention extraction job

# DESCRIPTION
# This experiment iterates over a series of snippets, produces an output and
# saves a subset of the model output:
# - generated text
# - attention matrix (typically condensed on both layer and head dim)
# - metadata (e.g. info on tokenization used, text prompt and generated)

input_data_folder: ./data/prompt/exp_v02
output_data_folder: ./data/model_output/exp_v02
local_model_folder: /mnt/huggingface_models


# if the stop_signal_line is not null it means that, if the stop_signal_line
# is in the prompt, all the lines after that will be ignored
stop_signal_line: "BUG FIX"

models:
  - Salesforce/codegen-16B-mono
#  - Salesforce/codegen-350M-mono


generation:
  # UNUSED number_of_seeds: 1
  number_of_tokens: 100  # use -1 for open-ended generation
  max_time_in_seconds: 60  # use -1 for infinite time
  temperature: 1.0


extract_attention: true

attention:
  # min, max, mean, sum, keep
  strategy_reduce_head: max
  # min, max, mean, sum, keep
  strategy_reduce_layer: max
  # more_weight_to_recent
  strategy_normalize_tokens: more_weight_to_recent

save_attention_matrix: true
# UNUSED number_of_source_code_heatmaps: 2


# this is typically to very large, it would saturate the memory too quickly.
save_raw_model_output: false

