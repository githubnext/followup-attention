{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Goal**\n",
    "Optimize the follow-up attention calculation in matrix form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-08T21:18:41.715902Z",
     "start_time": "2022-07-08T21:18:41.684348Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "from typing import List, Tuple, Any\n",
    "import json\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "import project_path\n",
    "\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "\n",
    "from attwizard.decoder import get_attention_tensor\n",
    "from attwizard.decoder import merge_attention_prompt_and_new_tokens\n",
    "from attwizard.decoder import get_attention_matrix\n",
    "from attwizard.decoder import condense_attention\n",
    "from attwizard.decoder import heatmap_visualize\n",
    "from attwizard.decoder import normalize_less_attention_on_early_tokens\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-08T21:15:56.188747Z",
     "start_time": "2022-07-08T21:15:56.162241Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-08T21:15:56.379662Z",
     "start_time": "2022-07-08T21:15:56.363140Z"
    }
   },
   "outputs": [],
   "source": [
    "MODEL_FOLDER = \"../huggingface_models\"\n",
    "HUGGING_FACE_REPO = \"Salesforce/codegen-350M-mono\"\n",
    "FOLDER_WITH_SAMPLES = \"code_snippet_samples\"\n",
    "OUTPUT_FOLDER = \"tmp\"\n",
    "\n",
    "EXP_NAME = \"exp_v07\"\n",
    "EXP_FOLDER = os.path.join(\"..\", \"data\", \"model_output\", EXP_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query Model (CodeGen 350M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-08T21:15:58.384125Z",
     "start_time": "2022-07-08T21:15:56.446141Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded locally...\n",
      "('CodeGenForCausalLM(\\n'\n",
      " '  (transformer): CodeGenModel(\\n'\n",
      " '    (wte): Embedding(51200, 1024)\\n'\n",
      " '    (drop): Dropout(p=0.0, inplace=False)\\n'\n",
      " '    (h): ModuleList(\\n'\n",
      " '      (0): CodeGenBlock(\\n'\n",
      " '        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\\n'\n",
      " '        (attn): CodeGenAttention(\\n'\n",
      " '          (attn_dropout): Dropout(p=0.0, inplace=False)\\n'\n",
      " '          (resid_dropout): Dropout(p=0.0, inplace=False)\\n'\n",
      " '          (qkv_proj): Linear(in_features=1024, out_features=3072, '\n",
      " 'bias=False)\\n'\n",
      " '          (out_proj): Linear(in')\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from attwizard.models.modeling_codegen import CodeGenForCausalLM\n",
    "from attwizard.script.utils import get_model_folder_path\n",
    "from pprint import pprint\n",
    "\n",
    "model_folder_path = get_model_folder_path(\n",
    "    model_folder=MODEL_FOLDER,\n",
    "    hugging_short_repo=HUGGING_FACE_REPO\n",
    ")\n",
    "\n",
    "if os.path.exists(os.path.join(model_folder_path, \"pytorch_model.bin\")):\n",
    "    print(\"Model loaded locally...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_folder_path)\n",
    "    model = CodeGenForCausalLM.from_pretrained(model_folder_path)\n",
    "    pprint(str(model)[:500])\n",
    "else:\n",
    "    print(\"You must download the model first with: attwizard.script.download_model.py.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-08T21:16:02.903055Z",
     "start_time": "2022-07-08T21:16:02.869714Z"
    }
   },
   "outputs": [],
   "source": [
    "prompt = \"\"\"a = 3\n",
    "b = 5\n",
    "c = a\n",
    "d = 7\n",
    "e = d\n",
    "f = b\n",
    "g = a\n",
    "h = 4 + d\n",
    "i = d\n",
    "j = h\n",
    "print(f) # prints the value '5'\n",
    "print(g) # prints the value '3'\n",
    "print(f) # prints the value '5'\n",
    "print(e) # prints the value '7'\n",
    "print(c) # prints the value '3'\n",
    "print(h) # prints the value '11'\n",
    "print(j) # prints the value '4'\n",
    "print(i) # prints the value '7'\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-08T21:16:03.997530Z",
     "start_time": "2022-07-08T21:16:03.472508Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "tmp = tokenizer(prompt, return_tensors=\"pt\")\n",
    "input_ids = tmp['input_ids']\n",
    "attention_mask = tmp['attention_mask']\n",
    "torch.manual_seed(37)  # 42\n",
    "\n",
    "N_NEW_TOKENS = 20\n",
    "\n",
    "model_output = model.generate(\n",
    "    input_ids,\n",
    "    attention_mask=attention_mask,\n",
    "    do_sample=True,\n",
    "    max_time=3,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    temperature=0.9,\n",
    "    output_attentions=True,\n",
    "    max_length=len(input_ids[0]) + N_NEW_TOKENS, \n",
    "    return_dict_in_generate=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-08T21:16:04.777997Z",
     "start_time": "2022-07-08T21:16:04.757941Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a = 3\n",
      "b = 5\n",
      "c = a\n",
      "d = 7\n",
      "e = d\n",
      "f = b\n",
      "g = a\n",
      "h = 4 + d\n",
      "i = d\n",
      "j = h\n",
      "print(f) # prints the value '5'\n",
      "print(g) # prints the value '3'\n",
      "print(f) # prints the value '5'\n",
      "print(e) # prints the value '7'\n",
      "print(c) # prints the value '3'\n",
      "print(h) # prints the value '11'\n",
      "print(j) # prints the value '4'\n",
      "print(i) # prints the value '7'\n",
      "print(k) # prints the value '1'\n",
      "print(l) # prints the value\n"
     ]
    }
   ],
   "source": [
    "generated_text = tokenizer.decode(model_output[\"sequences\"][0])\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backup Attention Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from attwizard.decoder import get_attention_tensor\n",
    "from attwizard.decoder import merge_attention_prompt_and_new_tokens\n",
    "from attwizard.decoder import get_attention_matrix\n",
    "from attwizard.decoder import condense_attention\n",
    "from attwizard.decoder import heatmap_visualize\n",
    "from attwizard.decoder import normalize_less_attention_on_early_tokens\n",
    "\n",
    "import torch\n",
    "\n",
    "tokens_all_attended = tokenizer.convert_ids_to_tokens(model_output[\"sequences\"][0])\n",
    "tokens_prompt = tokenizer.convert_ids_to_tokens(input_ids[0])\n",
    "tokens_generated = tokens_all_attended[len(tokens_prompt):]\n",
    "\n",
    "att_tensor = get_attention_tensor(\n",
    "    model_output=model_output\n",
    ")\n",
    "att_tensor = normalize_less_attention_on_early_tokens(\n",
    "    att_tensor=att_tensor\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save tensor\n",
    "att_tensor_np = att_tensor.numpy()\n",
    "# Save tensor as npy file\n",
    "with open(os.path.join(OUTPUT_FOLDER, \"raw_attention.npy\"), \"wb\") as f:\n",
    "    np.save(f, att_tensor_np)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Albert Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import typing\n",
    "from typing import ForwardRef\n",
    "def get_follow_up_attention_matrix_v2(\n",
    "        attention_tensor: typing.Union[np.ndarray, ForwardRef('torch.Tensor')],\n",
    "        normalise: bool = True, minus_mean = False):\n",
    "    \"\"\"\n",
    "    Extract the follow-up attention for a layer and head. \n",
    "    Assume that dims are:\n",
    "    0 is irrelevant (sum over it!)\n",
    "    1 is layer\n",
    "    2 isnâ€™t there\n",
    "    3 is head\n",
    "    4 is attended from or to\n",
    "    5 is attended to or from\n",
    "    \"\"\"\n",
    "    assert len(attention_tensor.shape) == 6\n",
    "    print(\"Start: \", attention_tensor.shape)\n",
    "    n_layers = attention_tensor.shape[1]\n",
    "    n_heads = attention_tensor.shape[3]\n",
    "    n_tokens = attention_tensor.shape[4]\n",
    "    assert n_tokens == attention_tensor.shape[5], \"to and from dimension mismatch\"\n",
    "    attention_tensor = attention_tensor.sum(2).sum(0)  # sum over the heads\n",
    "    output = np.zeros((n_tokens, n_tokens, n_layers - 1))\n",
    "    print(\"Condensed start: \", attention_tensor.shape)\n",
    "    # Condensed start:  (20, 16, 157, 157)\n",
    "\n",
    "    print(\"Start output: \", output.shape)\n",
    "    for k_layer in range(n_layers - 1):\n",
    "        print(\"Layer \", k_layer, \")\")\n",
    "        print(\"Select a layer and condense on the head dimension\")\n",
    "        print(\"Before size: \", attention_tensor.shape)\n",
    "        layer_matrix = attention_tensor[k_layer, :, :, :].sum(0)\n",
    "        next_layer_matrix = attention_tensor[k_layer + 1, :, :, :].sum(0)\n",
    "        print(\"After size: \", layer_matrix.shape)\n",
    "        # make it so each recipient has sum of attention = 1\n",
    "\n",
    "        print(\"Make is so that each person (recipient) has sum of attention of 1\")\n",
    "        for i in range(n_tokens):\n",
    "            print(\"layer_matrix[i, :].sum(): \", layer_matrix[i, :].sum())\n",
    "            layer_matrix[i, :] = layer_matrix[i, :] / layer_matrix[i, ].sum()\n",
    "            next_layer_matrix[i, :] = next_layer_matrix[i, :] / next_layer_matrix[i, ].sum()\n",
    "            print(\"layer_matrix[i, :].sum(): \", layer_matrix[i, :].sum())\n",
    "        \n",
    "        # Condensed start:  (20, 16, 157, 157)\n",
    "        for i_first_attended in range(n_tokens):\n",
    "            for j_second_attended in range(n_tokens):\n",
    "                # only tokens after i and j can be compared:\n",
    "                max_i_j = max(i_first_attended, j_second_attended)\n",
    "                # who attended to it in layer k?\n",
    "                attend_to_i_in_layer = attention_tensor[k_layer, :, max_i_j:, i_first_attended].sum(0)\n",
    "                # who attended to it in layer k+1?\n",
    "                attend_to_j_in_next_layer = attention_tensor[k_layer + 1, :, max_i_j:, j_second_attended].sum(0)\n",
    "                if minus_mean:\n",
    "                    attend_to_i_in_layer = attend_to_i_in_layer - torch.mean(attend_to_i_in_layer)\n",
    "                    attend_to_j_in_next_layer = attend_to_j_in_next_layer - torch.mean(attend_to_j_in_next_layer)\n",
    "                if normalise:\n",
    "                    attend_to_i_in_layer = attend_to_i_in_layer / np.linalg.norm(attend_to_i_in_layer)\n",
    "                    attend_to_j_in_next_layer = attend_to_j_in_next_layer / np.linalg.norm(attend_to_j_in_next_layer)\n",
    "                # take the dot product of the two\n",
    "                dotproduct = np.dot(attend_to_i_in_layer, attend_to_j_in_next_layer)\n",
    "                output[i_first_attended, j_second_attended, k_layer] = \\\n",
    "                    0 if np.isnan(dotproduct) else dotproduct\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start:  (20, 20, 1, 16, 157, 157)\n",
      "Condensed start:  (20, 16, 157, 157)\n",
      "Start output:  (157, 157, 19)\n",
      "Layer  0 )\n",
      "Select a layer and condense on the head dimension\n",
      "Before size:  (20, 16, 157, 157)\n",
      "After size:  (157, 157)\n",
      "Make is so that each person (recipient) has sum of attention of 1\n",
      "layer_matrix[i, :].sum():  16.0\n",
      "layer_matrix[i, :].sum():  1.0\n"
     ]
    }
   ],
   "source": [
    "att_matrix_3 = get_follow_up_attention_matrix_v2(att_tensor_np, normalise=True) # best so far"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_matrix_by_lines(att_matrix):\n",
    "    att_matrix[0,:] = 0.0000001\n",
    "    att_matrix[:,0] = 0.0000001\n",
    "    for i in range(att_matrix.shape[0]):\n",
    "        att_matrix[i, :] = att_matrix[i, :] / att_matrix[i, :].sum()\n",
    "    return att_matrix\n",
    "att_matrix_3_processed = normalize_matrix_by_lines(att_matrix_3.sum(2))\n",
    "\n",
    "# Save tensor as npy file\n",
    "with open(os.path.join(OUTPUT_FOLDER, \"attention_v3.npy\"), \"wb\") as f:\n",
    "    np.save(f, att_matrix_3_processed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read (Pre-computed) Attention tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read raw attention\n",
    "with open(os.path.join(OUTPUT_FOLDER, \"raw_attention.npy\"), \"rb\") as f:\n",
    "    att_tensor_np = np.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read attention matrix v3\n",
    "with open(os.path.join(OUTPUT_FOLDER, \"attention_v3.npy\"), \"rb\") as f:\n",
    "    att_matrix_3_processed = np.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 - slice/expand a matrix removing x lines aka [1..n] from the matrix\n",
    "\n",
    "# 2 - normalize every column (divide by the 2-norm) \n",
    "    # compute the 2-norm for each column\n",
    "        # compute the dot product of each column (with itself)\n",
    "        # compute the square root of each result\n",
    "\n",
    "# repeat this for two consecutive layers.\n",
    "\n",
    "# 3 - multiply each matrix slice by the the corresponding slice of the consecutive level matrix\n",
    "# (to get the similarity between token A and token B)\n",
    "# note that this multiplication will create the final results only for the \n",
    "# form token X (line) to token Y (column), where either X or Y is the first not empty line of the matrix slice\n",
    "\n",
    "# 4 - Merge the results for all the matrix multiplications\n",
    "\n",
    "\n",
    "# 5 - generalize in parallel for multiple layers\n",
    "\n",
    "# 6 - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1952, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.6581, 0.4913, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.3278, 0.6532, 0.3958, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.9497, 0.6666, 0.9811, 0.0874, 0.0000, 0.0000, 0.0000],\n",
       "        [0.7025, 0.6790, 0.9155, 0.2418, 0.1591, 0.0000, 0.0000],\n",
       "        [0.8035, 0.3813, 0.7860, 0.1115, 0.2477, 0.6524, 0.0000],\n",
       "        [0.3725, 0.7980, 0.8399, 0.1374, 0.2331, 0.9578, 0.3313]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def crete_random_matrix(size: int):\n",
    "    \"\"\"Create a random attention matrix with the given size.\"\"\"\n",
    "    base = torch.rand(size, size)\n",
    "    mask = torch.tril(torch.ones(size, size))\n",
    "    return base * mask\n",
    "\n",
    "N_TOKENS = 7\n",
    "# set torch seed \n",
    "torch.manual_seed(42)\n",
    "a = crete_random_matrix(N_TOKENS)\n",
    "b = crete_random_matrix(N_TOKENS)\n",
    "b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 1., 1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1., 1.]],\n",
       "\n",
       "        [[0., 0., 0., 0., 0., 0., 0.],\n",
       "         [1., 1., 1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1., 1.]],\n",
       "\n",
       "        [[0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0.],\n",
       "         [1., 1., 1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1., 1.]],\n",
       "\n",
       "        [[0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0.],\n",
       "         [1., 1., 1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1., 1.]],\n",
       "\n",
       "        [[0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0.],\n",
       "         [1., 1., 1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1., 1.]],\n",
       "\n",
       "        [[0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0.],\n",
       "         [1., 1., 1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1., 1.]],\n",
       "\n",
       "        [[0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0.],\n",
       "         [1., 1., 1., 1., 1., 1., 1.]]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1 - slice/expand a matrix removing x lines aka [1..n] from the matrix\n",
    "slicer_mat_rows = torch.triu(torch.ones(N_TOKENS, N_TOKENS))\n",
    "# add dimension\n",
    "slicer_mat_rows = slicer_mat_rows.unsqueeze(2)\n",
    "slicer_mat_rows = slicer_mat_rows.expand(-1, -1, N_TOKENS)\n",
    "slicer_mat_rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 0., 0., 0., 0., 0., 0.],\n",
       "         [1., 0., 0., 0., 0., 0., 0.],\n",
       "         [1., 0., 0., 0., 0., 0., 0.],\n",
       "         [1., 0., 0., 0., 0., 0., 0.],\n",
       "         [1., 0., 0., 0., 0., 0., 0.],\n",
       "         [1., 0., 0., 0., 0., 0., 0.],\n",
       "         [1., 0., 0., 0., 0., 0., 0.]],\n",
       "\n",
       "        [[1., 1., 0., 0., 0., 0., 0.],\n",
       "         [1., 1., 0., 0., 0., 0., 0.],\n",
       "         [1., 1., 0., 0., 0., 0., 0.],\n",
       "         [1., 1., 0., 0., 0., 0., 0.],\n",
       "         [1., 1., 0., 0., 0., 0., 0.],\n",
       "         [1., 1., 0., 0., 0., 0., 0.],\n",
       "         [1., 1., 0., 0., 0., 0., 0.]],\n",
       "\n",
       "        [[1., 1., 1., 0., 0., 0., 0.],\n",
       "         [1., 1., 1., 0., 0., 0., 0.],\n",
       "         [1., 1., 1., 0., 0., 0., 0.],\n",
       "         [1., 1., 1., 0., 0., 0., 0.],\n",
       "         [1., 1., 1., 0., 0., 0., 0.],\n",
       "         [1., 1., 1., 0., 0., 0., 0.],\n",
       "         [1., 1., 1., 0., 0., 0., 0.]],\n",
       "\n",
       "        [[1., 1., 1., 1., 0., 0., 0.],\n",
       "         [1., 1., 1., 1., 0., 0., 0.],\n",
       "         [1., 1., 1., 1., 0., 0., 0.],\n",
       "         [1., 1., 1., 1., 0., 0., 0.],\n",
       "         [1., 1., 1., 1., 0., 0., 0.],\n",
       "         [1., 1., 1., 1., 0., 0., 0.],\n",
       "         [1., 1., 1., 1., 0., 0., 0.]],\n",
       "\n",
       "        [[1., 1., 1., 1., 1., 0., 0.],\n",
       "         [1., 1., 1., 1., 1., 0., 0.],\n",
       "         [1., 1., 1., 1., 1., 0., 0.],\n",
       "         [1., 1., 1., 1., 1., 0., 0.],\n",
       "         [1., 1., 1., 1., 1., 0., 0.],\n",
       "         [1., 1., 1., 1., 1., 0., 0.],\n",
       "         [1., 1., 1., 1., 1., 0., 0.]],\n",
       "\n",
       "        [[1., 1., 1., 1., 1., 1., 0.],\n",
       "         [1., 1., 1., 1., 1., 1., 0.],\n",
       "         [1., 1., 1., 1., 1., 1., 0.],\n",
       "         [1., 1., 1., 1., 1., 1., 0.],\n",
       "         [1., 1., 1., 1., 1., 1., 0.],\n",
       "         [1., 1., 1., 1., 1., 1., 0.],\n",
       "         [1., 1., 1., 1., 1., 1., 0.]],\n",
       "\n",
       "        [[1., 1., 1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1., 1.]]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slicer_mat_col = torch.transpose(slicer_mat_rows, 2, 1)\n",
    "slicer_mat_col = torch.flip(slicer_mat_col, [2])\n",
    "slicer_mat_col = torch.flip(slicer_mat_col, [0])\n",
    "slicer_mat_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.8823, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.7936, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.7411, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.4414, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.5472, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.8090, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.7104, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
       "\n",
       "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.7936, 0.9408, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.7411, 0.4294, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.4414, 0.2969, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.5472, 0.0062, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.8090, 0.5779, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.7104, 0.9464, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
       "\n",
       "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.7411, 0.4294, 0.8854, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.4414, 0.2969, 0.8317, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.5472, 0.0062, 0.9516, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.8090, 0.5779, 0.9040, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.7104, 0.9464, 0.7890, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
       "\n",
       "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.4414, 0.2969, 0.8317, 0.1053, 0.0000, 0.0000, 0.0000],\n",
       "         [0.5472, 0.0062, 0.9516, 0.0753, 0.0000, 0.0000, 0.0000],\n",
       "         [0.8090, 0.5779, 0.9040, 0.5547, 0.0000, 0.0000, 0.0000],\n",
       "         [0.7104, 0.9464, 0.7890, 0.2814, 0.0000, 0.0000, 0.0000]],\n",
       "\n",
       "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.5472, 0.0062, 0.9516, 0.0753, 0.8860, 0.0000, 0.0000],\n",
       "         [0.8090, 0.5779, 0.9040, 0.5547, 0.3423, 0.0000, 0.0000],\n",
       "         [0.7104, 0.9464, 0.7890, 0.2814, 0.7886, 0.0000, 0.0000]],\n",
       "\n",
       "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.8090, 0.5779, 0.9040, 0.5547, 0.3423, 0.6343, 0.0000],\n",
       "         [0.7104, 0.9464, 0.7890, 0.2814, 0.7886, 0.5895, 0.0000]],\n",
       "\n",
       "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.7104, 0.9464, 0.7890, 0.2814, 0.7886, 0.5895, 0.7539]]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# replicate existing base matrix\n",
    "a_stacked = a.unsqueeze(0)\n",
    "a_stacked = a_stacked.expand(N_TOKENS, -1, -1)\n",
    "a_stacked\n",
    "# filter out previous rows\n",
    "a_stacked_filtered = a_stacked\n",
    "a_stacked_filtered = a_stacked_filtered * slicer_mat_rows\n",
    "a_stacked_filtered = a_stacked_filtered * slicer_mat_col\n",
    "a_stacked_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.9002353591068661"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v = [0.8823, 0.7936, 0.7411, 0.4414, 0.5472, 0.8090, 0.7104]\n",
    "np.linalg.norm(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([7, 7, 7])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_stacked_filtered.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1.9002, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [1.9002, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [1.9002, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [1.9002, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [1.9002, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [1.9002, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [1.9002, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
       "\n",
       "        [[1.6830, 1.5451, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [1.6830, 1.5451, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [1.6830, 1.5451, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [1.6830, 1.5451, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [1.6830, 1.5451, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [1.6830, 1.5451, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [1.6830, 1.5451, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
       "\n",
       "        [[1.4841, 1.2257, 1.9547, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [1.4841, 1.2257, 1.9547, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [1.4841, 1.2257, 1.9547, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [1.4841, 1.2257, 1.9547, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [1.4841, 1.2257, 1.9547, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [1.4841, 1.2257, 1.9547, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [1.4841, 1.2257, 1.9547, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
       "\n",
       "        [[1.2858, 1.1480, 1.7427, 0.6353, 0.0000, 0.0000, 0.0000],\n",
       "         [1.2858, 1.1480, 1.7427, 0.6353, 0.0000, 0.0000, 0.0000],\n",
       "         [1.2858, 1.1480, 1.7427, 0.6353, 0.0000, 0.0000, 0.0000],\n",
       "         [1.2858, 1.1480, 1.7427, 0.6353, 0.0000, 0.0000, 0.0000],\n",
       "         [1.2858, 1.1480, 1.7427, 0.6353, 0.0000, 0.0000, 0.0000],\n",
       "         [1.2858, 1.1480, 1.7427, 0.6353, 0.0000, 0.0000, 0.0000],\n",
       "         [1.2858, 1.1480, 1.7427, 0.6353, 0.0000, 0.0000, 0.0000]],\n",
       "\n",
       "        [[1.2077, 1.1089, 1.5314, 0.6265, 1.2346, 0.0000, 0.0000],\n",
       "         [1.2077, 1.1089, 1.5314, 0.6265, 1.2346, 0.0000, 0.0000],\n",
       "         [1.2077, 1.1089, 1.5314, 0.6265, 1.2346, 0.0000, 0.0000],\n",
       "         [1.2077, 1.1089, 1.5314, 0.6265, 1.2346, 0.0000, 0.0000],\n",
       "         [1.2077, 1.1089, 1.5314, 0.6265, 1.2346, 0.0000, 0.0000],\n",
       "         [1.2077, 1.1089, 1.5314, 0.6265, 1.2346, 0.0000, 0.0000],\n",
       "         [1.2077, 1.1089, 1.5314, 0.6265, 1.2346, 0.0000, 0.0000]],\n",
       "\n",
       "        [[1.0766, 1.1089, 1.1999, 0.6220, 0.8597, 0.8659, 0.0000],\n",
       "         [1.0766, 1.1089, 1.1999, 0.6220, 0.8597, 0.8659, 0.0000],\n",
       "         [1.0766, 1.1089, 1.1999, 0.6220, 0.8597, 0.8659, 0.0000],\n",
       "         [1.0766, 1.1089, 1.1999, 0.6220, 0.8597, 0.8659, 0.0000],\n",
       "         [1.0766, 1.1089, 1.1999, 0.6220, 0.8597, 0.8659, 0.0000],\n",
       "         [1.0766, 1.1089, 1.1999, 0.6220, 0.8597, 0.8659, 0.0000],\n",
       "         [1.0766, 1.1089, 1.1999, 0.6220, 0.8597, 0.8659, 0.0000]],\n",
       "\n",
       "        [[0.7104, 0.9464, 0.7890, 0.2814, 0.7886, 0.5895, 0.7539],\n",
       "         [0.7104, 0.9464, 0.7890, 0.2814, 0.7886, 0.5895, 0.7539],\n",
       "         [0.7104, 0.9464, 0.7890, 0.2814, 0.7886, 0.5895, 0.7539],\n",
       "         [0.7104, 0.9464, 0.7890, 0.2814, 0.7886, 0.5895, 0.7539],\n",
       "         [0.7104, 0.9464, 0.7890, 0.2814, 0.7886, 0.5895, 0.7539],\n",
       "         [0.7104, 0.9464, 0.7890, 0.2814, 0.7886, 0.5895, 0.7539],\n",
       "         [0.7104, 0.9464, 0.7890, 0.2814, 0.7886, 0.5895, 0.7539]]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# normalize each column with euclidean norm\n",
    "a_stacked_norm = torch.norm(a_stacked_filtered, dim=1)\n",
    "a_stacked_norm = a_stacked_norm.unsqueeze(1)\n",
    "a_stacked_norm = a_stacked_norm.expand(-1, N_TOKENS, -1)\n",
    "a_stacked_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.4643, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.4177, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.3900, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.2323, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.2880, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.4257, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.3739, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
       "\n",
       "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.4716, 0.6089, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.4403, 0.2779, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.2622, 0.1922, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.3251, 0.0040, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.4807, 0.3740, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.4221, 0.6125, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
       "\n",
       "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.4994, 0.3503, 0.4530, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.2974, 0.2423, 0.4255, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.3687, 0.0050, 0.4868, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.5451, 0.4715, 0.4625, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.4787, 0.7722, 0.4037, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
       "\n",
       "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.3433, 0.2586, 0.4772, 0.1658, 0.0000, 0.0000, 0.0000],\n",
       "         [0.4256, 0.0054, 0.5460, 0.1185, 0.0000, 0.0000, 0.0000],\n",
       "         [0.6291, 0.5034, 0.5187, 0.8731, 0.0000, 0.0000, 0.0000],\n",
       "         [0.5525, 0.8244, 0.4528, 0.4430, 0.0000, 0.0000, 0.0000]],\n",
       "\n",
       "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.4531, 0.0056, 0.6214, 0.1201, 0.7177, 0.0000, 0.0000],\n",
       "         [0.6698, 0.5212, 0.5903, 0.8853, 0.2773, 0.0000, 0.0000],\n",
       "         [0.5882, 0.8534, 0.5152, 0.4492, 0.6388, 0.0000, 0.0000]],\n",
       "\n",
       "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.7514, 0.5212, 0.7534, 0.8918, 0.3982, 0.7325, 0.0000],\n",
       "         [0.6599, 0.8535, 0.6576, 0.4525, 0.9173, 0.6807, 0.0000]],\n",
       "\n",
       "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]]])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Normalize the each value \n",
    "a_stacked_normalized = a_stacked_filtered / a_stacked_norm\n",
    "# replace nan\n",
    "a_stacked_normalized = torch.nan_to_num(a_stacked_normalized, nan=0.0)\n",
    "\n",
    "a_stacked_normalized\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_and_normalized(a: torch.Tensor):\n",
    "    \"\"\"Expand the N_tokens x N_tokens matrix for the number of tokens.\n",
    "    \n",
    "    Expand a matrix adding a new dimension N_tokens and remove\n",
    "    - x lines from the top and \n",
    "    - x columns from the bottom, \n",
    "    so to leave only a rectangular non-zero section.\n",
    "\n",
    "    Then normalize these rectangular section in a column wise fashion.\n",
    "    \"\"\"\n",
    "    n_tokens = a.shape[0]\n",
    "    # mask on rows\n",
    "    slicer_mat_rows = torch.triu(torch.ones(n_tokens, n_tokens))\n",
    "    slicer_mat_rows = slicer_mat_rows.unsqueeze(2)\n",
    "    slicer_mat_rows = slicer_mat_rows.expand(-1, -1, n_tokens)\n",
    "    # mask on columns\n",
    "    slicer_mat_col = torch.transpose(slicer_mat_rows, 2, 1)\n",
    "    slicer_mat_col = torch.flip(slicer_mat_col, [2])\n",
    "    slicer_mat_col = torch.flip(slicer_mat_col, [0])\n",
    "    # replicate existing base matrix\n",
    "    a_stacked = a.unsqueeze(0)\n",
    "    a_stacked = a_stacked.expand(n_tokens, -1, -1)\n",
    "    # keep only rectangular sections\n",
    "    a_stacked_filtered = a_stacked\n",
    "    a_stacked_filtered = a_stacked_filtered * slicer_mat_rows\n",
    "    a_stacked_filtered = a_stacked_filtered * slicer_mat_col\n",
    "    # normalize each column with euclidean norm\n",
    "    normalization_coeff = torch.norm(a_stacked_filtered, dim=1)\n",
    "    normalization_coeff = normalization_coeff.unsqueeze(1)\n",
    "    normalization_coeff = normalization_coeff.expand(-1, n_tokens, -1)\n",
    "    # Normalize the each value \n",
    "    a_stacked_normalized = a_stacked_filtered / normalization_coeff\n",
    "    # replace nan\n",
    "    a_stacked_normalized = torch.nan_to_num(a_stacked_normalized, nan=0.0)\n",
    "    return a_stacked_normalized\n",
    "\n",
    "current_level_stacked = expand_and_normalized(a)\n",
    "next_level_stacked = expand_and_normalized(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_from_a_to_b(\n",
    "        current_level_stacked: torch.Tensor,\n",
    "        next_level_stacked: torch.Tensor,):\n",
    "    \"\"\"Compute a token -> token matrix for consecutive layers.\"\"\"\n",
    "    n_tokens = current_level_stacked.shape[0]\n",
    "    assert current_level_stacked.shape[0] == next_level_stacked.shape[0], \"the two inputs must have the same dimensions\"\n",
    "    # generalized multiplication to compute the dot products in parallel\n",
    "    # this compares the number of followers of a token pairs in two \n",
    "    # consecutive layers\n",
    "    res = torch.einsum(\n",
    "        'bji,bjk->bik', \n",
    "        current_level_stacked, next_level_stacked)\n",
    "    # create mask to remove the extra values in the upper left section\n",
    "    # this is needed since the complete list of followers of a token \n",
    "    # are exactly all those tokens that follows in the sequence.\n",
    "    # Note that his sequence is decided by the token of the pair which comes\n",
    "    # last in the pair (aka we compare the largest set of followers which had\n",
    "    # the possibility to follow both tokens in the pairs).\n",
    "    slicer_mat_rows = torch.tril(torch.ones(n_tokens + 1, n_tokens + 1))[:-1, 1:]\n",
    "    slicer_mat_rows = slicer_mat_rows.unsqueeze(2)\n",
    "    slicer_mat_rows = slicer_mat_rows.expand(-1, -1, n_tokens)\n",
    "    transposed = torch.transpose(slicer_mat_rows, 2, 1)\n",
    "    mask_keep_only_last_slide = 1 - (slicer_mat_rows * transposed)\n",
    "    mask_keep_only_last_slide\n",
    "    res = res * mask_keep_only_last_slide\n",
    "    # condense the stacked version\n",
    "    return res.sum(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_TOKEN_POS = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.3433, 0.2586, 0.4772, 0.1658, 0.0000, 0.0000, 0.0000],\n",
       "        [0.4256, 0.0054, 0.5460, 0.1185, 0.0000, 0.0000, 0.0000],\n",
       "        [0.6291, 0.5034, 0.5187, 0.8731, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5525, 0.8244, 0.4528, 0.4430, 0.0000, 0.0000, 0.0000]])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_level_slice = current_level_stacked[TARGET_TOKEN_POS]\n",
    "current_level_slice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.7386, 0.5807, 0.5630, 0.1375, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5464, 0.5915, 0.5253, 0.3806, 0.0000, 0.0000, 0.0000],\n",
       "        [0.6249, 0.3322, 0.4510, 0.1755, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2897, 0.6952, 0.4820, 0.2163, 0.0000, 0.0000, 0.0000]])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_level_slice = next_level_stacked[TARGET_TOKEN_POS]\n",
    "next_level_slice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0000, 0.0000, 0.0000, 0.1658, 0.1185, 0.8731, 0.4430])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.1375, 0.3806, 0.1755, 0.2163])\n"
     ]
    }
   ],
   "source": [
    "d_current = current_level_slice[:, TARGET_TOKEN_POS]\n",
    "pprint(d_current)\n",
    "d_next = next_level_slice[:, TARGET_TOKEN_POS]\n",
    "pprint(d_next)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.3170)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.dot(d_current, d_next)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.8611, 0.7643, 0.7629, 0.3170, 0.0000, 0.0000, 0.0000])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.matmul(d_current, next_level_slice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.4391, 0.3043, 0.4624, 0.3170, 0.0000, 0.0000, 0.0000])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.matmul(d_next, current_level_slice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0392, 1.0441, 0.9669, 0.4391, 0.0000, 0.0000, 0.0000],\n",
       "        [0.7474, 0.8937, 0.7728, 0.3043, 0.0000, 0.0000, 0.0000],\n",
       "        [1.1061, 1.0872, 1.0077, 0.4624, 0.0000, 0.0000, 0.0000],\n",
       "        [0.8611, 0.7643, 0.7629, 0.3170, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.matmul(current_level_slice.t(), next_level_slice)\n",
    "# the only valid data are those on the edge of the section, thus \n",
    "# the last non-zero row \n",
    "# and the last non-zero column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.4643, 0.0000, 0.0000],\n",
       "         [0.4177, 0.0000, 0.0000],\n",
       "         [0.3900, 0.0000, 0.0000]],\n",
       "\n",
       "        [[0.0000, 0.0000, 0.0000],\n",
       "         [0.4716, 0.6089, 0.0000],\n",
       "         [0.4403, 0.2779, 0.0000]],\n",
       "\n",
       "        [[0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000],\n",
       "         [0.4994, 0.3503, 0.4530]]])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_level_stacked[:3, :3, :3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 0.0000, 0.0000, 0.4391, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.3043, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.4624, 0.0000, 0.0000, 0.0000],\n",
       "        [0.8611, 0.7643, 0.7629, 0.3170, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]])"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generalize for all the tokens\n",
    "\n",
    "# create mask to remove the extra values in the upper left section\n",
    "slicer_mat_rows = torch.tril(torch.ones(N_TOKENS + 1, N_TOKENS + 1))[:-1, 1:]\n",
    "slicer_mat_rows = slicer_mat_rows.unsqueeze(2)\n",
    "slicer_mat_rows = slicer_mat_rows.expand(-1, -1, N_TOKENS)\n",
    "transposed = torch.transpose(slicer_mat_rows, 2, 1)\n",
    "mask_keep_only_last_slide = 1 - (slicer_mat_rows * transposed)\n",
    "mask_keep_only_last_slide\n",
    "\n",
    "res = torch.einsum(\n",
    "    'bji,bjk->bik', \n",
    "    current_level_stacked, next_level_stacked)\n",
    "res = res * mask_keep_only_last_slide\n",
    "res[TARGET_TOKEN_POS, :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.7355, 0.9288, 0.8479, 0.4391, 0.3038, 1.2960, 0.4394],\n",
       "        [0.7165, 0.8044, 0.7163, 0.3043, 0.2664, 1.3367, 0.4394],\n",
       "        [0.9544, 1.1492, 0.8927, 0.4624, 0.2958, 1.2950, 0.4394],\n",
       "        [0.8611, 0.7643, 0.7629, 0.3170, 0.2779, 1.1724, 0.4394],\n",
       "        [0.7990, 0.9945, 0.9217, 0.4664, 0.2687, 1.3146, 0.4394],\n",
       "        [0.7822, 0.7418, 0.9564, 0.2817, 0.3956, 1.3049, 0.4394],\n",
       "        [0.5244, 0.8432, 1.0645, 0.4883, 0.2955, 1.6249, 0.4394]])"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.sum(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.7355, 0.9288, 0.8479, 0.4391, 0.3038, 1.2960, 0.4394],\n",
       "        [0.7165, 0.8044, 0.7163, 0.3043, 0.2664, 1.3367, 0.4394],\n",
       "        [0.9544, 1.1492, 0.8927, 0.4624, 0.2958, 1.2950, 0.4394],\n",
       "        [0.8611, 0.7643, 0.7629, 0.3170, 0.2779, 1.1724, 0.4394],\n",
       "        [0.7990, 0.9945, 0.9217, 0.4664, 0.2687, 1.3146, 0.4394],\n",
       "        [0.7822, 0.7418, 0.9564, 0.2817, 0.3956, 1.3049, 0.4394],\n",
       "        [0.5244, 0.8432, 1.0645, 0.4883, 0.2955, 1.6249, 0.4394]])"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_from_a_to_b(\n",
    "    current_level_stacked, next_level_stacked)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_matrix_by_lines(att_matrix):\n",
    "    att_matrix[0,:] = 0.0000001\n",
    "    att_matrix[:,0] = 0.0000001\n",
    "    for i in range(att_matrix.shape[0]):\n",
    "        att_matrix[i, :] = att_matrix[i, :] / att_matrix[i, :].sum()\n",
    "    return att_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 157, 157])"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "INPUT = torch.tensor(att_tensor_np)\n",
    "INPUT = INPUT.sum(2).sum(0)\n",
    "# sum all heads\n",
    "INPUT = INPUT.sum(1)\n",
    "INPUT.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:01<00:00, 18.52it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([19, 157, 157])"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "# consider each pair of two layers\n",
    "\n",
    "all_layers_results = []\n",
    "\n",
    "n_layers = INPUT.shape[0]\n",
    "for i in tqdm(range(n_layers-1)):\n",
    "    c_layer = INPUT[i]\n",
    "    n_layer = INPUT[i + 1]\n",
    "\n",
    "    current_level_stacked = expand_and_normalized(c_layer)\n",
    "    next_level_stacked = expand_and_normalized(n_layer)\n",
    "\n",
    "    res = compute_from_a_to_b(\n",
    "        current_level_stacked, next_level_stacked)\n",
    "    \n",
    "    all_layers_results.append(res)\n",
    "\n",
    "# stack results\n",
    "all_layers_results = torch.stack(all_layers_results, dim=0)\n",
    "all_layers_results.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([157, 157])"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "OUTPUT = all_layers_results.sum(0)\n",
    "OUTPUT =  normalize_matrix_by_lines(OUTPUT)\n",
    "OUTPUT.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(OUTPUT.numpy(), att_matrix_3_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(157, 157)"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "att_matrix_3_processed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff = OUTPUT.numpy() - att_matrix_3_processed"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": "20",
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "89aeb5387409b83552677e4f72611a4d419b4f3ee53322ab9dceb8e490a62be5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
