{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import project_path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Goal**: convert from a token mapping to a different one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'i': 0, 't': 'a'},\n",
      " {'i': 1, 't': 'b'},\n",
      " {'i': 2, 't': ' '},\n",
      " {'i': 3, 't': 'd'},\n",
      " {'i': 4, 't': 'e'},\n",
      " {'i': 5, 't': ' '},\n",
      " {'i': 6, 't': 'g'},\n",
      " {'i': 7, 't': 'h'},\n",
      " {'i': 8, 't': '.'},\n",
      " {'i': 9, 't': '\\n'},\n",
      " {'i': 10, 't': ' '},\n",
      " {'i': 11, 't': 'a'},\n",
      " {'i': 12, 't': ' '},\n",
      " {'i': 13, 't': 'b'},\n",
      " {'i': 14, 't': 'c'},\n",
      " {'i': 15, 't': '.'}]\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"ab de gh.\\n a bc.\"\"\"\n",
    "\n",
    "char_tokenization = [\n",
    "    {\n",
    "        \"t\": t,\n",
    "        \"i\": i,\n",
    "    }\n",
    "    for i, t in enumerate(list(text))\n",
    "]\n",
    "from pprint import pprint\n",
    "\n",
    "pprint(char_tokenization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok_char = list(text)\n",
    "tok_space = [\"ab\", \" \", \"de\", \" \", \"gh\", \".\", \"\\n\", \" \", \"a\", \" \", \"bc\", \".\"]\n",
    "tok_1 = [\"a\", \"b \", \"d\", \"e\", \" \", \"gh\", \".\", \"\\n\", \" \", \"a\", \" \", \"bc\", \".\"]\n",
    "tok_2 = [\"ab de\", \" \", \"gh.\\n\", \" \", \"a bc.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', 'b ', 'd', 'e', ' ', 'gh', '.', '\\n', ' ', 'a', ' ', 'bc', '.']"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok_2_solution = [\n",
    "    {'i': 0, 't': 'ab de', 'c': 0, 'l': 0, \"s\": 0},\n",
    "    {'i': 1, 't': ' ', 'c': 5, 'l': 0, \"s\": 5},\n",
    "    {'i': 2, 't': 'gh.\\n', 'c': 6, 'l': 0, \"s\": 6},\n",
    "    {'i': 3, 't': ' ', 'c': 0, 'l': 1, \"s\": 10},\n",
    "    {'i': 4, 't': 'a bc.', 'c': 1, 'l': 1, \"s\": 11},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Tuple, Any\n",
    "\n",
    "def get_tokens_with_col_and_line(text, tokens: List[str]):\n",
    "    \"\"\"Get tokens with column and line number.\n",
    "    \n",
    "    This function assumes that the concatenation of the tokens matches the text\n",
    "    as prefix. \n",
    "    \n",
    "    Args:\n",
    "        text (str): The text to tokenize.\n",
    "        tokens (list): The tokens to add column and line number to.\n",
    "\n",
    "    Returns:\n",
    "        list of dict with the following keys:\n",
    "            - i: The index of the token in the list of tokens.\n",
    "            - t: The token text.\n",
    "            - c: The column number of the token.\n",
    "            - l: The line number of the token.\n",
    "            - s: The start index of the token in the text.\n",
    "    \"\"\"\n",
    "    assert text.startswith(\"\".join(tokens))\n",
    "    tokens_with_col_and_line = []\n",
    "    line = 0\n",
    "    column = 0\n",
    "    tot_prefix_len = 0\n",
    "    for i, token in enumerate(tokens):\n",
    "        new_token =  {}\n",
    "        # Check if the token matches the text as prefix.\n",
    "        if text.startswith(token):\n",
    "            n_new_lines_char = token.count(\"\\n\")\n",
    "\n",
    "            new_token[\"s\"] = tot_prefix_len\n",
    "            tot_prefix_len += len(token)\n",
    "            new_token[\"i\"] = i\n",
    "            # Add the column and line number to the token.\n",
    "            new_token[\"c\"] = column\n",
    "            new_token[\"l\"] = line\n",
    "            # Update the column and line number.\n",
    "            column += len(token)\n",
    "            new_token[\"t\"] = token\n",
    "            line += n_new_lines_char\n",
    "            if n_new_lines_char > 0:\n",
    "                n_chars_after_last_new_line = \\\n",
    "                    len(new_token[\"t\"]) - (new_token[\"t\"].rfind(\"\\n\") + 1)\n",
    "                column = n_chars_after_last_new_line \n",
    "            # Add the token to the list of tokens with column and line number.\n",
    "            tokens_with_col_and_line.append(new_token)\n",
    "            # remove prefix from text\n",
    "            text = text[len(token):]\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"Token {token} does not match text {text}\"\n",
    "            )\n",
    "    return tokens_with_col_and_line\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'s': 0, 'i': 0, 'c': 0, 'l': 0, 't': 'a'},\n",
       " {'s': 1, 'i': 1, 'c': 1, 'l': 0, 't': 'b'},\n",
       " {'s': 2, 'i': 2, 'c': 2, 'l': 0, 't': ' '},\n",
       " {'s': 3, 'i': 3, 'c': 3, 'l': 0, 't': 'd'},\n",
       " {'s': 4, 'i': 4, 'c': 4, 'l': 0, 't': 'e'},\n",
       " {'s': 5, 'i': 5, 'c': 5, 'l': 0, 't': ' '},\n",
       " {'s': 6, 'i': 6, 'c': 6, 'l': 0, 't': 'g'},\n",
       " {'s': 7, 'i': 7, 'c': 7, 'l': 0, 't': 'h'},\n",
       " {'s': 8, 'i': 8, 'c': 8, 'l': 0, 't': '.'},\n",
       " {'s': 9, 'i': 9, 'c': 9, 'l': 0, 't': '\\n'},\n",
       " {'s': 10, 'i': 10, 'c': 0, 'l': 1, 't': ' '},\n",
       " {'s': 11, 'i': 11, 'c': 1, 'l': 1, 't': 'a'},\n",
       " {'s': 12, 'i': 12, 'c': 2, 'l': 1, 't': ' '},\n",
       " {'s': 13, 'i': 13, 'c': 3, 'l': 1, 't': 'b'},\n",
       " {'s': 14, 'i': 14, 'c': 4, 'l': 1, 't': 'c'},\n",
       " {'s': 15, 'i': 15, 'c': 5, 'l': 1, 't': '.'}]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_tokens_with_col_and_line(text, tok_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'s': 0, 'i': 0, 'c': 0, 'l': 0, 't': 'ab'},\n",
       " {'s': 2, 'i': 1, 'c': 2, 'l': 0, 't': ' '},\n",
       " {'s': 3, 'i': 2, 'c': 3, 'l': 0, 't': 'de'},\n",
       " {'s': 5, 'i': 3, 'c': 5, 'l': 0, 't': ' '},\n",
       " {'s': 6, 'i': 4, 'c': 6, 'l': 0, 't': 'gh'},\n",
       " {'s': 8, 'i': 5, 'c': 8, 'l': 0, 't': '.'},\n",
       " {'s': 9, 'i': 6, 'c': 9, 'l': 0, 't': '\\n'},\n",
       " {'s': 10, 'i': 7, 'c': 0, 'l': 1, 't': ' '},\n",
       " {'s': 11, 'i': 8, 'c': 1, 'l': 1, 't': 'a'},\n",
       " {'s': 12, 'i': 9, 'c': 2, 'l': 1, 't': ' '},\n",
       " {'s': 13, 'i': 10, 'c': 3, 'l': 1, 't': 'bc'},\n",
       " {'s': 15, 'i': 11, 'c': 5, 'l': 1, 't': '.'}]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_tokens_with_col_and_line(text, tok_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'s': 0, 'i': 0, 'c': 0, 'l': 0, 't': 'a'},\n",
       " {'s': 1, 'i': 1, 'c': 1, 'l': 0, 't': 'b '},\n",
       " {'s': 3, 'i': 2, 'c': 3, 'l': 0, 't': 'd'},\n",
       " {'s': 4, 'i': 3, 'c': 4, 'l': 0, 't': 'e'},\n",
       " {'s': 5, 'i': 4, 'c': 5, 'l': 0, 't': ' '},\n",
       " {'s': 6, 'i': 5, 'c': 6, 'l': 0, 't': 'gh'},\n",
       " {'s': 8, 'i': 6, 'c': 8, 'l': 0, 't': '.'},\n",
       " {'s': 9, 'i': 7, 'c': 9, 'l': 0, 't': '\\n'},\n",
       " {'s': 10, 'i': 8, 'c': 0, 'l': 1, 't': ' '},\n",
       " {'s': 11, 'i': 9, 'c': 1, 'l': 1, 't': 'a'},\n",
       " {'s': 12, 'i': 10, 'c': 2, 'l': 1, 't': ' '},\n",
       " {'s': 13, 'i': 11, 'c': 3, 'l': 1, 't': 'bc'},\n",
       " {'s': 15, 'i': 12, 'c': 5, 'l': 1, 't': '.'}]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_tokens_with_col_and_line(text, tok_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'s': 0, 'i': 0, 'c': 0, 'l': 0, 't': 'ab de'},\n",
       " {'s': 5, 'i': 1, 'c': 5, 'l': 0, 't': ' '},\n",
       " {'s': 6, 'i': 2, 'c': 6, 'l': 0, 't': 'gh.\\n'},\n",
       " {'s': 10, 'i': 3, 'c': 0, 'l': 1, 't': ' '},\n",
       " {'s': 11, 'i': 4, 'c': 1, 'l': 1, 't': 'a bc.'}]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_tokens_with_col_and_line(text, tok_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert tok_2_solution == get_tokens_with_col_and_line(text, tok_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_weigths_form_tok_to_tok(tokenization, weights, target_tokenization):\n",
    "    \"\"\"Convert the weights to the target tokenization.\"\"\"\n",
    "    weights_tok_char = convert_weights_from_tok_to_tok_char(tokenization, weights)\n",
    "    print(\"Intermediate char weights\")\n",
    "    pprint(weights_tok_char)\n",
    "    weights_tok = convert_weights_from_tok_char_to_tok(\n",
    "        weights_tok_char, target_tokenization\n",
    "    )\n",
    "    return weights_tok\n",
    "\n",
    "def convert_weights_from_tok_to_tok_char(tokenization, weights):\n",
    "    \"\"\"Convert the weights to the char tokenization.\n",
    "    \n",
    "    Note that the weight of a char is derived by the weight of a token divided\n",
    "    by the number of chars in the token.\n",
    "    \"\"\"\n",
    "    char_weights = []\n",
    "    for w, t in zip(weights, tokenization):\n",
    "        new_weight = w / len(t[\"t\"])\n",
    "        char_weights.extend([new_weight] * len(t[\"t\"]))\n",
    "    return char_weights\n",
    "\n",
    "def convert_weights_from_tok_char_to_tok(weights, target_tokenization):\n",
    "    \"\"\"Convert the weights to the tokenization.\n",
    "    \n",
    "    Note that the weight of a token is derived by the sum of the weights of the\n",
    "    chars in the token.\n",
    "    \"\"\"\n",
    "    token_weights = []\n",
    "    for t in target_tokenization:\n",
    "        new_weight = sum(weights[t[\"s\"]:t[\"s\"] + len(t[\"t\"])])\n",
    "        print(f\"Token {t['t']} has weight {new_weight}\")\n",
    "        token_weights.append(new_weight)\n",
    "    return token_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'c': 0, 'i': 0, 'l': 0, 's': 0, 't': 'ab de'},\n",
      " {'c': 5, 'i': 1, 'l': 0, 's': 5, 't': ' '},\n",
      " {'c': 6, 'i': 2, 'l': 0, 's': 6, 't': 'gh.\\n'},\n",
      " {'c': 0, 'i': 3, 'l': 1, 's': 10, 't': ' '},\n",
      " {'c': 1, 'i': 4, 'l': 1, 's': 11, 't': 'a bc.'}]\n",
      "'Target tokenization 1'\n",
      "[{'c': 0, 'i': 0, 'l': 0, 's': 0, 't': 'a'},\n",
      " {'c': 1, 'i': 1, 'l': 0, 's': 1, 't': 'b '},\n",
      " {'c': 3, 'i': 2, 'l': 0, 's': 3, 't': 'd'},\n",
      " {'c': 4, 'i': 3, 'l': 0, 's': 4, 't': 'e'},\n",
      " {'c': 5, 'i': 4, 'l': 0, 's': 5, 't': ' '},\n",
      " {'c': 6, 'i': 5, 'l': 0, 's': 6, 't': 'gh'},\n",
      " {'c': 8, 'i': 6, 'l': 0, 's': 8, 't': '.'},\n",
      " {'c': 9, 'i': 7, 'l': 0, 's': 9, 't': '\\n'},\n",
      " {'c': 0, 'i': 8, 'l': 1, 's': 10, 't': ' '},\n",
      " {'c': 1, 'i': 9, 'l': 1, 's': 11, 't': 'a'},\n",
      " {'c': 2, 'i': 10, 'l': 1, 's': 12, 't': ' '},\n",
      " {'c': 3, 'i': 11, 'l': 1, 's': 13, 't': 'bc'},\n",
      " {'c': 5, 'i': 12, 'l': 1, 's': 15, 't': '.'}]\n",
      "Intermediate char weights\n",
      "[0.2,\n",
      " 0.2,\n",
      " 0.2,\n",
      " 0.2,\n",
      " 0.2,\n",
      " 1.0,\n",
      " 0.25,\n",
      " 0.25,\n",
      " 0.25,\n",
      " 0.25,\n",
      " 1.0,\n",
      " 0.2,\n",
      " 0.2,\n",
      " 0.2,\n",
      " 0.2,\n",
      " 0.2]\n",
      "Token a has weight 0.2\n",
      "Token b  has weight 0.4\n",
      "Token d has weight 0.2\n",
      "Token e has weight 0.2\n",
      "Token   has weight 1.0\n",
      "Token gh has weight 0.5\n",
      "Token . has weight 0.25\n",
      "Token \n",
      " has weight 0.25\n",
      "Token   has weight 1.0\n",
      "Token a has weight 0.2\n",
      "Token   has weight 0.2\n",
      "Token bc has weight 0.4\n",
      "Token . has weight 0.2\n",
      "[0.2, 0.4, 0.2, 0.2, 1.0, 0.5, 0.25, 0.25, 1.0, 0.2, 0.2, 0.4, 0.2]\n"
     ]
    }
   ],
   "source": [
    "weights_2 = [1, 1, 1, 1, 1]\n",
    "tokenization_2 = get_tokens_with_col_and_line(text, tok_2)\n",
    "pprint(tokenization_2)\n",
    "\n",
    "pprint(\"Target tokenization 1\")\n",
    "tokenization_1 = get_tokens_with_col_and_line(text, tok_1)\n",
    "pprint(tokenization_1)\n",
    "weights_1 = convert_weigths_form_tok_to_tok(\n",
    "    tokenization_2, weights_2, tokenization_1)\n",
    "pprint(weights_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.isclose(sum(weights_1), sum(weights_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.9 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cdbdd4cf7f5282af921e9e9f8e89536c1a615a288e4742df81a89abd7b1a94fe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
