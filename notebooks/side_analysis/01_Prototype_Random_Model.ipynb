{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Goal:\n",
    "Experiment with the hugging face library:\n",
    "- download a simple model\n",
    "- use it for inference\n",
    "- extract the attention maps (perhaps via debugging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-03T10:27:12.990701Z",
     "start_time": "2022-07-03T10:27:04.861357Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPTJModel, GPTJForCausalLM\n",
    "import torch\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"ydshieh/tiny-random-gptj-for-causal-lm\")\n",
    "model = GPTJForCausalLM.from_pretrained(\"ydshieh/tiny-random-gptj-for-causal-lm\")\n",
    "\n",
    "#tokenizer = GPT2Tokenizer.from_pretrained(\"hf-internal-testing/tiny-random-gptj\")\n",
    "#model = GPTJModel.from_pretrained(\"hf-internal-testing/tiny-random-gptj\")\n",
    "#help(GPTJModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-03T10:26:56.927040Z",
     "start_time": "2022-07-03T10:26:56.922954Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTJForCausalLM(\n",
       "  (transformer): GPTJModel(\n",
       "    (wte): Embedding(1000, 32)\n",
       "    (drop): Dropout(p=0.0, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0): GPTJBlock(\n",
       "        (ln_1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTJAttention(\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (k_proj): Linear(in_features=32, out_features=32, bias=False)\n",
       "          (v_proj): Linear(in_features=32, out_features=32, bias=False)\n",
       "          (q_proj): Linear(in_features=32, out_features=32, bias=False)\n",
       "          (out_proj): Linear(in_features=32, out_features=32, bias=False)\n",
       "        )\n",
       "        (mlp): GPTJMLP(\n",
       "          (fc_in): Linear(in_features=32, out_features=128, bias=True)\n",
       "          (fc_out): Linear(in_features=128, out_features=32, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): GPTJBlock(\n",
       "        (ln_1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTJAttention(\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (k_proj): Linear(in_features=32, out_features=32, bias=False)\n",
       "          (v_proj): Linear(in_features=32, out_features=32, bias=False)\n",
       "          (q_proj): Linear(in_features=32, out_features=32, bias=False)\n",
       "          (out_proj): Linear(in_features=32, out_features=32, bias=False)\n",
       "        )\n",
       "        (mlp): GPTJMLP(\n",
       "          (fc_in): Linear(in_features=32, out_features=128, bias=True)\n",
       "          (fc_out): Linear(in_features=128, out_features=32, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): GPTJBlock(\n",
       "        (ln_1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTJAttention(\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (k_proj): Linear(in_features=32, out_features=32, bias=False)\n",
       "          (v_proj): Linear(in_features=32, out_features=32, bias=False)\n",
       "          (q_proj): Linear(in_features=32, out_features=32, bias=False)\n",
       "          (out_proj): Linear(in_features=32, out_features=32, bias=False)\n",
       "        )\n",
       "        (mlp): GPTJMLP(\n",
       "          (fc_in): Linear(in_features=32, out_features=128, bias=True)\n",
       "          (fc_out): Linear(in_features=128, out_features=32, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): GPTJBlock(\n",
       "        (ln_1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTJAttention(\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (k_proj): Linear(in_features=32, out_features=32, bias=False)\n",
       "          (v_proj): Linear(in_features=32, out_features=32, bias=False)\n",
       "          (q_proj): Linear(in_features=32, out_features=32, bias=False)\n",
       "          (out_proj): Linear(in_features=32, out_features=32, bias=False)\n",
       "        )\n",
       "        (mlp): GPTJMLP(\n",
       "          (fc_in): Linear(in_features=32, out_features=128, bias=True)\n",
       "          (fc_out): Linear(in_features=128, out_features=32, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): GPTJBlock(\n",
       "        (ln_1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTJAttention(\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (k_proj): Linear(in_features=32, out_features=32, bias=False)\n",
       "          (v_proj): Linear(in_features=32, out_features=32, bias=False)\n",
       "          (q_proj): Linear(in_features=32, out_features=32, bias=False)\n",
       "          (out_proj): Linear(in_features=32, out_features=32, bias=False)\n",
       "        )\n",
       "        (mlp): GPTJMLP(\n",
       "          (fc_in): Linear(in_features=32, out_features=128, bias=True)\n",
       "          (fc_out): Linear(in_features=128, out_features=32, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=32, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-03T10:24:58.462421Z",
     "start_time": "2022-07-03T10:24:58.450173Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 40, 416,  79,  12, 230,  89, 231, 432, 301, 212, 933]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
    "input_ids = inputs[\"input_ids\"]\n",
    "# return_tensors=\"pt\" stands for pytorch as return type and not standard python\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-03T10:25:00.854080Z",
     "start_time": "2022-07-03T10:25:00.846440Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ġ char represents the space.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['H', 'ell', 'o', ',', 'Ġm', 'y', 'Ġd', 'og', 'Ġis', 'Ġc', 'ute']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Ġ char represents the space.\" )\n",
    "tokenizer.convert_ids_to_tokens(input_ids[0])  \n",
    "# [0] is to get the first element in the batch (coincidentally also the only one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-01T15:28:07.481925Z",
     "start_time": "2022-07-01T15:28:07.441172Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on GPT2Tokenizer in module transformers.models.gpt2.tokenization_gpt2 object:\n",
      "\n",
      "class GPT2Tokenizer(transformers.tokenization_utils.PreTrainedTokenizer)\n",
      " |  GPT2Tokenizer(vocab_file, merges_file, errors='replace', unk_token='<|endoftext|>', bos_token='<|endoftext|>', eos_token='<|endoftext|>', add_prefix_space=False, **kwargs)\n",
      " |  \n",
      " |  Construct a GPT-2 tokenizer. Based on byte-level Byte-Pair-Encoding.\n",
      " |  \n",
      " |  This tokenizer has been trained to treat spaces like parts of the tokens (a bit like sentencepiece) so a word will\n",
      " |  be encoded differently whether it is at the beginning of the sentence (without space) or not:\n",
      " |  \n",
      " |  ::\n",
      " |  \n",
      " |      >>> from transformers import GPT2Tokenizer\n",
      " |      >>> tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
      " |      >>> tokenizer(\"Hello world\")['input_ids']\n",
      " |      [15496, 995]\n",
      " |      >>> tokenizer(\" Hello world\")['input_ids']\n",
      " |      [18435, 995]\n",
      " |  \n",
      " |  You can get around that behavior by passing ``add_prefix_space=True`` when instantiating this tokenizer or when you\n",
      " |  call it on some text, but since the model was not pretrained this way, it might yield a decrease in performance.\n",
      " |  \n",
      " |  .. note::\n",
      " |  \n",
      " |      When used with ``is_split_into_words=True``, this tokenizer will add a space before each word (even the first\n",
      " |      one).\n",
      " |  \n",
      " |  This tokenizer inherits from :class:`~transformers.PreTrainedTokenizer` which contains most of the main methods.\n",
      " |  Users should refer to this superclass for more information regarding those methods.\n",
      " |  \n",
      " |  Args:\n",
      " |      vocab_file (:obj:`str`):\n",
      " |          Path to the vocabulary file.\n",
      " |      merges_file (:obj:`str`):\n",
      " |          Path to the merges file.\n",
      " |      errors (:obj:`str`, `optional`, defaults to :obj:`\"replace\"`):\n",
      " |          Paradigm to follow when decoding bytes to UTF-8. See `bytes.decode\n",
      " |          <https://docs.python.org/3/library/stdtypes.html#bytes.decode>`__ for more information.\n",
      " |      unk_token (:obj:`str`, `optional`, defaults to :obj:`<|endoftext|>`):\n",
      " |          The unknown token. A token that is not in the vocabulary cannot be converted to an ID and is set to be this\n",
      " |          token instead.\n",
      " |      bos_token (:obj:`str`, `optional`, defaults to :obj:`<|endoftext|>`):\n",
      " |          The beginning of sequence token.\n",
      " |      eos_token (:obj:`str`, `optional`, defaults to :obj:`<|endoftext|>`):\n",
      " |          The end of sequence token.\n",
      " |      add_prefix_space (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      " |          Whether or not to add an initial space to the input. This allows to treat the leading word just as any\n",
      " |          other word. (GPT2 tokenizer detect beginning of words by the preceding space).\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      GPT2Tokenizer\n",
      " |      transformers.tokenization_utils.PreTrainedTokenizer\n",
      " |      transformers.tokenization_utils_base.PreTrainedTokenizerBase\n",
      " |      transformers.tokenization_utils_base.SpecialTokensMixin\n",
      " |      transformers.file_utils.PushToHubMixin\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, vocab_file, merges_file, errors='replace', unk_token='<|endoftext|>', bos_token='<|endoftext|>', eos_token='<|endoftext|>', add_prefix_space=False, **kwargs)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  bpe(self, token)\n",
      " |  \n",
      " |  convert_tokens_to_string(self, tokens)\n",
      " |      Converts a sequence of tokens (string) in a single string.\n",
      " |  \n",
      " |  get_vocab(self)\n",
      " |      Returns the vocabulary as a dictionary of token to index.\n",
      " |      \n",
      " |      :obj:`tokenizer.get_vocab()[token]` is equivalent to :obj:`tokenizer.convert_tokens_to_ids(token)` when\n",
      " |      :obj:`token` is in the vocab.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :obj:`Dict[str, int]`: The vocabulary.\n",
      " |  \n",
      " |  prepare_for_tokenization(self, text, is_split_into_words=False, **kwargs)\n",
      " |      Performs any necessary transformations before tokenization.\n",
      " |      \n",
      " |      This method should pop the arguments from kwargs and return the remaining :obj:`kwargs` as well. We test the\n",
      " |      :obj:`kwargs` at the end of the encoding process to be sure all the arguments have been used.\n",
      " |      \n",
      " |      Args:\n",
      " |          text (:obj:`str`):\n",
      " |              The text to prepare.\n",
      " |          is_split_into_words (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      " |              Whether or not the input is already pre-tokenized (e.g., split into words). If set to :obj:`True`, the\n",
      " |              tokenizer assumes the input is already split into words (for instance, by splitting it on whitespace)\n",
      " |              which it will tokenize. This is useful for NER or token classification.\n",
      " |          kwargs:\n",
      " |              Keyword arguments to use for the tokenization.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :obj:`Tuple[str, Dict[str, Any]]`: The prepared text and the unused kwargs.\n",
      " |  \n",
      " |  save_vocabulary(self, save_directory: str, filename_prefix: Union[str, NoneType] = None) -> Tuple[str]\n",
      " |      Save only the vocabulary of the tokenizer (vocabulary + added tokens).\n",
      " |      \n",
      " |      This method won't save the configuration and special token mappings of the tokenizer. Use\n",
      " |      :meth:`~transformers.PreTrainedTokenizerFast._save_pretrained` to save the whole state of the tokenizer.\n",
      " |      \n",
      " |      Args:\n",
      " |          save_directory (:obj:`str`):\n",
      " |              The directory in which to save the vocabulary.\n",
      " |          filename_prefix (:obj:`str`, `optional`):\n",
      " |              An optional prefix to add to the named of the saved files.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :obj:`Tuple(str)`: Paths to the files saved.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties defined here:\n",
      " |  \n",
      " |  vocab_size\n",
      " |      :obj:`int`: Size of the base vocabulary (without the added tokens).\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  max_model_input_sizes = {'distilgpt2': 1024, 'gpt2': 1024, 'gpt2-large...\n",
      " |  \n",
      " |  model_input_names = ['input_ids', 'attention_mask']\n",
      " |  \n",
      " |  pretrained_vocab_files_map = {'merges_file': {'distilgpt2': 'https://h...\n",
      " |  \n",
      " |  vocab_files_names = {'merges_file': 'merges.txt', 'vocab_file': 'vocab...\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from transformers.tokenization_utils.PreTrainedTokenizer:\n",
      " |  \n",
      " |  __len__(self)\n",
      " |      Size of the full vocabulary with the added tokens.\n",
      " |  \n",
      " |  convert_ids_to_tokens(self, ids: Union[int, List[int]], skip_special_tokens: bool = False) -> Union[str, List[str]]\n",
      " |      Converts a single index or a sequence of indices in a token or a sequence of tokens, using the vocabulary and\n",
      " |      added tokens.\n",
      " |      \n",
      " |      Args:\n",
      " |          ids (:obj:`int` or :obj:`List[int]`):\n",
      " |              The token id (or token ids) to convert to tokens.\n",
      " |          skip_special_tokens (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      " |              Whether or not to remove special tokens in the decoding.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :obj:`str` or :obj:`List[str]`: The decoded token(s).\n",
      " |  \n",
      " |  convert_tokens_to_ids(self, tokens: Union[str, List[str]]) -> Union[int, List[int]]\n",
      " |      Converts a token string (or a sequence of tokens) in a single integer id (or a sequence of ids), using the\n",
      " |      vocabulary.\n",
      " |      \n",
      " |      Args:\n",
      " |          tokens (:obj:`str` or :obj:`List[str]`): One or several token(s) to convert to token id(s).\n",
      " |      \n",
      " |      Returns:\n",
      " |          :obj:`int` or :obj:`List[int]`: The token id or list of token ids.\n",
      " |  \n",
      " |  get_added_vocab(self) -> Dict[str, int]\n",
      " |      Returns the added tokens in the vocabulary as a dictionary of token to index.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :obj:`Dict[str, int]`: The added tokens.\n",
      " |  \n",
      " |  get_special_tokens_mask(self, token_ids_0: List, token_ids_1: Union[List, NoneType] = None, already_has_special_tokens: bool = False) -> List[int]\n",
      " |      Retrieves sequence ids from a token list that has no special tokens added. This method is called when adding\n",
      " |      special tokens using the tokenizer ``prepare_for_model`` or ``encode_plus`` methods.\n",
      " |      \n",
      " |      Args:\n",
      " |          token_ids_0 (:obj:`List[int]`):\n",
      " |              List of ids of the first sequence.\n",
      " |          token_ids_1 (:obj:`List[int]`, `optional`):\n",
      " |              List of ids of the second sequence.\n",
      " |          already_has_special_tokens (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      " |              Whether or not the token list is already formatted with special tokens for the model.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A list of integers in the range [0, 1]: 1 for a special token, 0 for a sequence token.\n",
      " |  \n",
      " |  num_special_tokens_to_add(self, pair: bool = False) -> int\n",
      " |      Returns the number of added tokens when encoding a sequence with special tokens.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This encodes a dummy input and checks the number of added tokens, and is therefore not efficient. Do not\n",
      " |          put this inside your training loop.\n",
      " |      \n",
      " |      Args:\n",
      " |          pair (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      " |              Whether the number of added tokens should be computed in the case of a sequence pair or a single\n",
      " |              sequence.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :obj:`int`: Number of special tokens added to sequences.\n",
      " |  \n",
      " |  tokenize(self, text: str, **kwargs) -> List[str]\n",
      " |      Converts a string in a sequence of tokens, using the tokenizer.\n",
      " |      \n",
      " |      Split in words for word-based vocabulary or sub-words for sub-word-based vocabularies\n",
      " |      (BPE/SentencePieces/WordPieces). Takes care of added tokens.\n",
      " |      \n",
      " |      Args:\n",
      " |          text (:obj:`str`):\n",
      " |              The sequence to be encoded.\n",
      " |          **kwargs (additional keyword arguments):\n",
      " |              Passed along to the model-specific ``prepare_for_tokenization`` preprocessing method.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :obj:`List[str]`: The list of tokens.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from transformers.tokenization_utils.PreTrainedTokenizer:\n",
      " |  \n",
      " |  is_fast\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from transformers.tokenization_utils_base.PreTrainedTokenizerBase:\n",
      " |  \n",
      " |  __call__(self, text: Union[str, List[str], List[List[str]]], text_pair: Union[str, List[str], List[List[str]], NoneType] = None, add_special_tokens: bool = True, padding: Union[bool, str, transformers.file_utils.PaddingStrategy] = False, truncation: Union[bool, str, transformers.tokenization_utils_base.TruncationStrategy] = False, max_length: Union[int, NoneType] = None, stride: int = 0, is_split_into_words: bool = False, pad_to_multiple_of: Union[int, NoneType] = None, return_tensors: Union[str, transformers.file_utils.TensorType, NoneType] = None, return_token_type_ids: Union[bool, NoneType] = None, return_attention_mask: Union[bool, NoneType] = None, return_overflowing_tokens: bool = False, return_special_tokens_mask: bool = False, return_offsets_mapping: bool = False, return_length: bool = False, verbose: bool = True, **kwargs) -> transformers.tokenization_utils_base.BatchEncoding\n",
      " |      Main method to tokenize and prepare for the model one or several sequence(s) or one or several pair(s) of\n",
      " |      sequences.\n",
      " |      \n",
      " |      Args:\n",
      " |          text (:obj:`str`, :obj:`List[str]`, :obj:`List[List[str]]`):\n",
      " |              The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings\n",
      " |              (pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set\n",
      " |              :obj:`is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n",
      " |          text_pair (:obj:`str`, :obj:`List[str]`, :obj:`List[List[str]]`):\n",
      " |              The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings\n",
      " |              (pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set\n",
      " |              :obj:`is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n",
      " |      \n",
      " |          add_special_tokens (:obj:`bool`, `optional`, defaults to :obj:`True`):\n",
      " |              Whether or not to encode the sequences with the special tokens relative to their model.\n",
      " |          padding (:obj:`bool`, :obj:`str` or :class:`~transformers.file_utils.PaddingStrategy`, `optional`, defaults to :obj:`False`):\n",
      " |              Activates and controls padding. Accepts the following values:\n",
      " |      \n",
      " |              * :obj:`True` or :obj:`'longest'`: Pad to the longest sequence in the batch (or no padding if only a\n",
      " |                single sequence if provided).\n",
      " |              * :obj:`'max_length'`: Pad to a maximum length specified with the argument :obj:`max_length` or to the\n",
      " |                maximum acceptable input length for the model if that argument is not provided.\n",
      " |              * :obj:`False` or :obj:`'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of\n",
      " |                different lengths).\n",
      " |          truncation (:obj:`bool`, :obj:`str` or :class:`~transformers.tokenization_utils_base.TruncationStrategy`, `optional`, defaults to :obj:`False`):\n",
      " |              Activates and controls truncation. Accepts the following values:\n",
      " |      \n",
      " |              * :obj:`True` or :obj:`'longest_first'`: Truncate to a maximum length specified with the argument\n",
      " |                :obj:`max_length` or to the maximum acceptable input length for the model if that argument is not\n",
      " |                provided. This will truncate token by token, removing a token from the longest sequence in the pair\n",
      " |                if a pair of sequences (or a batch of pairs) is provided.\n",
      " |              * :obj:`'only_first'`: Truncate to a maximum length specified with the argument :obj:`max_length` or to\n",
      " |                the maximum acceptable input length for the model if that argument is not provided. This will only\n",
      " |                truncate the first sequence of a pair if a pair of sequences (or a batch of pairs) is provided.\n",
      " |              * :obj:`'only_second'`: Truncate to a maximum length specified with the argument :obj:`max_length` or\n",
      " |                to the maximum acceptable input length for the model if that argument is not provided. This will only\n",
      " |                truncate the second sequence of a pair if a pair of sequences (or a batch of pairs) is provided.\n",
      " |              * :obj:`False` or :obj:`'do_not_truncate'` (default): No truncation (i.e., can output batch with\n",
      " |                sequence lengths greater than the model maximum admissible input size).\n",
      " |          max_length (:obj:`int`, `optional`):\n",
      " |              Controls the maximum length to use by one of the truncation/padding parameters.\n",
      " |      \n",
      " |              If left unset or set to :obj:`None`, this will use the predefined model maximum length if a maximum\n",
      " |              length is required by one of the truncation/padding parameters. If the model has no specific maximum\n",
      " |              input length (like XLNet) truncation/padding to a maximum length will be deactivated.\n",
      " |          stride (:obj:`int`, `optional`, defaults to 0):\n",
      " |              If set to a number along with :obj:`max_length`, the overflowing tokens returned when\n",
      " |              :obj:`return_overflowing_tokens=True` will contain some tokens from the end of the truncated sequence\n",
      " |              returned to provide some overlap between truncated and overflowing sequences. The value of this\n",
      " |              argument defines the number of overlapping tokens.\n",
      " |          is_split_into_words (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      " |              Whether or not the input is already pre-tokenized (e.g., split into words). If set to :obj:`True`, the\n",
      " |              tokenizer assumes the input is already split into words (for instance, by splitting it on whitespace)\n",
      " |              which it will tokenize. This is useful for NER or token classification.\n",
      " |          pad_to_multiple_of (:obj:`int`, `optional`):\n",
      " |              If set will pad the sequence to a multiple of the provided value. This is especially useful to enable\n",
      " |              the use of Tensor Cores on NVIDIA hardware with compute capability >= 7.5 (Volta).\n",
      " |          return_tensors (:obj:`str` or :class:`~transformers.file_utils.TensorType`, `optional`):\n",
      " |              If set, will return tensors instead of list of python integers. Acceptable values are:\n",
      " |      \n",
      " |              * :obj:`'tf'`: Return TensorFlow :obj:`tf.constant` objects.\n",
      " |              * :obj:`'pt'`: Return PyTorch :obj:`torch.Tensor` objects.\n",
      " |              * :obj:`'np'`: Return Numpy :obj:`np.ndarray` objects.\n",
      " |      \n",
      " |          return_token_type_ids (:obj:`bool`, `optional`):\n",
      " |              Whether to return token type IDs. If left to the default, will return the token type IDs according to\n",
      " |              the specific tokenizer's default, defined by the :obj:`return_outputs` attribute.\n",
      " |      \n",
      " |              `What are token type IDs? <../glossary.html#token-type-ids>`__\n",
      " |          return_attention_mask (:obj:`bool`, `optional`):\n",
      " |              Whether to return the attention mask. If left to the default, will return the attention mask according\n",
      " |              to the specific tokenizer's default, defined by the :obj:`return_outputs` attribute.\n",
      " |      \n",
      " |              `What are attention masks? <../glossary.html#attention-mask>`__\n",
      " |          return_overflowing_tokens (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      " |              Whether or not to return overflowing token sequences. If a pair of sequences of input ids (or a batch\n",
      " |              of pairs) is provided with :obj:`truncation_strategy = longest_first` or :obj:`True`, an error is\n",
      " |              raised instead of returning overflowing tokens.\n",
      " |          return_special_tokens_mask (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      " |              Whether or not to return special tokens mask information.\n",
      " |          return_offsets_mapping (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      " |              Whether or not to return :obj:`(char_start, char_end)` for each token.\n",
      " |      \n",
      " |              This is only available on fast tokenizers inheriting from\n",
      " |              :class:`~transformers.PreTrainedTokenizerFast`, if using Python's tokenizer, this method will raise\n",
      " |              :obj:`NotImplementedError`.\n",
      " |          return_length  (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      " |              Whether or not to return the lengths of the encoded inputs.\n",
      " |          verbose (:obj:`bool`, `optional`, defaults to :obj:`True`):\n",
      " |              Whether or not to print more information and warnings.\n",
      " |          **kwargs: passed to the :obj:`self.tokenize()` method\n",
      " |      \n",
      " |      Return:\n",
      " |          :class:`~transformers.BatchEncoding`: A :class:`~transformers.BatchEncoding` with the following fields:\n",
      " |      \n",
      " |          - **input_ids** -- List of token ids to be fed to a model.\n",
      " |      \n",
      " |            `What are input IDs? <../glossary.html#input-ids>`__\n",
      " |      \n",
      " |          - **token_type_ids** -- List of token type ids to be fed to a model (when :obj:`return_token_type_ids=True`\n",
      " |            or if `\"token_type_ids\"` is in :obj:`self.model_input_names`).\n",
      " |      \n",
      " |            `What are token type IDs? <../glossary.html#token-type-ids>`__\n",
      " |      \n",
      " |          - **attention_mask** -- List of indices specifying which tokens should be attended to by the model (when\n",
      " |            :obj:`return_attention_mask=True` or if `\"attention_mask\"` is in :obj:`self.model_input_names`).\n",
      " |      \n",
      " |            `What are attention masks? <../glossary.html#attention-mask>`__\n",
      " |      \n",
      " |          - **overflowing_tokens** -- List of overflowing tokens sequences (when a :obj:`max_length` is specified and\n",
      " |            :obj:`return_overflowing_tokens=True`).\n",
      " |          - **num_truncated_tokens** -- Number of tokens truncated (when a :obj:`max_length` is specified and\n",
      " |            :obj:`return_overflowing_tokens=True`).\n",
      " |          - **special_tokens_mask** -- List of 0s and 1s, with 1 specifying added special tokens and 0 specifying\n",
      " |            regular sequence tokens (when :obj:`add_special_tokens=True` and :obj:`return_special_tokens_mask=True`).\n",
      " |          - **length** -- The length of the inputs (when :obj:`return_length=True`)\n",
      " |  \n",
      " |  __repr__(self) -> str\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  as_target_tokenizer(self)\n",
      " |      Temporarily sets the tokenizer for encoding the targets. Useful for tokenizer associated to\n",
      " |      sequence-to-sequence models that need a slightly different processing for the labels.\n",
      " |  \n",
      " |  batch_decode(self, sequences: Union[List[int], List[List[int]], ForwardRef('np.ndarray'), ForwardRef('torch.Tensor'), ForwardRef('tf.Tensor')], skip_special_tokens: bool = False, clean_up_tokenization_spaces: bool = True, **kwargs) -> List[str]\n",
      " |      Convert a list of lists of token ids into a list of strings by calling decode.\n",
      " |      \n",
      " |      Args:\n",
      " |          sequences (:obj:`Union[List[int], List[List[int]], np.ndarray, torch.Tensor, tf.Tensor]`):\n",
      " |              List of tokenized input ids. Can be obtained using the ``__call__`` method.\n",
      " |          skip_special_tokens (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      " |              Whether or not to remove special tokens in the decoding.\n",
      " |          clean_up_tokenization_spaces (:obj:`bool`, `optional`, defaults to :obj:`True`):\n",
      " |              Whether or not to clean up the tokenization spaces.\n",
      " |          kwargs (additional keyword arguments, `optional`):\n",
      " |              Will be passed to the underlying model specific decode method.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :obj:`List[str]`: The list of decoded sentences.\n",
      " |  \n",
      " |  batch_encode_plus(self, batch_text_or_text_pairs: Union[List[str], List[Tuple[str, str]], List[List[str]], List[Tuple[List[str], List[str]]], List[List[int]], List[Tuple[List[int], List[int]]]], add_special_tokens: bool = True, padding: Union[bool, str, transformers.file_utils.PaddingStrategy] = False, truncation: Union[bool, str, transformers.tokenization_utils_base.TruncationStrategy] = False, max_length: Union[int, NoneType] = None, stride: int = 0, is_split_into_words: bool = False, pad_to_multiple_of: Union[int, NoneType] = None, return_tensors: Union[str, transformers.file_utils.TensorType, NoneType] = None, return_token_type_ids: Union[bool, NoneType] = None, return_attention_mask: Union[bool, NoneType] = None, return_overflowing_tokens: bool = False, return_special_tokens_mask: bool = False, return_offsets_mapping: bool = False, return_length: bool = False, verbose: bool = True, **kwargs) -> transformers.tokenization_utils_base.BatchEncoding\n",
      " |      Tokenize and prepare for the model a list of sequences or a list of pairs of sequences.\n",
      " |      \n",
      " |      .. warning::\n",
      " |          This method is deprecated, ``__call__`` should be used instead.\n",
      " |      \n",
      " |      Args:\n",
      " |          batch_text_or_text_pairs (:obj:`List[str]`, :obj:`List[Tuple[str, str]]`, :obj:`List[List[str]]`, :obj:`List[Tuple[List[str], List[str]]]`, and for not-fast tokenizers, also :obj:`List[List[int]]`, :obj:`List[Tuple[List[int], List[int]]]`):\n",
      " |              Batch of sequences or pair of sequences to be encoded. This can be a list of\n",
      " |              string/string-sequences/int-sequences or a list of pair of string/string-sequences/int-sequence (see\n",
      " |              details in ``encode_plus``).\n",
      " |      \n",
      " |          add_special_tokens (:obj:`bool`, `optional`, defaults to :obj:`True`):\n",
      " |              Whether or not to encode the sequences with the special tokens relative to their model.\n",
      " |          padding (:obj:`bool`, :obj:`str` or :class:`~transformers.file_utils.PaddingStrategy`, `optional`, defaults to :obj:`False`):\n",
      " |              Activates and controls padding. Accepts the following values:\n",
      " |      \n",
      " |              * :obj:`True` or :obj:`'longest'`: Pad to the longest sequence in the batch (or no padding if only a\n",
      " |                single sequence if provided).\n",
      " |              * :obj:`'max_length'`: Pad to a maximum length specified with the argument :obj:`max_length` or to the\n",
      " |                maximum acceptable input length for the model if that argument is not provided.\n",
      " |              * :obj:`False` or :obj:`'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of\n",
      " |                different lengths).\n",
      " |          truncation (:obj:`bool`, :obj:`str` or :class:`~transformers.tokenization_utils_base.TruncationStrategy`, `optional`, defaults to :obj:`False`):\n",
      " |              Activates and controls truncation. Accepts the following values:\n",
      " |      \n",
      " |              * :obj:`True` or :obj:`'longest_first'`: Truncate to a maximum length specified with the argument\n",
      " |                :obj:`max_length` or to the maximum acceptable input length for the model if that argument is not\n",
      " |                provided. This will truncate token by token, removing a token from the longest sequence in the pair\n",
      " |                if a pair of sequences (or a batch of pairs) is provided.\n",
      " |              * :obj:`'only_first'`: Truncate to a maximum length specified with the argument :obj:`max_length` or to\n",
      " |                the maximum acceptable input length for the model if that argument is not provided. This will only\n",
      " |                truncate the first sequence of a pair if a pair of sequences (or a batch of pairs) is provided.\n",
      " |              * :obj:`'only_second'`: Truncate to a maximum length specified with the argument :obj:`max_length` or\n",
      " |                to the maximum acceptable input length for the model if that argument is not provided. This will only\n",
      " |                truncate the second sequence of a pair if a pair of sequences (or a batch of pairs) is provided.\n",
      " |              * :obj:`False` or :obj:`'do_not_truncate'` (default): No truncation (i.e., can output batch with\n",
      " |                sequence lengths greater than the model maximum admissible input size).\n",
      " |          max_length (:obj:`int`, `optional`):\n",
      " |              Controls the maximum length to use by one of the truncation/padding parameters.\n",
      " |      \n",
      " |              If left unset or set to :obj:`None`, this will use the predefined model maximum length if a maximum\n",
      " |              length is required by one of the truncation/padding parameters. If the model has no specific maximum\n",
      " |              input length (like XLNet) truncation/padding to a maximum length will be deactivated.\n",
      " |          stride (:obj:`int`, `optional`, defaults to 0):\n",
      " |              If set to a number along with :obj:`max_length`, the overflowing tokens returned when\n",
      " |              :obj:`return_overflowing_tokens=True` will contain some tokens from the end of the truncated sequence\n",
      " |              returned to provide some overlap between truncated and overflowing sequences. The value of this\n",
      " |              argument defines the number of overlapping tokens.\n",
      " |          is_split_into_words (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      " |              Whether or not the input is already pre-tokenized (e.g., split into words). If set to :obj:`True`, the\n",
      " |              tokenizer assumes the input is already split into words (for instance, by splitting it on whitespace)\n",
      " |              which it will tokenize. This is useful for NER or token classification.\n",
      " |          pad_to_multiple_of (:obj:`int`, `optional`):\n",
      " |              If set will pad the sequence to a multiple of the provided value. This is especially useful to enable\n",
      " |              the use of Tensor Cores on NVIDIA hardware with compute capability >= 7.5 (Volta).\n",
      " |          return_tensors (:obj:`str` or :class:`~transformers.file_utils.TensorType`, `optional`):\n",
      " |              If set, will return tensors instead of list of python integers. Acceptable values are:\n",
      " |      \n",
      " |              * :obj:`'tf'`: Return TensorFlow :obj:`tf.constant` objects.\n",
      " |              * :obj:`'pt'`: Return PyTorch :obj:`torch.Tensor` objects.\n",
      " |              * :obj:`'np'`: Return Numpy :obj:`np.ndarray` objects.\n",
      " |      \n",
      " |          return_token_type_ids (:obj:`bool`, `optional`):\n",
      " |              Whether to return token type IDs. If left to the default, will return the token type IDs according to\n",
      " |              the specific tokenizer's default, defined by the :obj:`return_outputs` attribute.\n",
      " |      \n",
      " |              `What are token type IDs? <../glossary.html#token-type-ids>`__\n",
      " |          return_attention_mask (:obj:`bool`, `optional`):\n",
      " |              Whether to return the attention mask. If left to the default, will return the attention mask according\n",
      " |              to the specific tokenizer's default, defined by the :obj:`return_outputs` attribute.\n",
      " |      \n",
      " |              `What are attention masks? <../glossary.html#attention-mask>`__\n",
      " |          return_overflowing_tokens (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      " |              Whether or not to return overflowing token sequences. If a pair of sequences of input ids (or a batch\n",
      " |              of pairs) is provided with :obj:`truncation_strategy = longest_first` or :obj:`True`, an error is\n",
      " |              raised instead of returning overflowing tokens.\n",
      " |          return_special_tokens_mask (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      " |              Whether or not to return special tokens mask information.\n",
      " |          return_offsets_mapping (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      " |              Whether or not to return :obj:`(char_start, char_end)` for each token.\n",
      " |      \n",
      " |              This is only available on fast tokenizers inheriting from\n",
      " |              :class:`~transformers.PreTrainedTokenizerFast`, if using Python's tokenizer, this method will raise\n",
      " |              :obj:`NotImplementedError`.\n",
      " |          return_length  (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      " |              Whether or not to return the lengths of the encoded inputs.\n",
      " |          verbose (:obj:`bool`, `optional`, defaults to :obj:`True`):\n",
      " |              Whether or not to print more information and warnings.\n",
      " |          **kwargs: passed to the :obj:`self.tokenize()` method\n",
      " |      \n",
      " |      Return:\n",
      " |          :class:`~transformers.BatchEncoding`: A :class:`~transformers.BatchEncoding` with the following fields:\n",
      " |      \n",
      " |          - **input_ids** -- List of token ids to be fed to a model.\n",
      " |      \n",
      " |            `What are input IDs? <../glossary.html#input-ids>`__\n",
      " |      \n",
      " |          - **token_type_ids** -- List of token type ids to be fed to a model (when :obj:`return_token_type_ids=True`\n",
      " |            or if `\"token_type_ids\"` is in :obj:`self.model_input_names`).\n",
      " |      \n",
      " |            `What are token type IDs? <../glossary.html#token-type-ids>`__\n",
      " |      \n",
      " |          - **attention_mask** -- List of indices specifying which tokens should be attended to by the model (when\n",
      " |            :obj:`return_attention_mask=True` or if `\"attention_mask\"` is in :obj:`self.model_input_names`).\n",
      " |      \n",
      " |            `What are attention masks? <../glossary.html#attention-mask>`__\n",
      " |      \n",
      " |          - **overflowing_tokens** -- List of overflowing tokens sequences (when a :obj:`max_length` is specified and\n",
      " |            :obj:`return_overflowing_tokens=True`).\n",
      " |          - **num_truncated_tokens** -- Number of tokens truncated (when a :obj:`max_length` is specified and\n",
      " |            :obj:`return_overflowing_tokens=True`).\n",
      " |          - **special_tokens_mask** -- List of 0s and 1s, with 1 specifying added special tokens and 0 specifying\n",
      " |            regular sequence tokens (when :obj:`add_special_tokens=True` and :obj:`return_special_tokens_mask=True`).\n",
      " |          - **length** -- The length of the inputs (when :obj:`return_length=True`)\n",
      " |  \n",
      " |  build_inputs_with_special_tokens(self, token_ids_0: List[int], token_ids_1: Union[List[int], NoneType] = None) -> List[int]\n",
      " |      Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and\n",
      " |      adding special tokens.\n",
      " |      \n",
      " |      This implementation does not add special tokens and this method should be overridden in a subclass.\n",
      " |      \n",
      " |      Args:\n",
      " |          token_ids_0 (:obj:`List[int]`): The first tokenized sequence.\n",
      " |          token_ids_1 (:obj:`List[int]`, `optional`): The second tokenized sequence.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :obj:`List[int]`: The model input with special tokens.\n",
      " |  \n",
      " |  create_token_type_ids_from_sequences(self, token_ids_0: List[int], token_ids_1: Union[List[int], NoneType] = None) -> List[int]\n",
      " |      Create the token type IDs corresponding to the sequences passed. `What are token type IDs?\n",
      " |      <../glossary.html#token-type-ids>`__\n",
      " |      \n",
      " |      Should be overridden in a subclass if the model has a special way of building those.\n",
      " |      \n",
      " |      Args:\n",
      " |          token_ids_0 (:obj:`List[int]`): The first tokenized sequence.\n",
      " |          token_ids_1 (:obj:`List[int]`, `optional`): The second tokenized sequence.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :obj:`List[int]`: The token type ids.\n",
      " |  \n",
      " |  decode(self, token_ids: Union[int, List[int], ForwardRef('np.ndarray'), ForwardRef('torch.Tensor'), ForwardRef('tf.Tensor')], skip_special_tokens: bool = False, clean_up_tokenization_spaces: bool = True, **kwargs) -> str\n",
      " |      Converts a sequence of ids in a string, using the tokenizer and vocabulary with options to remove special\n",
      " |      tokens and clean up tokenization spaces.\n",
      " |      \n",
      " |      Similar to doing ``self.convert_tokens_to_string(self.convert_ids_to_tokens(token_ids))``.\n",
      " |      \n",
      " |      Args:\n",
      " |          token_ids (:obj:`Union[int, List[int], np.ndarray, torch.Tensor, tf.Tensor]`):\n",
      " |              List of tokenized input ids. Can be obtained using the ``__call__`` method.\n",
      " |          skip_special_tokens (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      " |              Whether or not to remove special tokens in the decoding.\n",
      " |          clean_up_tokenization_spaces (:obj:`bool`, `optional`, defaults to :obj:`True`):\n",
      " |              Whether or not to clean up the tokenization spaces.\n",
      " |          kwargs (additional keyword arguments, `optional`):\n",
      " |              Will be passed to the underlying model specific decode method.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :obj:`str`: The decoded sentence.\n",
      " |  \n",
      " |  encode(self, text: Union[str, List[str], List[int]], text_pair: Union[str, List[str], List[int], NoneType] = None, add_special_tokens: bool = True, padding: Union[bool, str, transformers.file_utils.PaddingStrategy] = False, truncation: Union[bool, str, transformers.tokenization_utils_base.TruncationStrategy] = False, max_length: Union[int, NoneType] = None, stride: int = 0, return_tensors: Union[str, transformers.file_utils.TensorType, NoneType] = None, **kwargs) -> List[int]\n",
      " |      Converts a string to a sequence of ids (integer), using the tokenizer and vocabulary.\n",
      " |      \n",
      " |      Same as doing ``self.convert_tokens_to_ids(self.tokenize(text))``.\n",
      " |      \n",
      " |      Args:\n",
      " |          text (:obj:`str`, :obj:`List[str]` or :obj:`List[int]`):\n",
      " |              The first sequence to be encoded. This can be a string, a list of strings (tokenized string using the\n",
      " |              ``tokenize`` method) or a list of integers (tokenized string ids using the ``convert_tokens_to_ids``\n",
      " |              method).\n",
      " |          text_pair (:obj:`str`, :obj:`List[str]` or :obj:`List[int]`, `optional`):\n",
      " |              Optional second sequence to be encoded. This can be a string, a list of strings (tokenized string using\n",
      " |              the ``tokenize`` method) or a list of integers (tokenized string ids using the\n",
      " |              ``convert_tokens_to_ids`` method).\n",
      " |      \n",
      " |          add_special_tokens (:obj:`bool`, `optional`, defaults to :obj:`True`):\n",
      " |              Whether or not to encode the sequences with the special tokens relative to their model.\n",
      " |          padding (:obj:`bool`, :obj:`str` or :class:`~transformers.file_utils.PaddingStrategy`, `optional`, defaults to :obj:`False`):\n",
      " |              Activates and controls padding. Accepts the following values:\n",
      " |      \n",
      " |              * :obj:`True` or :obj:`'longest'`: Pad to the longest sequence in the batch (or no padding if only a\n",
      " |                single sequence if provided).\n",
      " |              * :obj:`'max_length'`: Pad to a maximum length specified with the argument :obj:`max_length` or to the\n",
      " |                maximum acceptable input length for the model if that argument is not provided.\n",
      " |              * :obj:`False` or :obj:`'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of\n",
      " |                different lengths).\n",
      " |          truncation (:obj:`bool`, :obj:`str` or :class:`~transformers.tokenization_utils_base.TruncationStrategy`, `optional`, defaults to :obj:`False`):\n",
      " |              Activates and controls truncation. Accepts the following values:\n",
      " |      \n",
      " |              * :obj:`True` or :obj:`'longest_first'`: Truncate to a maximum length specified with the argument\n",
      " |                :obj:`max_length` or to the maximum acceptable input length for the model if that argument is not\n",
      " |                provided. This will truncate token by token, removing a token from the longest sequence in the pair\n",
      " |                if a pair of sequences (or a batch of pairs) is provided.\n",
      " |              * :obj:`'only_first'`: Truncate to a maximum length specified with the argument :obj:`max_length` or to\n",
      " |                the maximum acceptable input length for the model if that argument is not provided. This will only\n",
      " |                truncate the first sequence of a pair if a pair of sequences (or a batch of pairs) is provided.\n",
      " |              * :obj:`'only_second'`: Truncate to a maximum length specified with the argument :obj:`max_length` or\n",
      " |                to the maximum acceptable input length for the model if that argument is not provided. This will only\n",
      " |                truncate the second sequence of a pair if a pair of sequences (or a batch of pairs) is provided.\n",
      " |              * :obj:`False` or :obj:`'do_not_truncate'` (default): No truncation (i.e., can output batch with\n",
      " |                sequence lengths greater than the model maximum admissible input size).\n",
      " |          max_length (:obj:`int`, `optional`):\n",
      " |              Controls the maximum length to use by one of the truncation/padding parameters.\n",
      " |      \n",
      " |              If left unset or set to :obj:`None`, this will use the predefined model maximum length if a maximum\n",
      " |              length is required by one of the truncation/padding parameters. If the model has no specific maximum\n",
      " |              input length (like XLNet) truncation/padding to a maximum length will be deactivated.\n",
      " |          stride (:obj:`int`, `optional`, defaults to 0):\n",
      " |              If set to a number along with :obj:`max_length`, the overflowing tokens returned when\n",
      " |              :obj:`return_overflowing_tokens=True` will contain some tokens from the end of the truncated sequence\n",
      " |              returned to provide some overlap between truncated and overflowing sequences. The value of this\n",
      " |              argument defines the number of overlapping tokens.\n",
      " |          is_split_into_words (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      " |              Whether or not the input is already pre-tokenized (e.g., split into words). If set to :obj:`True`, the\n",
      " |              tokenizer assumes the input is already split into words (for instance, by splitting it on whitespace)\n",
      " |              which it will tokenize. This is useful for NER or token classification.\n",
      " |          pad_to_multiple_of (:obj:`int`, `optional`):\n",
      " |              If set will pad the sequence to a multiple of the provided value. This is especially useful to enable\n",
      " |              the use of Tensor Cores on NVIDIA hardware with compute capability >= 7.5 (Volta).\n",
      " |          return_tensors (:obj:`str` or :class:`~transformers.file_utils.TensorType`, `optional`):\n",
      " |              If set, will return tensors instead of list of python integers. Acceptable values are:\n",
      " |      \n",
      " |              * :obj:`'tf'`: Return TensorFlow :obj:`tf.constant` objects.\n",
      " |              * :obj:`'pt'`: Return PyTorch :obj:`torch.Tensor` objects.\n",
      " |              * :obj:`'np'`: Return Numpy :obj:`np.ndarray` objects.\n",
      " |      \n",
      " |          **kwargs: Passed along to the `.tokenize()` method.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :obj:`List[int]`, :obj:`torch.Tensor`, :obj:`tf.Tensor` or :obj:`np.ndarray`: The tokenized ids of the\n",
      " |          text.\n",
      " |  \n",
      " |  encode_plus(self, text: Union[str, List[str], List[int]], text_pair: Union[str, List[str], List[int], NoneType] = None, add_special_tokens: bool = True, padding: Union[bool, str, transformers.file_utils.PaddingStrategy] = False, truncation: Union[bool, str, transformers.tokenization_utils_base.TruncationStrategy] = False, max_length: Union[int, NoneType] = None, stride: int = 0, is_split_into_words: bool = False, pad_to_multiple_of: Union[int, NoneType] = None, return_tensors: Union[str, transformers.file_utils.TensorType, NoneType] = None, return_token_type_ids: Union[bool, NoneType] = None, return_attention_mask: Union[bool, NoneType] = None, return_overflowing_tokens: bool = False, return_special_tokens_mask: bool = False, return_offsets_mapping: bool = False, return_length: bool = False, verbose: bool = True, **kwargs) -> transformers.tokenization_utils_base.BatchEncoding\n",
      " |      Tokenize and prepare for the model a sequence or a pair of sequences.\n",
      " |      \n",
      " |      .. warning::\n",
      " |          This method is deprecated, ``__call__`` should be used instead.\n",
      " |      \n",
      " |      Args:\n",
      " |          text (:obj:`str`, :obj:`List[str]` or :obj:`List[int]` (the latter only for not-fast tokenizers)):\n",
      " |              The first sequence to be encoded. This can be a string, a list of strings (tokenized string using the\n",
      " |              ``tokenize`` method) or a list of integers (tokenized string ids using the ``convert_tokens_to_ids``\n",
      " |              method).\n",
      " |          text_pair (:obj:`str`, :obj:`List[str]` or :obj:`List[int]`, `optional`):\n",
      " |              Optional second sequence to be encoded. This can be a string, a list of strings (tokenized string using\n",
      " |              the ``tokenize`` method) or a list of integers (tokenized string ids using the\n",
      " |              ``convert_tokens_to_ids`` method).\n",
      " |      \n",
      " |          add_special_tokens (:obj:`bool`, `optional`, defaults to :obj:`True`):\n",
      " |              Whether or not to encode the sequences with the special tokens relative to their model.\n",
      " |          padding (:obj:`bool`, :obj:`str` or :class:`~transformers.file_utils.PaddingStrategy`, `optional`, defaults to :obj:`False`):\n",
      " |              Activates and controls padding. Accepts the following values:\n",
      " |      \n",
      " |              * :obj:`True` or :obj:`'longest'`: Pad to the longest sequence in the batch (or no padding if only a\n",
      " |                single sequence if provided).\n",
      " |              * :obj:`'max_length'`: Pad to a maximum length specified with the argument :obj:`max_length` or to the\n",
      " |                maximum acceptable input length for the model if that argument is not provided.\n",
      " |              * :obj:`False` or :obj:`'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of\n",
      " |                different lengths).\n",
      " |          truncation (:obj:`bool`, :obj:`str` or :class:`~transformers.tokenization_utils_base.TruncationStrategy`, `optional`, defaults to :obj:`False`):\n",
      " |              Activates and controls truncation. Accepts the following values:\n",
      " |      \n",
      " |              * :obj:`True` or :obj:`'longest_first'`: Truncate to a maximum length specified with the argument\n",
      " |                :obj:`max_length` or to the maximum acceptable input length for the model if that argument is not\n",
      " |                provided. This will truncate token by token, removing a token from the longest sequence in the pair\n",
      " |                if a pair of sequences (or a batch of pairs) is provided.\n",
      " |              * :obj:`'only_first'`: Truncate to a maximum length specified with the argument :obj:`max_length` or to\n",
      " |                the maximum acceptable input length for the model if that argument is not provided. This will only\n",
      " |                truncate the first sequence of a pair if a pair of sequences (or a batch of pairs) is provided.\n",
      " |              * :obj:`'only_second'`: Truncate to a maximum length specified with the argument :obj:`max_length` or\n",
      " |                to the maximum acceptable input length for the model if that argument is not provided. This will only\n",
      " |                truncate the second sequence of a pair if a pair of sequences (or a batch of pairs) is provided.\n",
      " |              * :obj:`False` or :obj:`'do_not_truncate'` (default): No truncation (i.e., can output batch with\n",
      " |                sequence lengths greater than the model maximum admissible input size).\n",
      " |          max_length (:obj:`int`, `optional`):\n",
      " |              Controls the maximum length to use by one of the truncation/padding parameters.\n",
      " |      \n",
      " |              If left unset or set to :obj:`None`, this will use the predefined model maximum length if a maximum\n",
      " |              length is required by one of the truncation/padding parameters. If the model has no specific maximum\n",
      " |              input length (like XLNet) truncation/padding to a maximum length will be deactivated.\n",
      " |          stride (:obj:`int`, `optional`, defaults to 0):\n",
      " |              If set to a number along with :obj:`max_length`, the overflowing tokens returned when\n",
      " |              :obj:`return_overflowing_tokens=True` will contain some tokens from the end of the truncated sequence\n",
      " |              returned to provide some overlap between truncated and overflowing sequences. The value of this\n",
      " |              argument defines the number of overlapping tokens.\n",
      " |          is_split_into_words (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      " |              Whether or not the input is already pre-tokenized (e.g., split into words). If set to :obj:`True`, the\n",
      " |              tokenizer assumes the input is already split into words (for instance, by splitting it on whitespace)\n",
      " |              which it will tokenize. This is useful for NER or token classification.\n",
      " |          pad_to_multiple_of (:obj:`int`, `optional`):\n",
      " |              If set will pad the sequence to a multiple of the provided value. This is especially useful to enable\n",
      " |              the use of Tensor Cores on NVIDIA hardware with compute capability >= 7.5 (Volta).\n",
      " |          return_tensors (:obj:`str` or :class:`~transformers.file_utils.TensorType`, `optional`):\n",
      " |              If set, will return tensors instead of list of python integers. Acceptable values are:\n",
      " |      \n",
      " |              * :obj:`'tf'`: Return TensorFlow :obj:`tf.constant` objects.\n",
      " |              * :obj:`'pt'`: Return PyTorch :obj:`torch.Tensor` objects.\n",
      " |              * :obj:`'np'`: Return Numpy :obj:`np.ndarray` objects.\n",
      " |      \n",
      " |          return_token_type_ids (:obj:`bool`, `optional`):\n",
      " |              Whether to return token type IDs. If left to the default, will return the token type IDs according to\n",
      " |              the specific tokenizer's default, defined by the :obj:`return_outputs` attribute.\n",
      " |      \n",
      " |              `What are token type IDs? <../glossary.html#token-type-ids>`__\n",
      " |          return_attention_mask (:obj:`bool`, `optional`):\n",
      " |              Whether to return the attention mask. If left to the default, will return the attention mask according\n",
      " |              to the specific tokenizer's default, defined by the :obj:`return_outputs` attribute.\n",
      " |      \n",
      " |              `What are attention masks? <../glossary.html#attention-mask>`__\n",
      " |          return_overflowing_tokens (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      " |              Whether or not to return overflowing token sequences. If a pair of sequences of input ids (or a batch\n",
      " |              of pairs) is provided with :obj:`truncation_strategy = longest_first` or :obj:`True`, an error is\n",
      " |              raised instead of returning overflowing tokens.\n",
      " |          return_special_tokens_mask (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      " |              Whether or not to return special tokens mask information.\n",
      " |          return_offsets_mapping (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      " |              Whether or not to return :obj:`(char_start, char_end)` for each token.\n",
      " |      \n",
      " |              This is only available on fast tokenizers inheriting from\n",
      " |              :class:`~transformers.PreTrainedTokenizerFast`, if using Python's tokenizer, this method will raise\n",
      " |              :obj:`NotImplementedError`.\n",
      " |          return_length  (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      " |              Whether or not to return the lengths of the encoded inputs.\n",
      " |          verbose (:obj:`bool`, `optional`, defaults to :obj:`True`):\n",
      " |              Whether or not to print more information and warnings.\n",
      " |          **kwargs: passed to the :obj:`self.tokenize()` method\n",
      " |      \n",
      " |      Return:\n",
      " |          :class:`~transformers.BatchEncoding`: A :class:`~transformers.BatchEncoding` with the following fields:\n",
      " |      \n",
      " |          - **input_ids** -- List of token ids to be fed to a model.\n",
      " |      \n",
      " |            `What are input IDs? <../glossary.html#input-ids>`__\n",
      " |      \n",
      " |          - **token_type_ids** -- List of token type ids to be fed to a model (when :obj:`return_token_type_ids=True`\n",
      " |            or if `\"token_type_ids\"` is in :obj:`self.model_input_names`).\n",
      " |      \n",
      " |            `What are token type IDs? <../glossary.html#token-type-ids>`__\n",
      " |      \n",
      " |          - **attention_mask** -- List of indices specifying which tokens should be attended to by the model (when\n",
      " |            :obj:`return_attention_mask=True` or if `\"attention_mask\"` is in :obj:`self.model_input_names`).\n",
      " |      \n",
      " |            `What are attention masks? <../glossary.html#attention-mask>`__\n",
      " |      \n",
      " |          - **overflowing_tokens** -- List of overflowing tokens sequences (when a :obj:`max_length` is specified and\n",
      " |            :obj:`return_overflowing_tokens=True`).\n",
      " |          - **num_truncated_tokens** -- Number of tokens truncated (when a :obj:`max_length` is specified and\n",
      " |            :obj:`return_overflowing_tokens=True`).\n",
      " |          - **special_tokens_mask** -- List of 0s and 1s, with 1 specifying added special tokens and 0 specifying\n",
      " |            regular sequence tokens (when :obj:`add_special_tokens=True` and :obj:`return_special_tokens_mask=True`).\n",
      " |          - **length** -- The length of the inputs (when :obj:`return_length=True`)\n",
      " |  \n",
      " |  pad(self, encoded_inputs: Union[transformers.tokenization_utils_base.BatchEncoding, List[transformers.tokenization_utils_base.BatchEncoding], Dict[str, List[int]], Dict[str, List[List[int]]], List[Dict[str, List[int]]]], padding: Union[bool, str, transformers.file_utils.PaddingStrategy] = True, max_length: Union[int, NoneType] = None, pad_to_multiple_of: Union[int, NoneType] = None, return_attention_mask: Union[bool, NoneType] = None, return_tensors: Union[str, transformers.file_utils.TensorType, NoneType] = None, verbose: bool = True) -> transformers.tokenization_utils_base.BatchEncoding\n",
      " |      Pad a single encoded input or a batch of encoded inputs up to predefined length or to the max sequence length\n",
      " |      in the batch.\n",
      " |      \n",
      " |      Padding side (left/right) padding token ids are defined at the tokenizer level (with ``self.padding_side``,\n",
      " |      ``self.pad_token_id`` and ``self.pad_token_type_id``)\n",
      " |      \n",
      " |      .. note::\n",
      " |      \n",
      " |          If the ``encoded_inputs`` passed are dictionary of numpy arrays, PyTorch tensors or TensorFlow tensors, the\n",
      " |          result will use the same type unless you provide a different tensor type with ``return_tensors``. In the\n",
      " |          case of PyTorch tensors, you will lose the specific device of your tensors however.\n",
      " |      \n",
      " |      Args:\n",
      " |          encoded_inputs (:class:`~transformers.BatchEncoding`, list of :class:`~transformers.BatchEncoding`, :obj:`Dict[str, List[int]]`, :obj:`Dict[str, List[List[int]]` or :obj:`List[Dict[str, List[int]]]`):\n",
      " |              Tokenized inputs. Can represent one input (:class:`~transformers.BatchEncoding` or :obj:`Dict[str,\n",
      " |              List[int]]`) or a batch of tokenized inputs (list of :class:`~transformers.BatchEncoding`, `Dict[str,\n",
      " |              List[List[int]]]` or `List[Dict[str, List[int]]]`) so you can use this method during preprocessing as\n",
      " |              well as in a PyTorch Dataloader collate function.\n",
      " |      \n",
      " |              Instead of :obj:`List[int]` you can have tensors (numpy arrays, PyTorch tensors or TensorFlow tensors),\n",
      " |              see the note above for the return type.\n",
      " |          padding (:obj:`bool`, :obj:`str` or :class:`~transformers.file_utils.PaddingStrategy`, `optional`, defaults to :obj:`True`):\n",
      " |               Select a strategy to pad the returned sequences (according to the model's padding side and padding\n",
      " |               index) among:\n",
      " |      \n",
      " |              * :obj:`True` or :obj:`'longest'`: Pad to the longest sequence in the batch (or no padding if only a\n",
      " |                single sequence if provided).\n",
      " |              * :obj:`'max_length'`: Pad to a maximum length specified with the argument :obj:`max_length` or to the\n",
      " |                maximum acceptable input length for the model if that argument is not provided.\n",
      " |              * :obj:`False` or :obj:`'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of\n",
      " |                different lengths).\n",
      " |          max_length (:obj:`int`, `optional`):\n",
      " |              Maximum length of the returned list and optionally padding length (see above).\n",
      " |          pad_to_multiple_of (:obj:`int`, `optional`):\n",
      " |              If set will pad the sequence to a multiple of the provided value.\n",
      " |      \n",
      " |              This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability\n",
      " |              >= 7.5 (Volta).\n",
      " |          return_attention_mask (:obj:`bool`, `optional`):\n",
      " |              Whether to return the attention mask. If left to the default, will return the attention mask according\n",
      " |              to the specific tokenizer's default, defined by the :obj:`return_outputs` attribute.\n",
      " |      \n",
      " |              `What are attention masks? <../glossary.html#attention-mask>`__\n",
      " |          return_tensors (:obj:`str` or :class:`~transformers.file_utils.TensorType`, `optional`):\n",
      " |              If set, will return tensors instead of list of python integers. Acceptable values are:\n",
      " |      \n",
      " |              * :obj:`'tf'`: Return TensorFlow :obj:`tf.constant` objects.\n",
      " |              * :obj:`'pt'`: Return PyTorch :obj:`torch.Tensor` objects.\n",
      " |              * :obj:`'np'`: Return Numpy :obj:`np.ndarray` objects.\n",
      " |          verbose (:obj:`bool`, `optional`, defaults to :obj:`True`):\n",
      " |              Whether or not to print more information and warnings.\n",
      " |  \n",
      " |  prepare_for_model(self, ids: List[int], pair_ids: Union[List[int], NoneType] = None, add_special_tokens: bool = True, padding: Union[bool, str, transformers.file_utils.PaddingStrategy] = False, truncation: Union[bool, str, transformers.tokenization_utils_base.TruncationStrategy] = False, max_length: Union[int, NoneType] = None, stride: int = 0, pad_to_multiple_of: Union[int, NoneType] = None, return_tensors: Union[str, transformers.file_utils.TensorType, NoneType] = None, return_token_type_ids: Union[bool, NoneType] = None, return_attention_mask: Union[bool, NoneType] = None, return_overflowing_tokens: bool = False, return_special_tokens_mask: bool = False, return_offsets_mapping: bool = False, return_length: bool = False, verbose: bool = True, prepend_batch_axis: bool = False, **kwargs) -> transformers.tokenization_utils_base.BatchEncoding\n",
      " |      Prepares a sequence of input id, or a pair of sequences of inputs ids so that it can be used by the model. It\n",
      " |      adds special tokens, truncates sequences if overflowing while taking into account the special tokens and\n",
      " |      manages a moving window (with user defined stride) for overflowing tokens. Please Note, for `pair_ids`\n",
      " |      different than `None` and `truncation_strategy = longest_first` or `True`, it is not possible to return\n",
      " |      overflowing tokens. Such a combination of arguments will raise an error.\n",
      " |      \n",
      " |      Args:\n",
      " |          ids (:obj:`List[int]`):\n",
      " |              Tokenized input ids of the first sequence. Can be obtained from a string by chaining the ``tokenize``\n",
      " |              and ``convert_tokens_to_ids`` methods.\n",
      " |          pair_ids (:obj:`List[int]`, `optional`):\n",
      " |              Tokenized input ids of the second sequence. Can be obtained from a string by chaining the ``tokenize``\n",
      " |              and ``convert_tokens_to_ids`` methods.\n",
      " |      \n",
      " |          add_special_tokens (:obj:`bool`, `optional`, defaults to :obj:`True`):\n",
      " |              Whether or not to encode the sequences with the special tokens relative to their model.\n",
      " |          padding (:obj:`bool`, :obj:`str` or :class:`~transformers.file_utils.PaddingStrategy`, `optional`, defaults to :obj:`False`):\n",
      " |              Activates and controls padding. Accepts the following values:\n",
      " |      \n",
      " |              * :obj:`True` or :obj:`'longest'`: Pad to the longest sequence in the batch (or no padding if only a\n",
      " |                single sequence if provided).\n",
      " |              * :obj:`'max_length'`: Pad to a maximum length specified with the argument :obj:`max_length` or to the\n",
      " |                maximum acceptable input length for the model if that argument is not provided.\n",
      " |              * :obj:`False` or :obj:`'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of\n",
      " |                different lengths).\n",
      " |          truncation (:obj:`bool`, :obj:`str` or :class:`~transformers.tokenization_utils_base.TruncationStrategy`, `optional`, defaults to :obj:`False`):\n",
      " |              Activates and controls truncation. Accepts the following values:\n",
      " |      \n",
      " |              * :obj:`True` or :obj:`'longest_first'`: Truncate to a maximum length specified with the argument\n",
      " |                :obj:`max_length` or to the maximum acceptable input length for the model if that argument is not\n",
      " |                provided. This will truncate token by token, removing a token from the longest sequence in the pair\n",
      " |                if a pair of sequences (or a batch of pairs) is provided.\n",
      " |              * :obj:`'only_first'`: Truncate to a maximum length specified with the argument :obj:`max_length` or to\n",
      " |                the maximum acceptable input length for the model if that argument is not provided. This will only\n",
      " |                truncate the first sequence of a pair if a pair of sequences (or a batch of pairs) is provided.\n",
      " |              * :obj:`'only_second'`: Truncate to a maximum length specified with the argument :obj:`max_length` or\n",
      " |                to the maximum acceptable input length for the model if that argument is not provided. This will only\n",
      " |                truncate the second sequence of a pair if a pair of sequences (or a batch of pairs) is provided.\n",
      " |              * :obj:`False` or :obj:`'do_not_truncate'` (default): No truncation (i.e., can output batch with\n",
      " |                sequence lengths greater than the model maximum admissible input size).\n",
      " |          max_length (:obj:`int`, `optional`):\n",
      " |              Controls the maximum length to use by one of the truncation/padding parameters.\n",
      " |      \n",
      " |              If left unset or set to :obj:`None`, this will use the predefined model maximum length if a maximum\n",
      " |              length is required by one of the truncation/padding parameters. If the model has no specific maximum\n",
      " |              input length (like XLNet) truncation/padding to a maximum length will be deactivated.\n",
      " |          stride (:obj:`int`, `optional`, defaults to 0):\n",
      " |              If set to a number along with :obj:`max_length`, the overflowing tokens returned when\n",
      " |              :obj:`return_overflowing_tokens=True` will contain some tokens from the end of the truncated sequence\n",
      " |              returned to provide some overlap between truncated and overflowing sequences. The value of this\n",
      " |              argument defines the number of overlapping tokens.\n",
      " |          is_split_into_words (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      " |              Whether or not the input is already pre-tokenized (e.g., split into words). If set to :obj:`True`, the\n",
      " |              tokenizer assumes the input is already split into words (for instance, by splitting it on whitespace)\n",
      " |              which it will tokenize. This is useful for NER or token classification.\n",
      " |          pad_to_multiple_of (:obj:`int`, `optional`):\n",
      " |              If set will pad the sequence to a multiple of the provided value. This is especially useful to enable\n",
      " |              the use of Tensor Cores on NVIDIA hardware with compute capability >= 7.5 (Volta).\n",
      " |          return_tensors (:obj:`str` or :class:`~transformers.file_utils.TensorType`, `optional`):\n",
      " |              If set, will return tensors instead of list of python integers. Acceptable values are:\n",
      " |      \n",
      " |              * :obj:`'tf'`: Return TensorFlow :obj:`tf.constant` objects.\n",
      " |              * :obj:`'pt'`: Return PyTorch :obj:`torch.Tensor` objects.\n",
      " |              * :obj:`'np'`: Return Numpy :obj:`np.ndarray` objects.\n",
      " |      \n",
      " |          return_token_type_ids (:obj:`bool`, `optional`):\n",
      " |              Whether to return token type IDs. If left to the default, will return the token type IDs according to\n",
      " |              the specific tokenizer's default, defined by the :obj:`return_outputs` attribute.\n",
      " |      \n",
      " |              `What are token type IDs? <../glossary.html#token-type-ids>`__\n",
      " |          return_attention_mask (:obj:`bool`, `optional`):\n",
      " |              Whether to return the attention mask. If left to the default, will return the attention mask according\n",
      " |              to the specific tokenizer's default, defined by the :obj:`return_outputs` attribute.\n",
      " |      \n",
      " |              `What are attention masks? <../glossary.html#attention-mask>`__\n",
      " |          return_overflowing_tokens (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      " |              Whether or not to return overflowing token sequences. If a pair of sequences of input ids (or a batch\n",
      " |              of pairs) is provided with :obj:`truncation_strategy = longest_first` or :obj:`True`, an error is\n",
      " |              raised instead of returning overflowing tokens.\n",
      " |          return_special_tokens_mask (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      " |              Whether or not to return special tokens mask information.\n",
      " |          return_offsets_mapping (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      " |              Whether or not to return :obj:`(char_start, char_end)` for each token.\n",
      " |      \n",
      " |              This is only available on fast tokenizers inheriting from\n",
      " |              :class:`~transformers.PreTrainedTokenizerFast`, if using Python's tokenizer, this method will raise\n",
      " |              :obj:`NotImplementedError`.\n",
      " |          return_length  (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      " |              Whether or not to return the lengths of the encoded inputs.\n",
      " |          verbose (:obj:`bool`, `optional`, defaults to :obj:`True`):\n",
      " |              Whether or not to print more information and warnings.\n",
      " |          **kwargs: passed to the :obj:`self.tokenize()` method\n",
      " |      \n",
      " |      Return:\n",
      " |          :class:`~transformers.BatchEncoding`: A :class:`~transformers.BatchEncoding` with the following fields:\n",
      " |      \n",
      " |          - **input_ids** -- List of token ids to be fed to a model.\n",
      " |      \n",
      " |            `What are input IDs? <../glossary.html#input-ids>`__\n",
      " |      \n",
      " |          - **token_type_ids** -- List of token type ids to be fed to a model (when :obj:`return_token_type_ids=True`\n",
      " |            or if `\"token_type_ids\"` is in :obj:`self.model_input_names`).\n",
      " |      \n",
      " |            `What are token type IDs? <../glossary.html#token-type-ids>`__\n",
      " |      \n",
      " |          - **attention_mask** -- List of indices specifying which tokens should be attended to by the model (when\n",
      " |            :obj:`return_attention_mask=True` or if `\"attention_mask\"` is in :obj:`self.model_input_names`).\n",
      " |      \n",
      " |            `What are attention masks? <../glossary.html#attention-mask>`__\n",
      " |      \n",
      " |          - **overflowing_tokens** -- List of overflowing tokens sequences (when a :obj:`max_length` is specified and\n",
      " |            :obj:`return_overflowing_tokens=True`).\n",
      " |          - **num_truncated_tokens** -- Number of tokens truncated (when a :obj:`max_length` is specified and\n",
      " |            :obj:`return_overflowing_tokens=True`).\n",
      " |          - **special_tokens_mask** -- List of 0s and 1s, with 1 specifying added special tokens and 0 specifying\n",
      " |            regular sequence tokens (when :obj:`add_special_tokens=True` and :obj:`return_special_tokens_mask=True`).\n",
      " |          - **length** -- The length of the inputs (when :obj:`return_length=True`)\n",
      " |  \n",
      " |  prepare_seq2seq_batch(self, src_texts: List[str], tgt_texts: Union[List[str], NoneType] = None, max_length: Union[int, NoneType] = None, max_target_length: Union[int, NoneType] = None, padding: str = 'longest', return_tensors: str = None, truncation: bool = True, **kwargs) -> transformers.tokenization_utils_base.BatchEncoding\n",
      " |      Prepare model inputs for translation. For best performance, translate one sentence at a time.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          src_texts (:obj:`List[str]`):\n",
      " |              List of documents to summarize or source language texts.\n",
      " |          tgt_texts (:obj:`list`, `optional`):\n",
      " |              List of summaries or target language texts.\n",
      " |          max_length (:obj:`int`, `optional`):\n",
      " |              Controls the maximum length for encoder inputs (documents to summarize or source language texts) If\n",
      " |              left unset or set to :obj:`None`, this will use the predefined model maximum length if a maximum length\n",
      " |              is required by one of the truncation/padding parameters. If the model has no specific maximum input\n",
      " |              length (like XLNet) truncation/padding to a maximum length will be deactivated.\n",
      " |          max_target_length (:obj:`int`, `optional`):\n",
      " |              Controls the maximum length of decoder inputs (target language texts or summaries) If left unset or set\n",
      " |              to :obj:`None`, this will use the max_length value.\n",
      " |          padding (:obj:`bool`, :obj:`str` or :class:`~transformers.file_utils.PaddingStrategy`, `optional`, defaults to :obj:`False`):\n",
      " |              Activates and controls padding. Accepts the following values:\n",
      " |      \n",
      " |              * :obj:`True` or :obj:`'longest'`: Pad to the longest sequence in the batch (or no padding if only a\n",
      " |                single sequence if provided).\n",
      " |              * :obj:`'max_length'`: Pad to a maximum length specified with the argument :obj:`max_length` or to the\n",
      " |                maximum acceptable input length for the model if that argument is not provided.\n",
      " |              * :obj:`False` or :obj:`'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of\n",
      " |                different lengths).\n",
      " |          return_tensors (:obj:`str` or :class:`~transformers.file_utils.TensorType`, `optional`):\n",
      " |              If set, will return tensors instead of list of python integers. Acceptable values are:\n",
      " |      \n",
      " |              * :obj:`'tf'`: Return TensorFlow :obj:`tf.constant` objects.\n",
      " |              * :obj:`'pt'`: Return PyTorch :obj:`torch.Tensor` objects.\n",
      " |              * :obj:`'np'`: Return Numpy :obj:`np.ndarray` objects.\n",
      " |          truncation (:obj:`bool`, :obj:`str` or :class:`~transformers.tokenization_utils_base.TruncationStrategy`, `optional`, defaults to :obj:`True`):\n",
      " |              Activates and controls truncation. Accepts the following values:\n",
      " |      \n",
      " |              * :obj:`True` or :obj:`'longest_first'`: Truncate to a maximum length specified with the argument\n",
      " |                :obj:`max_length` or to the maximum acceptable input length for the model if that argument is not\n",
      " |                provided. This will truncate token by token, removing a token from the longest sequence in the pair\n",
      " |                if a pair of sequences (or a batch of pairs) is provided.\n",
      " |              * :obj:`'only_first'`: Truncate to a maximum length specified with the argument :obj:`max_length` or to\n",
      " |                the maximum acceptable input length for the model if that argument is not provided. This will only\n",
      " |                truncate the first sequence of a pair if a pair of sequences (or a batch of pairs) is provided.\n",
      " |              * :obj:`'only_second'`: Truncate to a maximum length specified with the argument :obj:`max_length` or\n",
      " |                to the maximum acceptable input length for the model if that argument is not provided. This will only\n",
      " |                truncate the second sequence of a pair if a pair of sequences (or a batch of pairs) is provided.\n",
      " |              * :obj:`False` or :obj:`'do_not_truncate'` (default): No truncation (i.e., can output batch with\n",
      " |                sequence lengths greater than the model maximum admissible input size).\n",
      " |          **kwargs:\n",
      " |              Additional keyword arguments passed along to :obj:`self.__call__`.\n",
      " |      \n",
      " |      Return:\n",
      " |          :class:`~transformers.BatchEncoding`: A :class:`~transformers.BatchEncoding` with the following fields:\n",
      " |      \n",
      " |          - **input_ids** -- List of token ids to be fed to the encoder.\n",
      " |          - **attention_mask** -- List of indices specifying which tokens should be attended to by the model.\n",
      " |          - **labels** -- List of token ids for tgt_texts.\n",
      " |      \n",
      " |          The full set of keys ``[input_ids, attention_mask, labels]``, will only be returned if tgt_texts is passed.\n",
      " |          Otherwise, input_ids, attention_mask will be the only keys.\n",
      " |  \n",
      " |  push_to_hub(self, repo_path_or_name: Union[str, NoneType] = None, repo_url: Union[str, NoneType] = None, use_temp_dir: bool = False, commit_message: Union[str, NoneType] = None, organization: Union[str, NoneType] = None, private: Union[bool, NoneType] = None, use_auth_token: Union[bool, str, NoneType] = None) -> str\n",
      " |      Upload the tokenizer files to the 🤗 Model Hub while synchronizing a local clone of the repo in\n",
      " |      :obj:`repo_path_or_name`.\n",
      " |      \n",
      " |      Parameters:\n",
      " |          repo_path_or_name (:obj:`str`, `optional`):\n",
      " |              Can either be a repository name for your tokenizer in the Hub or a path to a local folder (in which case\n",
      " |              the repository will have the name of that local folder). If not specified, will default to the name\n",
      " |              given by :obj:`repo_url` and a local directory with that name will be created.\n",
      " |          repo_url (:obj:`str`, `optional`):\n",
      " |              Specify this in case you want to push to an existing repository in the hub. If unspecified, a new\n",
      " |              repository will be created in your namespace (unless you specify an :obj:`organization`) with\n",
      " |              :obj:`repo_name`.\n",
      " |          use_temp_dir (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      " |              Whether or not to clone the distant repo in a temporary directory or in :obj:`repo_path_or_name` inside\n",
      " |              the current working directory. This will slow things down if you are making changes in an existing repo\n",
      " |              since you will need to clone the repo before every push.\n",
      " |          commit_message (:obj:`str`, `optional`):\n",
      " |              Message to commit while pushing. Will default to :obj:`\"add tokenizer\"`.\n",
      " |          organization (:obj:`str`, `optional`):\n",
      " |              Organization in which you want to push your tokenizer (you must be a member of this organization).\n",
      " |          private (:obj:`bool`, `optional`):\n",
      " |              Whether or not the repository created should be private (requires a paying subscription).\n",
      " |          use_auth_token (:obj:`bool` or :obj:`str`, `optional`):\n",
      " |              The token to use as HTTP bearer authorization for remote files. If :obj:`True`, will use the token\n",
      " |              generated when running :obj:`transformers-cli login` (stored in :obj:`~/.huggingface`). Will default to\n",
      " |              :obj:`True` if :obj:`repo_url` is not specified.\n",
      " |      \n",
      " |      \n",
      " |      Returns:\n",
      " |          :obj:`str`: The url of the commit of your tokenizer in the given repository.\n",
      " |      \n",
      " |      Examples::\n",
      " |      \n",
      " |          from transformers import AutoTokenizer\n",
      " |      \n",
      " |          tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
      " |      \n",
      " |          # Push the tokenizer to your namespace with the name \"my-finetuned-bert\" and have a local clone in the\n",
      " |          # `my-finetuned-bert` folder.\n",
      " |          tokenizer.push_to_hub(\"my-finetuned-bert\")\n",
      " |      \n",
      " |          # Push the tokenizer to your namespace with the name \"my-finetuned-bert\" with no local clone.\n",
      " |          tokenizer.push_to_hub(\"my-finetuned-bert\", use_temp_dir=True)\n",
      " |      \n",
      " |          # Push the tokenizer to an organization with the name \"my-finetuned-bert\" and have a local clone in the\n",
      " |          # `my-finetuned-bert` folder.\n",
      " |          tokenizer.push_to_hub(\"my-finetuned-bert\", organization=\"huggingface\")\n",
      " |      \n",
      " |          # Make a change to an existing repo that has been cloned locally in `my-finetuned-bert`.\n",
      " |          tokenizer.push_to_hub(\"my-finetuned-bert\", repo_url=\"https://huggingface.co/sgugger/my-finetuned-bert\")\n",
      " |  \n",
      " |  save_pretrained(self, save_directory: Union[str, os.PathLike], legacy_format: Union[bool, NoneType] = None, filename_prefix: Union[str, NoneType] = None, push_to_hub: bool = False, **kwargs) -> Tuple[str]\n",
      " |      Save the full tokenizer state.\n",
      " |      \n",
      " |      \n",
      " |      This method make sure the full tokenizer can then be re-loaded using the\n",
      " |      :meth:`~transformers.tokenization_utils_base.PreTrainedTokenizer.from_pretrained` class method..\n",
      " |      \n",
      " |      .. Warning::\n",
      " |         This won't save modifications you may have applied to the tokenizer after the instantiation (for instance,\n",
      " |         modifying :obj:`tokenizer.do_lower_case` after creation).\n",
      " |      \n",
      " |      Args:\n",
      " |          save_directory (:obj:`str` or :obj:`os.PathLike`): The path to a directory where the tokenizer will be saved.\n",
      " |          legacy_format (:obj:`bool`, `optional`):\n",
      " |              Only applicable for a fast tokenizer. If unset (default), will save the tokenizer in the unified JSON\n",
      " |              format as well as in legacy format if it exists, i.e. with tokenizer specific vocabulary and a separate\n",
      " |              added_tokens files.\n",
      " |      \n",
      " |              If :obj:`False`, will only save the tokenizer in the unified JSON format. This format is incompatible\n",
      " |              with \"slow\" tokenizers (not powered by the `tokenizers` library), so the tokenizer will not be able to\n",
      " |              be loaded in the corresponding \"slow\" tokenizer.\n",
      " |      \n",
      " |              If :obj:`True`, will save the tokenizer in legacy format. If the \"slow\" tokenizer doesn't exits, a\n",
      " |              value error is raised.\n",
      " |          filename_prefix: (:obj:`str`, `optional`):\n",
      " |              A prefix to add to the names of the files saved by the tokenizer.\n",
      " |          push_to_hub (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      " |              Whether or not to push your model to the Hugging Face model hub after saving it.\n",
      " |      \n",
      " |              .. warning::\n",
      " |      \n",
      " |                  Using :obj:`push_to_hub=True` will synchronize the repository you are pushing to with\n",
      " |                  :obj:`save_directory`, which requires :obj:`save_directory` to be a local clone of the repo you are\n",
      " |                  pushing to if it's an existing folder. Pass along :obj:`temp_dir=True` to use a temporary directory\n",
      " |                  instead.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A tuple of :obj:`str`: The files saved.\n",
      " |  \n",
      " |  truncate_sequences(self, ids: List[int], pair_ids: Union[List[int], NoneType] = None, num_tokens_to_remove: int = 0, truncation_strategy: Union[str, transformers.tokenization_utils_base.TruncationStrategy] = 'longest_first', stride: int = 0) -> Tuple[List[int], List[int], List[int]]\n",
      " |      Truncates a sequence pair in-place following the strategy.\n",
      " |      \n",
      " |      Args:\n",
      " |          ids (:obj:`List[int]`):\n",
      " |              Tokenized input ids of the first sequence. Can be obtained from a string by chaining the ``tokenize``\n",
      " |              and ``convert_tokens_to_ids`` methods.\n",
      " |          pair_ids (:obj:`List[int]`, `optional`):\n",
      " |              Tokenized input ids of the second sequence. Can be obtained from a string by chaining the ``tokenize``\n",
      " |              and ``convert_tokens_to_ids`` methods.\n",
      " |          num_tokens_to_remove (:obj:`int`, `optional`, defaults to 0):\n",
      " |              Number of tokens to remove using the truncation strategy.\n",
      " |          truncation_strategy (:obj:`str` or :class:`~transformers.tokenization_utils_base.TruncationStrategy`, `optional`, defaults to :obj:`False`):\n",
      " |              The strategy to follow for truncation. Can be:\n",
      " |      \n",
      " |              * :obj:`'longest_first'`: Truncate to a maximum length specified with the argument :obj:`max_length` or\n",
      " |                to the maximum acceptable input length for the model if that argument is not provided. This will\n",
      " |                truncate token by token, removing a token from the longest sequence in the pair if a pair of\n",
      " |                sequences (or a batch of pairs) is provided.\n",
      " |              * :obj:`'only_first'`: Truncate to a maximum length specified with the argument :obj:`max_length` or to\n",
      " |                the maximum acceptable input length for the model if that argument is not provided. This will only\n",
      " |                truncate the first sequence of a pair if a pair of sequences (or a batch of pairs) is provided.\n",
      " |              * :obj:`'only_second'`: Truncate to a maximum length specified with the argument :obj:`max_length` or\n",
      " |                to the maximum acceptable input length for the model if that argument is not provided. This will only\n",
      " |                truncate the second sequence of a pair if a pair of sequences (or a batch of pairs) is provided.\n",
      " |              * :obj:`'do_not_truncate'` (default): No truncation (i.e., can output batch with sequence lengths\n",
      " |                greater than the model maximum admissible input size).\n",
      " |          stride (:obj:`int`, `optional`, defaults to 0):\n",
      " |              If set to a positive number, the overflowing tokens returned will contain some tokens from the main\n",
      " |              sequence returned. The value of this argument defines the number of additional tokens.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :obj:`Tuple[List[int], List[int], List[int]]`: The truncated ``ids``, the truncated ``pair_ids`` and the\n",
      " |          list of overflowing tokens. Note: The `longest_first` strategy returns empty list of overflowing_tokens if\n",
      " |          a pair of sequences (or a batch of pairs) is provided.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from transformers.tokenization_utils_base.PreTrainedTokenizerBase:\n",
      " |  \n",
      " |  from_pretrained(pretrained_model_name_or_path: Union[str, os.PathLike], *init_inputs, **kwargs) from builtins.type\n",
      " |      Instantiate a :class:`~transformers.tokenization_utils_base.PreTrainedTokenizerBase` (or a derived class) from\n",
      " |      a predefined tokenizer.\n",
      " |      \n",
      " |      Args:\n",
      " |          pretrained_model_name_or_path (:obj:`str` or :obj:`os.PathLike`):\n",
      " |              Can be either:\n",
      " |      \n",
      " |              - A string, the `model id` of a predefined tokenizer hosted inside a model repo on huggingface.co.\n",
      " |                Valid model ids can be located at the root-level, like ``bert-base-uncased``, or namespaced under a\n",
      " |                user or organization name, like ``dbmdz/bert-base-german-cased``.\n",
      " |              - A path to a `directory` containing vocabulary files required by the tokenizer, for instance saved\n",
      " |                using the :meth:`~transformers.tokenization_utils_base.PreTrainedTokenizerBase.save_pretrained`\n",
      " |                method, e.g., ``./my_model_directory/``.\n",
      " |              - (**Deprecated**, not applicable to all derived classes) A path or url to a single saved vocabulary\n",
      " |                file (if and only if the tokenizer only requires a single vocabulary file like Bert or XLNet), e.g.,\n",
      " |                ``./my_model_directory/vocab.txt``.\n",
      " |          cache_dir (:obj:`str` or :obj:`os.PathLike`, `optional`):\n",
      " |              Path to a directory in which a downloaded predefined tokenizer vocabulary files should be cached if the\n",
      " |              standard cache should not be used.\n",
      " |          force_download (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      " |              Whether or not to force the (re-)download the vocabulary files and override the cached versions if they\n",
      " |              exist.\n",
      " |          resume_download (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      " |              Whether or not to delete incompletely received files. Attempt to resume the download if such a file\n",
      " |              exists.\n",
      " |          proxies (:obj:`Dict[str, str], `optional`):\n",
      " |              A dictionary of proxy servers to use by protocol or endpoint, e.g., :obj:`{'http': 'foo.bar:3128',\n",
      " |              'http://hostname': 'foo.bar:4012'}`. The proxies are used on each request.\n",
      " |          use_auth_token (:obj:`str` or `bool`, `optional`):\n",
      " |              The token to use as HTTP bearer authorization for remote files. If :obj:`True`, will use the token\n",
      " |              generated when running :obj:`transformers-cli login` (stored in :obj:`~/.huggingface`).\n",
      " |          local_files_only (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      " |              Whether or not to only rely on local files and not to attempt to download any files.\n",
      " |          revision(:obj:`str`, `optional`, defaults to :obj:`\"main\"`):\n",
      " |              The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a\n",
      " |              git-based system for storing models and other artifacts on huggingface.co, so ``revision`` can be any\n",
      " |              identifier allowed by git.\n",
      " |          subfolder (:obj:`str`, `optional`):\n",
      " |              In case the relevant files are located inside a subfolder of the model repo on huggingface.co (e.g. for\n",
      " |              facebook/rag-token-base), specify it here.\n",
      " |          inputs (additional positional arguments, `optional`):\n",
      " |              Will be passed along to the Tokenizer ``__init__`` method.\n",
      " |          kwargs (additional keyword arguments, `optional`):\n",
      " |              Will be passed to the Tokenizer ``__init__`` method. Can be used to set special tokens like\n",
      " |              ``bos_token``, ``eos_token``, ``unk_token``, ``sep_token``, ``pad_token``, ``cls_token``,\n",
      " |              ``mask_token``, ``additional_special_tokens``. See parameters in the ``__init__`` for more details.\n",
      " |      \n",
      " |      .. note::\n",
      " |      \n",
      " |          Passing :obj:`use_auth_token=True` is required when you want to use a private model.\n",
      " |      \n",
      " |      Examples::\n",
      " |      \n",
      " |          # We can't instantiate directly the base class `PreTrainedTokenizerBase` so let's show our examples on a derived class: BertTokenizer\n",
      " |          # Download vocabulary from huggingface.co and cache.\n",
      " |          tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
      " |      \n",
      " |          # Download vocabulary from huggingface.co (user-uploaded) and cache.\n",
      " |          tokenizer = BertTokenizer.from_pretrained('dbmdz/bert-base-german-cased')\n",
      " |      \n",
      " |          # If vocabulary files are in a directory (e.g. tokenizer was saved using `save_pretrained('./test/saved_model/')`)\n",
      " |          tokenizer = BertTokenizer.from_pretrained('./test/saved_model/')\n",
      " |      \n",
      " |          # If the tokenizer uses a single vocabulary file, you can point directly to this file\n",
      " |          tokenizer = BertTokenizer.from_pretrained('./test/saved_model/my_vocab.txt')\n",
      " |      \n",
      " |          # You can link tokens to special vocabulary when instantiating\n",
      " |          tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', unk_token='<unk>')\n",
      " |          # You should be sure '<unk>' is in the vocabulary when doing that.\n",
      " |          # Otherwise use tokenizer.add_special_tokens({'unk_token': '<unk>'}) instead)\n",
      " |          assert tokenizer.unk_token == '<unk>'\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods inherited from transformers.tokenization_utils_base.PreTrainedTokenizerBase:\n",
      " |  \n",
      " |  clean_up_tokenization(out_string: str) -> str\n",
      " |      Clean up a list of simple English tokenization artifacts like spaces before punctuations and abbreviated forms.\n",
      " |      \n",
      " |      Args:\n",
      " |          out_string (:obj:`str`): The text to clean up.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :obj:`str`: The cleaned-up string.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from transformers.tokenization_utils_base.PreTrainedTokenizerBase:\n",
      " |  \n",
      " |  max_len_sentences_pair\n",
      " |      :obj:`int`: The maximum combined length of a pair of sentences that can be fed to the model.\n",
      " |  \n",
      " |  max_len_single_sentence\n",
      " |      :obj:`int`: The maximum length of a sentence that can be fed to the model.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from transformers.tokenization_utils_base.PreTrainedTokenizerBase:\n",
      " |  \n",
      " |  __annotations__ = {'max_model_input_sizes': typing.Dict[str, typing.Un...\n",
      " |  \n",
      " |  padding_side = 'right'\n",
      " |  \n",
      " |  pretrained_init_configuration = {}\n",
      " |  \n",
      " |  slow_tokenizer_class = None\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from transformers.tokenization_utils_base.SpecialTokensMixin:\n",
      " |  \n",
      " |  add_special_tokens(self, special_tokens_dict: Dict[str, Union[str, tokenizers.AddedToken]]) -> int\n",
      " |      Add a dictionary of special tokens (eos, pad, cls, etc.) to the encoder and link them to class attributes. If\n",
      " |      special tokens are NOT in the vocabulary, they are added to it (indexed starting from the last index of the\n",
      " |      current vocabulary).\n",
      " |      \n",
      " |      .. Note::\n",
      " |          When adding new tokens to the vocabulary, you should make sure to also resize the token embedding matrix of\n",
      " |          the model so that its embedding matrix matches the tokenizer.\n",
      " |      \n",
      " |          In order to do that, please use the :meth:`~transformers.PreTrainedModel.resize_token_embeddings` method.\n",
      " |      \n",
      " |      Using :obj:`add_special_tokens` will ensure your special tokens can be used in several ways:\n",
      " |      \n",
      " |      - Special tokens are carefully handled by the tokenizer (they are never split).\n",
      " |      - You can easily refer to special tokens using tokenizer class attributes like :obj:`tokenizer.cls_token`. This\n",
      " |        makes it easy to develop model-agnostic training and fine-tuning scripts.\n",
      " |      \n",
      " |      When possible, special tokens are already registered for provided pretrained models (for instance\n",
      " |      :class:`~transformers.BertTokenizer` :obj:`cls_token` is already registered to be :obj`'[CLS]'` and XLM's one\n",
      " |      is also registered to be :obj:`'</s>'`).\n",
      " |      \n",
      " |      Args:\n",
      " |          special_tokens_dict (dictionary `str` to `str` or :obj:`tokenizers.AddedToken`):\n",
      " |              Keys should be in the list of predefined special attributes: [``bos_token``, ``eos_token``,\n",
      " |              ``unk_token``, ``sep_token``, ``pad_token``, ``cls_token``, ``mask_token``,\n",
      " |              ``additional_special_tokens``].\n",
      " |      \n",
      " |              Tokens are only added if they are not already in the vocabulary (tested by checking if the tokenizer\n",
      " |              assign the index of the ``unk_token`` to them).\n",
      " |      \n",
      " |      Returns:\n",
      " |          :obj:`int`: Number of tokens added to the vocabulary.\n",
      " |      \n",
      " |      Examples::\n",
      " |      \n",
      " |          # Let's see how to add a new classification token to GPT-2\n",
      " |          tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
      " |          model = GPT2Model.from_pretrained('gpt2')\n",
      " |      \n",
      " |          special_tokens_dict = {'cls_token': '<CLS>'}\n",
      " |      \n",
      " |          num_added_toks = tokenizer.add_special_tokens(special_tokens_dict)\n",
      " |          print('We have added', num_added_toks, 'tokens')\n",
      " |          # Notice: resize_token_embeddings expect to receive the full size of the new vocabulary, i.e., the length of the tokenizer.\n",
      " |          model.resize_token_embeddings(len(tokenizer))\n",
      " |      \n",
      " |          assert tokenizer.cls_token == '<CLS>'\n",
      " |  \n",
      " |  add_tokens(self, new_tokens: Union[str, tokenizers.AddedToken, List[Union[str, tokenizers.AddedToken]]], special_tokens: bool = False) -> int\n",
      " |      Add a list of new tokens to the tokenizer class. If the new tokens are not in the vocabulary, they are added to\n",
      " |      it with indices starting from length of the current vocabulary.\n",
      " |      \n",
      " |      .. Note::\n",
      " |          When adding new tokens to the vocabulary, you should make sure to also resize the token embedding matrix of\n",
      " |          the model so that its embedding matrix matches the tokenizer.\n",
      " |      \n",
      " |          In order to do that, please use the :meth:`~transformers.PreTrainedModel.resize_token_embeddings` method.\n",
      " |      \n",
      " |      Args:\n",
      " |          new_tokens (:obj:`str`, :obj:`tokenizers.AddedToken` or a list of `str` or :obj:`tokenizers.AddedToken`):\n",
      " |              Tokens are only added if they are not already in the vocabulary. :obj:`tokenizers.AddedToken` wraps a\n",
      " |              string token to let you personalize its behavior: whether this token should only match against a single\n",
      " |              word, whether this token should strip all potential whitespaces on the left side, whether this token\n",
      " |              should strip all potential whitespaces on the right side, etc.\n",
      " |          special_tokens (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      " |              Can be used to specify if the token is a special token. This mostly change the normalization behavior\n",
      " |              (special tokens like CLS or [MASK] are usually not lower-cased for instance).\n",
      " |      \n",
      " |              See details for :obj:`tokenizers.AddedToken` in HuggingFace tokenizers library.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :obj:`int`: Number of tokens added to the vocabulary.\n",
      " |      \n",
      " |      Examples::\n",
      " |      \n",
      " |          # Let's see how to increase the vocabulary of Bert model and tokenizer\n",
      " |          tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
      " |          model = BertModel.from_pretrained('bert-base-uncased')\n",
      " |      \n",
      " |          num_added_toks = tokenizer.add_tokens(['new_tok1', 'my_new-tok2'])\n",
      " |          print('We have added', num_added_toks, 'tokens')\n",
      " |           # Notice: resize_token_embeddings expect to receive the full size of the new vocabulary, i.e., the length of the tokenizer.\n",
      " |          model.resize_token_embeddings(len(tokenizer))\n",
      " |  \n",
      " |  sanitize_special_tokens(self) -> int\n",
      " |      Make sure that all the special tokens attributes of the tokenizer (:obj:`tokenizer.mask_token`,\n",
      " |      :obj:`tokenizer.cls_token`, etc.) are in the vocabulary.\n",
      " |      \n",
      " |      Add the missing ones to the vocabulary if needed.\n",
      " |      \n",
      " |      Return:\n",
      " |          :obj:`int`: The number of tokens added in the vocabulary during the operation.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from transformers.tokenization_utils_base.SpecialTokensMixin:\n",
      " |  \n",
      " |  all_special_ids\n",
      " |      :obj:`List[int]`: List the ids of the special tokens(:obj:`'<unk>'`, :obj:`'<cls>'`, etc.) mapped to class\n",
      " |      attributes.\n",
      " |  \n",
      " |  all_special_tokens\n",
      " |      :obj:`List[str]`: All the special tokens (:obj:`'<unk>'`, :obj:`'<cls>'`, etc.) mapped to class attributes.\n",
      " |      \n",
      " |      Convert tokens of :obj:`tokenizers.AddedToken` type to string.\n",
      " |  \n",
      " |  all_special_tokens_extended\n",
      " |      :obj:`List[Union[str, tokenizers.AddedToken]]`: All the special tokens (:obj:`'<unk>'`, :obj:`'<cls>'`, etc.)\n",
      " |      mapped to class attributes.\n",
      " |      \n",
      " |      Don't convert tokens of :obj:`tokenizers.AddedToken` type to string so they can be used to control more finely\n",
      " |      how special tokens are tokenized.\n",
      " |  \n",
      " |  pad_token_type_id\n",
      " |      :obj:`int`: Id of the padding token type in the vocabulary.\n",
      " |  \n",
      " |  special_tokens_map\n",
      " |      :obj:`Dict[str, Union[str, List[str]]]`: A dictionary mapping special token class attributes (:obj:`cls_token`,\n",
      " |      :obj:`unk_token`, etc.) to their values (:obj:`'<unk>'`, :obj:`'<cls>'`, etc.).\n",
      " |      \n",
      " |      Convert potential tokens of :obj:`tokenizers.AddedToken` type to string.\n",
      " |  \n",
      " |  special_tokens_map_extended\n",
      " |      :obj:`Dict[str, Union[str, tokenizers.AddedToken, List[Union[str, tokenizers.AddedToken]]]]`: A dictionary\n",
      " |      mapping special token class attributes (:obj:`cls_token`, :obj:`unk_token`, etc.) to their values\n",
      " |      (:obj:`'<unk>'`, :obj:`'<cls>'`, etc.).\n",
      " |      \n",
      " |      Don't convert tokens of :obj:`tokenizers.AddedToken` type to string so they can be used to control more finely\n",
      " |      how special tokens are tokenized.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from transformers.tokenization_utils_base.SpecialTokensMixin:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  additional_special_tokens\n",
      " |      :obj:`List[str]`: All the additional special tokens you may want to use. Log an error if used while not having\n",
      " |      been set.\n",
      " |  \n",
      " |  additional_special_tokens_ids\n",
      " |      :obj:`List[int]`: Ids of all the additional special tokens in the vocabulary. Log an error if used while not\n",
      " |      having been set.\n",
      " |  \n",
      " |  bos_token\n",
      " |      :obj:`str`: Beginning of sentence token. Log an error if used while not having been set.\n",
      " |  \n",
      " |  bos_token_id\n",
      " |      :obj:`Optional[int]`: Id of the beginning of sentence token in the vocabulary. Returns :obj:`None` if the token\n",
      " |      has not been set.\n",
      " |  \n",
      " |  cls_token\n",
      " |      :obj:`str`: Classification token, to extract a summary of an input sequence leveraging self-attention along the\n",
      " |      full depth of the model. Log an error if used while not having been set.\n",
      " |  \n",
      " |  cls_token_id\n",
      " |      :obj:`Optional[int]`: Id of the classification token in the vocabulary, to extract a summary of an input\n",
      " |      sequence leveraging self-attention along the full depth of the model.\n",
      " |      \n",
      " |      Returns :obj:`None` if the token has not been set.\n",
      " |  \n",
      " |  eos_token\n",
      " |      :obj:`str`: End of sentence token. Log an error if used while not having been set.\n",
      " |  \n",
      " |  eos_token_id\n",
      " |      :obj:`Optional[int]`: Id of the end of sentence token in the vocabulary. Returns :obj:`None` if the token has\n",
      " |      not been set.\n",
      " |  \n",
      " |  mask_token\n",
      " |      :obj:`str`: Mask token, to use when training a model with masked-language modeling. Log an error if used while\n",
      " |      not having been set.\n",
      " |  \n",
      " |  mask_token_id\n",
      " |      :obj:`Optional[int]`: Id of the mask token in the vocabulary, used when training a model with masked-language\n",
      " |      modeling. Returns :obj:`None` if the token has not been set.\n",
      " |  \n",
      " |  pad_token\n",
      " |      :obj:`str`: Padding token. Log an error if used while not having been set.\n",
      " |  \n",
      " |  pad_token_id\n",
      " |      :obj:`Optional[int]`: Id of the padding token in the vocabulary. Returns :obj:`None` if the token has not been\n",
      " |      set.\n",
      " |  \n",
      " |  sep_token\n",
      " |      :obj:`str`: Separation token, to separate context and query in an input sequence. Log an error if used while\n",
      " |      not having been set.\n",
      " |  \n",
      " |  sep_token_id\n",
      " |      :obj:`Optional[int]`: Id of the separation token in the vocabulary, to separate context and query in an input\n",
      " |      sequence. Returns :obj:`None` if the token has not been set.\n",
      " |  \n",
      " |  unk_token\n",
      " |      :obj:`str`: Unknown token. Log an error if used while not having been set.\n",
      " |  \n",
      " |  unk_token_id\n",
      " |      :obj:`Optional[int]`: Id of the unknown token in the vocabulary. Returns :obj:`None` if the token has not been\n",
      " |      set.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from transformers.tokenization_utils_base.SpecialTokensMixin:\n",
      " |  \n",
      " |  SPECIAL_TOKENS_ATTRIBUTES = ['bos_token', 'eos_token', 'unk_token', 's...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-03T10:25:06.385577Z",
     "start_time": "2022-07-03T10:25:06.284543Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CausalLMOutputWithPast(loss=None, logits=tensor([[[-0.0323,  0.0318, -0.0662,  ...,  0.1837,  0.0578, -0.1297],\n",
       "         [-0.0123,  0.0069, -0.0111,  ...,  0.1814,  0.0552, -0.0187],\n",
       "         [-0.0038,  0.0315, -0.0168,  ...,  0.0853,  0.0231, -0.2297],\n",
       "         ...,\n",
       "         [ 0.1834, -0.1816, -0.0232,  ..., -0.3520, -0.1645, -0.0654],\n",
       "         [ 0.1003, -0.1515,  0.1125,  ..., -0.0605, -0.0996, -0.1787],\n",
       "         [ 0.0786,  0.0333, -0.0046,  ..., -0.0942,  0.0644, -0.1715]]],\n",
       "       grad_fn=<ViewBackward0>), past_key_values=((tensor([[[[-1.1634e-02,  1.6961e-02,  1.9648e-02,  3.7738e-02,  3.8144e-02,\n",
       "           -1.9056e-01, -1.1523e-01, -1.3935e-01],\n",
       "          [ 4.4637e-02,  1.2676e-01, -9.8951e-02,  1.5734e-02,  1.1940e-01,\n",
       "            1.7108e-01,  4.0918e-02, -2.3823e-01],\n",
       "          [-9.0130e-02, -1.9191e-02,  1.0203e-01, -2.2726e-01, -8.5284e-02,\n",
       "           -6.4323e-02, -9.6822e-02,  5.4474e-02],\n",
       "          [ 1.4120e-01, -9.3763e-02, -6.3809e-02,  2.1546e-01, -2.6702e-02,\n",
       "            6.1472e-02,  1.7037e-01, -2.7901e-01],\n",
       "          [ 1.4176e-01, -1.6807e-02, -4.4471e-03, -1.9359e-01, -5.7122e-02,\n",
       "            4.6753e-02,  3.0729e-01,  2.3676e-01],\n",
       "          [ 7.0518e-02,  7.8727e-03,  1.9349e-01, -7.3101e-02,  5.8459e-02,\n",
       "            1.6731e-02,  1.3889e-01,  1.4562e-01],\n",
       "          [-5.9286e-02, -2.6144e-02, -2.5400e-01,  6.1904e-02, -1.6280e-01,\n",
       "           -1.2562e-01,  1.3566e-01, -1.6672e-01],\n",
       "          [ 1.0477e-02,  1.1497e-01, -1.4880e-01, -5.6538e-03,  2.2987e-01,\n",
       "            9.8768e-02,  9.2187e-02, -1.4258e-01],\n",
       "          [-3.8903e-02,  9.3936e-02, -2.0056e-01,  1.0188e-01,  1.0073e-01,\n",
       "            2.5931e-01, -1.5122e-01,  6.4776e-02],\n",
       "          [ 2.8164e-01, -3.1969e-01,  5.5711e-02, -5.0954e-02,  9.0732e-04,\n",
       "           -2.3541e-01,  1.8109e-01,  1.0423e-01],\n",
       "          [-1.2349e-01,  1.7397e-01, -4.4352e-02, -1.5030e-01, -4.4129e-02,\n",
       "           -1.1971e-02,  1.5403e-01,  7.9608e-02]],\n",
       "\n",
       "         [[-1.1764e-01,  1.1511e-01,  8.0415e-02, -2.1986e-01,  3.9291e-02,\n",
       "           -8.3343e-02, -1.0620e-01, -5.1667e-03],\n",
       "          [ 5.3713e-02, -8.0309e-02,  1.8525e-01,  3.2155e-02,  2.8992e-02,\n",
       "           -9.6907e-02,  7.7755e-02, -9.2644e-02],\n",
       "          [-8.3665e-02,  1.3642e-01,  1.2090e-01, -9.9213e-02,  1.0696e-01,\n",
       "           -5.6669e-02,  8.9791e-02,  7.8112e-03],\n",
       "          [ 5.3161e-02,  9.1301e-02,  1.5519e-01,  8.0641e-02, -1.4275e-03,\n",
       "            3.5034e-02, -5.3617e-02, -8.2671e-02],\n",
       "          [-1.1989e-02,  8.6534e-02, -1.1235e-01,  4.4310e-02,  1.0154e-01,\n",
       "           -1.9342e-01,  1.7453e-01,  7.3811e-02],\n",
       "          [ 8.1397e-02,  6.4346e-02,  3.3693e-02, -1.6113e-01,  1.5633e-02,\n",
       "            7.2156e-02, -8.6075e-02, -2.0708e-01],\n",
       "          [ 6.4725e-02,  9.4864e-02,  1.1622e-01,  9.7129e-02, -1.8016e-01,\n",
       "           -2.9950e-01, -6.7530e-02,  1.0840e-01],\n",
       "          [ 1.4783e-01,  4.9881e-02, -9.8009e-02, -3.6303e-04,  5.1818e-02,\n",
       "            1.4490e-01, -5.1958e-02, -1.8248e-01],\n",
       "          [ 7.8771e-02, -1.9040e-01,  7.8523e-02, -7.9667e-02, -8.3114e-02,\n",
       "            9.2253e-02, -9.1940e-02, -8.6423e-02],\n",
       "          [-8.2968e-02, -1.2700e-01, -5.8750e-02, -1.0491e-01,  5.0412e-02,\n",
       "           -1.0009e-01,  1.2124e-02,  8.1179e-02],\n",
       "          [-5.1957e-02, -8.4338e-02,  5.1226e-02,  1.0699e-02,  2.9525e-05,\n",
       "           -7.0400e-02, -2.6843e-02, -1.9419e-02]],\n",
       "\n",
       "         [[-4.2629e-02,  1.0973e-02,  2.6501e-02, -1.7135e-02, -9.9608e-02,\n",
       "           -1.5877e-01,  3.7133e-02, -1.0556e-01],\n",
       "          [ 2.1171e-01, -9.4436e-02, -7.7961e-02, -1.0408e-02, -1.3994e-01,\n",
       "            6.3245e-03, -1.2804e-01,  5.9441e-02],\n",
       "          [ 5.6323e-02,  3.1883e-02,  3.3018e-02, -6.4846e-03, -5.8424e-02,\n",
       "           -6.3883e-02,  1.4561e-02, -1.1243e-01],\n",
       "          [ 5.1500e-02, -8.6362e-02,  7.1113e-02, -8.3506e-02,  7.4962e-02,\n",
       "            1.2470e-01, -3.6715e-02, -1.1791e-01],\n",
       "          [-1.7267e-01,  1.0515e-01, -1.3449e-02, -1.1172e-01, -8.6149e-02,\n",
       "           -5.4270e-03,  2.3428e-02, -3.6559e-02],\n",
       "          [ 1.4644e-01,  1.7554e-02,  1.9106e-02, -2.8001e-02,  5.9060e-02,\n",
       "           -2.1075e-01,  7.2263e-02,  1.6772e-01],\n",
       "          [ 1.5539e-01,  1.5554e-01, -2.1749e-02,  1.4792e-02, -2.8965e-02,\n",
       "            1.5513e-01, -2.8826e-02,  2.8441e-02],\n",
       "          [-4.6869e-02, -2.4094e-01,  1.6486e-02, -2.3105e-01, -1.9095e-01,\n",
       "            1.6886e-01, -6.8683e-02, -3.5834e-02],\n",
       "          [-6.7190e-03, -2.2104e-01,  3.8227e-02,  6.0270e-02, -2.9720e-03,\n",
       "            9.6224e-02,  5.0275e-02,  1.7321e-02],\n",
       "          [ 1.5181e-01, -2.0032e-02,  9.6690e-02, -1.4850e-01, -1.8629e-01,\n",
       "           -1.5600e-01,  1.4641e-01, -1.6888e-01],\n",
       "          [ 4.6789e-02, -1.9338e-01,  1.6639e-01, -5.0428e-02,  1.1306e-01,\n",
       "            1.1267e-01, -2.6574e-02, -1.9834e-02]],\n",
       "\n",
       "         [[ 2.8000e-02, -3.3042e-01,  5.3479e-02,  2.6779e-02,  5.0221e-02,\n",
       "           -9.7454e-02,  2.0627e-01, -1.8860e-01],\n",
       "          [-7.0656e-02,  4.5840e-02,  7.4708e-02,  3.3710e-02,  4.2909e-02,\n",
       "            1.0618e-01, -1.2812e-01,  4.4919e-02],\n",
       "          [ 9.0996e-02,  1.8781e-02, -3.0429e-01, -3.4282e-02,  9.1390e-02,\n",
       "            1.2340e-01, -1.3521e-01, -1.4093e-01],\n",
       "          [-5.7764e-02,  4.9410e-02, -3.2762e-02,  1.7684e-01, -1.0625e-01,\n",
       "            9.5606e-02, -5.0261e-02, -1.6652e-01],\n",
       "          [ 1.0519e-01, -1.6112e-01, -2.4287e-02, -2.1816e-01,  4.6226e-02,\n",
       "            2.4137e-01,  2.1777e-02,  1.6850e-03],\n",
       "          [-1.4346e-01,  1.7726e-02,  1.4836e-01, -1.2083e-01, -1.6551e-01,\n",
       "           -1.4894e-01,  3.1264e-01, -5.1161e-02],\n",
       "          [-4.0003e-02,  6.8588e-02, -1.7152e-01,  5.2209e-02, -1.3557e-03,\n",
       "            2.2127e-02,  7.3312e-02,  7.4128e-02],\n",
       "          [ 5.5417e-02,  1.3397e-01,  6.7124e-02,  1.5389e-01,  3.3180e-02,\n",
       "            5.3799e-02, -2.7860e-01,  5.4054e-03],\n",
       "          [ 6.7161e-03, -1.4476e-01,  4.3243e-02,  1.1622e-01, -2.3678e-01,\n",
       "            6.8261e-03,  7.7343e-02,  9.1908e-05],\n",
       "          [-4.2376e-02, -1.1804e-01,  1.0103e-01, -1.3221e-01,  7.4880e-02,\n",
       "           -2.1874e-02,  3.9505e-02, -1.1147e-01],\n",
       "          [ 1.0106e-01,  7.8975e-02,  2.9985e-02, -7.4409e-02,  8.6392e-02,\n",
       "           -1.6802e-01, -3.4289e-02, -8.6799e-02]]]],\n",
       "       grad_fn=<PermuteBackward0>), tensor([[[[ 0.0171,  0.0542, -0.0357, -0.2530, -0.0771,  0.0444, -0.0457,\n",
       "            0.0414],\n",
       "          [-0.1304, -0.0094,  0.1798, -0.1285, -0.1417,  0.0649,  0.0496,\n",
       "            0.0045],\n",
       "          [-0.0412,  0.0593,  0.0683, -0.1396, -0.1308,  0.0642, -0.0179,\n",
       "           -0.0785],\n",
       "          [-0.1040, -0.1036,  0.1527, -0.2540,  0.0367, -0.1308, -0.1459,\n",
       "           -0.0280],\n",
       "          [-0.0753,  0.0140,  0.1072,  0.1540, -0.0721, -0.0223,  0.0664,\n",
       "           -0.1219],\n",
       "          [-0.0180, -0.0676,  0.0565,  0.2897,  0.0609,  0.0856, -0.0153,\n",
       "           -0.0092],\n",
       "          [-0.0984, -0.0185, -0.2002, -0.0884, -0.0269, -0.2230,  0.1372,\n",
       "           -0.0688],\n",
       "          [-0.1243, -0.0179,  0.0065, -0.0463, -0.0237,  0.1439,  0.0956,\n",
       "            0.0459],\n",
       "          [ 0.0235, -0.0206,  0.0386, -0.0189,  0.1745,  0.0803, -0.1914,\n",
       "            0.0732],\n",
       "          [-0.0712, -0.2740, -0.1134, -0.0805, -0.1173, -0.1283,  0.0007,\n",
       "           -0.0315],\n",
       "          [ 0.1260, -0.2229, -0.1396, -0.2044,  0.1768, -0.0701, -0.0981,\n",
       "            0.0388]],\n",
       "\n",
       "         [[-0.2187, -0.1078, -0.0536, -0.0766, -0.1960,  0.1010, -0.0416,\n",
       "           -0.0470],\n",
       "          [-0.1120, -0.0102,  0.0515, -0.1295, -0.0870, -0.1129,  0.0204,\n",
       "           -0.0640],\n",
       "          [-0.1788,  0.1906,  0.0687,  0.0517,  0.0230,  0.1185,  0.0145,\n",
       "           -0.1558],\n",
       "          [ 0.0549, -0.1014,  0.0103, -0.0200, -0.0189, -0.0928, -0.0344,\n",
       "            0.0692],\n",
       "          [ 0.1118,  0.2035,  0.2110,  0.0191,  0.1571, -0.0238,  0.0520,\n",
       "           -0.0852],\n",
       "          [ 0.2085, -0.0846, -0.1498, -0.1200, -0.0840, -0.0121,  0.0770,\n",
       "            0.1008],\n",
       "          [-0.0966,  0.0085,  0.0008, -0.0844, -0.1003, -0.1835,  0.0719,\n",
       "           -0.0233],\n",
       "          [-0.0130, -0.2064, -0.0206,  0.1142, -0.0586,  0.0789,  0.1476,\n",
       "            0.1441],\n",
       "          [-0.0979, -0.1958,  0.0515, -0.0619, -0.0806, -0.1905, -0.0613,\n",
       "            0.2120],\n",
       "          [-0.0829,  0.1261,  0.1687,  0.0284,  0.1966,  0.0092,  0.2029,\n",
       "           -0.0020],\n",
       "          [ 0.1319,  0.1403,  0.1151,  0.0951,  0.0463, -0.0688,  0.0970,\n",
       "           -0.0556]],\n",
       "\n",
       "         [[ 0.0076, -0.0719,  0.0037,  0.0842, -0.1174, -0.1181,  0.0707,\n",
       "           -0.1699],\n",
       "          [-0.0413,  0.1353, -0.0762, -0.0775, -0.1491, -0.0675, -0.1613,\n",
       "           -0.2008],\n",
       "          [-0.0493, -0.0245,  0.0240, -0.0642,  0.1018, -0.2692, -0.0756,\n",
       "           -0.0624],\n",
       "          [ 0.1534,  0.0673,  0.2374, -0.1305, -0.0959, -0.0733, -0.1458,\n",
       "           -0.0114],\n",
       "          [-0.2285, -0.0530, -0.0400, -0.1419,  0.1228, -0.1328, -0.1030,\n",
       "            0.1169],\n",
       "          [-0.1287,  0.0461, -0.0800,  0.0120,  0.0117,  0.1206,  0.0879,\n",
       "            0.0896],\n",
       "          [ 0.1197, -0.0533,  0.1268, -0.0888, -0.0894,  0.0803,  0.1332,\n",
       "            0.0619],\n",
       "          [ 0.0080,  0.0849, -0.1694,  0.0910, -0.0812, -0.0206, -0.0440,\n",
       "           -0.1039],\n",
       "          [ 0.1578,  0.0896,  0.0576, -0.0020, -0.1422,  0.1545,  0.1266,\n",
       "           -0.1256],\n",
       "          [-0.1036, -0.1662, -0.0846,  0.0487,  0.0231, -0.0573,  0.0299,\n",
       "            0.1811],\n",
       "          [-0.0300, -0.1060,  0.0279,  0.0984, -0.0115,  0.0457,  0.0991,\n",
       "            0.0984]],\n",
       "\n",
       "         [[ 0.2276, -0.0988, -0.0505, -0.0474,  0.0255, -0.1545, -0.0298,\n",
       "            0.1196],\n",
       "          [ 0.2243, -0.0518, -0.2338,  0.1420, -0.2158,  0.0649, -0.2444,\n",
       "            0.0921],\n",
       "          [ 0.1617, -0.0242,  0.0938,  0.0932,  0.0760, -0.1134, -0.1600,\n",
       "            0.1168],\n",
       "          [-0.0631,  0.1038, -0.0729,  0.0988, -0.0320,  0.1449, -0.0061,\n",
       "           -0.0559],\n",
       "          [-0.0377,  0.1357,  0.0549, -0.1232, -0.1101, -0.1341, -0.0709,\n",
       "           -0.0467],\n",
       "          [-0.1323, -0.0174,  0.0900, -0.0579,  0.0015,  0.1445,  0.1882,\n",
       "           -0.1156],\n",
       "          [-0.2668,  0.0916, -0.1083,  0.0335, -0.0629, -0.0319,  0.0676,\n",
       "            0.0050],\n",
       "          [-0.0332, -0.1007, -0.0690,  0.0224, -0.1172,  0.2198,  0.0441,\n",
       "            0.0516],\n",
       "          [ 0.1446,  0.0340, -0.1633, -0.0384, -0.0801,  0.1902, -0.0661,\n",
       "           -0.1240],\n",
       "          [-0.2720,  0.0388, -0.0587,  0.0495, -0.1034, -0.2154,  0.0399,\n",
       "           -0.1323],\n",
       "          [ 0.0962,  0.0548, -0.0123,  0.0331,  0.1080, -0.1707,  0.0256,\n",
       "            0.0419]]]], grad_fn=<PermuteBackward0>)), (tensor([[[[ 2.0790e-01, -1.2516e-01, -1.6788e-01,  1.6620e-01,  2.2577e-01,\n",
       "           -7.2222e-02, -2.1929e-02,  5.9205e-02],\n",
       "          [ 5.0149e-02, -3.1754e-02,  1.3771e-01,  1.7316e-02, -2.0625e-02,\n",
       "            1.0715e-01, -8.0109e-02, -9.2016e-03],\n",
       "          [ 3.4048e-02,  8.4499e-02, -9.1632e-02, -1.6247e-01,  4.4562e-02,\n",
       "            7.1992e-04,  3.4425e-02,  7.0090e-02],\n",
       "          [ 1.3377e-01, -4.3195e-02,  3.7674e-03,  1.3233e-01,  8.2975e-02,\n",
       "           -9.7951e-02, -2.4402e-02, -1.0076e-01],\n",
       "          [ 1.0030e-01,  1.4483e-01,  9.3068e-02,  6.6800e-02, -7.2833e-02,\n",
       "            1.8582e-01, -9.2421e-02, -1.4145e-02],\n",
       "          [ 1.1476e-01, -5.8891e-03,  2.0719e-02,  1.2829e-01,  4.8744e-02,\n",
       "           -2.2790e-01,  1.2819e-02,  1.4481e-01],\n",
       "          [ 8.5194e-03, -6.4964e-02,  9.0801e-02,  1.0755e-01, -1.3775e-01,\n",
       "            1.8124e-02, -1.6237e-01, -8.7375e-02],\n",
       "          [-2.4532e-01,  1.0377e-01,  1.3679e-01,  1.4722e-01, -9.2835e-02,\n",
       "           -6.4611e-02,  1.3135e-02,  8.3805e-02],\n",
       "          [-1.3094e-01, -2.2824e-02,  1.3340e-01,  1.0370e-01,  3.9290e-02,\n",
       "            1.1690e-01,  5.8894e-02,  7.7966e-02],\n",
       "          [ 3.3632e-02, -1.4160e-02, -4.0776e-02,  1.5104e-01,  3.9780e-02,\n",
       "           -2.5793e-02, -1.5083e-01, -5.5012e-02],\n",
       "          [ 1.1525e-01, -1.9895e-01, -5.6371e-02, -4.0238e-02,  5.6363e-02,\n",
       "           -1.5351e-01,  8.4263e-03, -5.8050e-03]],\n",
       "\n",
       "         [[ 1.2401e-02,  2.7458e-02, -1.6287e-01,  1.6553e-02,  5.6227e-02,\n",
       "           -7.7730e-02,  1.6722e-02, -1.3129e-01],\n",
       "          [ 4.6745e-02, -3.2105e-02,  7.2943e-04,  1.3091e-01, -2.5757e-01,\n",
       "           -1.2568e-02, -1.5207e-01, -2.5302e-01],\n",
       "          [-1.2988e-01,  2.3277e-01, -1.5518e-02, -1.2891e-01,  4.7131e-02,\n",
       "           -5.1556e-02,  5.1028e-03,  9.0947e-04],\n",
       "          [ 1.1436e-01, -4.8823e-03,  8.2837e-02,  1.2514e-01, -1.1488e-01,\n",
       "            1.7737e-01,  6.7049e-02, -1.0899e-01],\n",
       "          [-1.2591e-01, -1.9558e-01, -3.1247e-02, -1.0754e-01,  6.9161e-04,\n",
       "            1.8169e-01, -1.8718e-01, -8.2822e-02],\n",
       "          [-3.7546e-02,  1.3117e-01, -9.0126e-02,  7.7615e-02, -4.1177e-02,\n",
       "            3.9803e-02, -6.9127e-02,  3.5655e-02],\n",
       "          [-1.3522e-01, -6.7020e-03,  1.0782e-01,  1.0809e-01, -9.7207e-02,\n",
       "           -5.1354e-02, -2.1179e-03, -1.1571e-01],\n",
       "          [ 8.1856e-02, -9.8846e-02, -7.9651e-02,  5.9920e-02,  5.1688e-02,\n",
       "            3.5584e-02, -1.4888e-01, -8.9193e-02],\n",
       "          [ 4.8966e-02, -1.1101e-01,  1.5982e-02,  1.1381e-01, -1.3000e-01,\n",
       "           -1.7411e-01, -1.0386e-01, -1.2580e-02],\n",
       "          [ 1.6799e-01, -2.1901e-02, -1.3369e-02, -9.9750e-02, -1.1097e-02,\n",
       "            1.6363e-01, -2.1953e-01, -8.4025e-02],\n",
       "          [ 9.1970e-02,  2.0400e-01, -1.2013e-01,  1.2605e-01,  9.6529e-02,\n",
       "            1.1341e-02,  1.5505e-01,  2.1762e-01]],\n",
       "\n",
       "         [[-6.0195e-02,  9.5428e-02,  7.2132e-02, -7.6157e-02, -3.8029e-02,\n",
       "            1.3629e-01,  2.1995e-01,  5.6482e-02],\n",
       "          [-5.6090e-02, -4.9580e-02, -1.7518e-01,  4.1926e-02,  3.5576e-02,\n",
       "            1.4750e-01, -1.6069e-01,  5.4290e-02],\n",
       "          [-1.3709e-01, -2.0356e-01, -7.3694e-02, -8.0086e-02, -4.0045e-02,\n",
       "           -1.5826e-01, -6.3786e-03,  9.3870e-02],\n",
       "          [ 6.3956e-03, -6.5399e-02, -4.7622e-02,  6.5664e-02,  6.5642e-02,\n",
       "           -1.7337e-02, -3.3507e-02,  1.0298e-01],\n",
       "          [-8.7720e-03,  4.2906e-02, -2.5259e-02,  8.5314e-02, -1.4487e-01,\n",
       "            4.8929e-02, -2.2859e-01,  2.7196e-01],\n",
       "          [ 8.0156e-02, -8.7889e-02, -9.2215e-02, -2.7815e-02, -3.5232e-02,\n",
       "           -2.0428e-04,  3.4566e-03, -9.6184e-02],\n",
       "          [ 6.3093e-02, -7.1869e-02, -3.7693e-02, -9.9585e-02,  1.4665e-01,\n",
       "            8.7445e-02, -7.6155e-02,  4.3390e-02],\n",
       "          [-8.2527e-03,  7.0518e-02, -7.2438e-02,  6.6484e-03, -1.5759e-01,\n",
       "            7.0887e-02,  3.8417e-02, -1.2396e-01],\n",
       "          [ 1.0692e-01, -5.8566e-02,  1.4279e-02,  3.9188e-02,  3.7359e-02,\n",
       "            5.0966e-02,  1.0206e-01, -3.1531e-01],\n",
       "          [ 1.2280e-01, -2.4359e-02, -7.5955e-02, -1.4997e-01, -1.0467e-02,\n",
       "           -1.1791e-01, -3.8093e-02,  1.9855e-01],\n",
       "          [ 3.6385e-02, -1.6474e-01,  5.9323e-02,  4.1284e-02,  1.8294e-02,\n",
       "           -1.0233e-01, -1.6125e-02, -5.2097e-02]],\n",
       "\n",
       "         [[-5.9181e-03,  9.1150e-04,  7.2069e-02, -6.7338e-02, -2.4773e-01,\n",
       "           -7.6096e-02,  4.3424e-02,  6.8898e-03],\n",
       "          [-2.8162e-03, -3.0635e-02,  4.8292e-02, -2.6512e-02, -5.9783e-02,\n",
       "            2.4856e-02,  1.3626e-01, -4.0391e-02],\n",
       "          [ 5.4308e-02,  1.9885e-01,  1.0817e-01, -1.8088e-01, -8.5917e-02,\n",
       "            1.1123e-01, -1.4551e-03, -3.4782e-02],\n",
       "          [ 5.6707e-02, -4.8272e-02,  1.8143e-02, -5.0196e-02,  4.9051e-03,\n",
       "           -6.7304e-02,  1.0937e-01, -1.3346e-02],\n",
       "          [-6.4423e-02,  1.1912e-03,  1.6881e-02,  1.5500e-02, -8.5252e-02,\n",
       "            5.9443e-02,  3.5600e-03,  3.4983e-02],\n",
       "          [ 7.5398e-02, -1.0848e-02, -1.1703e-01,  5.0332e-02,  1.2775e-01,\n",
       "            1.3580e-02,  6.5031e-02,  1.0255e-01],\n",
       "          [-1.0381e-01,  4.6063e-03, -1.2249e-01, -5.6194e-02, -3.9520e-02,\n",
       "           -4.9739e-02,  8.4318e-02,  4.8580e-02],\n",
       "          [ 2.7005e-02, -3.4004e-02, -1.2389e-01,  1.9477e-01,  3.8389e-02,\n",
       "           -4.2300e-03,  8.4507e-02, -1.7950e-01],\n",
       "          [-7.2697e-02, -7.6316e-02, -1.4160e-01,  2.0706e-01, -6.7395e-02,\n",
       "            8.1866e-02,  3.1925e-02, -4.4685e-02],\n",
       "          [-2.5820e-02,  6.3055e-02, -5.3304e-02,  5.0960e-02, -2.9516e-02,\n",
       "           -5.1566e-02,  6.6331e-03,  8.2767e-02],\n",
       "          [ 1.0476e-01, -1.2231e-01,  1.3301e-01,  3.1781e-02,  1.0989e-01,\n",
       "            3.7473e-02, -1.4554e-01, -7.5831e-03]]]],\n",
       "       grad_fn=<PermuteBackward0>), tensor([[[[ 3.0424e-02, -1.2000e-01, -3.6902e-02,  1.4046e-01, -8.5728e-02,\n",
       "            5.7887e-02,  5.0584e-02, -2.0318e-01],\n",
       "          [-5.0121e-02, -6.6799e-02,  1.2202e-01, -3.9613e-02, -2.2841e-01,\n",
       "           -2.2697e-01, -1.0758e-01, -9.3229e-02],\n",
       "          [-2.9101e-02, -5.3271e-02, -1.4933e-01,  4.1018e-02,  1.0692e-01,\n",
       "           -1.3772e-01,  7.1656e-02, -1.1497e-01],\n",
       "          [-2.6833e-02, -4.0031e-04,  2.3007e-01,  4.4859e-02, -6.4392e-02,\n",
       "            1.5206e-02, -2.1286e-03,  6.9598e-02],\n",
       "          [ 6.3839e-02, -8.7692e-02, -8.5181e-02, -1.4678e-01, -4.4756e-02,\n",
       "           -4.7138e-02,  1.6214e-02, -4.9017e-02],\n",
       "          [ 1.6018e-01,  3.9254e-02,  1.9874e-01, -6.9810e-02,  4.8486e-02,\n",
       "           -7.4850e-02, -1.0911e-01,  1.4588e-01],\n",
       "          [-6.3813e-02,  1.2197e-01,  1.6901e-01, -1.7705e-01, -2.1709e-01,\n",
       "           -3.6522e-02,  6.7583e-02, -1.0838e-01],\n",
       "          [ 1.3559e-01, -2.0780e-02, -6.0023e-02,  7.0963e-02,  6.2085e-02,\n",
       "           -1.5443e-01, -7.6837e-02,  1.5099e-01],\n",
       "          [-2.8093e-02,  8.0031e-02,  5.7515e-02,  2.7983e-02, -2.1391e-01,\n",
       "           -4.5522e-02, -2.1053e-02,  5.6269e-03],\n",
       "          [ 7.4436e-02, -7.6772e-02,  1.3846e-01, -7.7578e-02,  2.1550e-03,\n",
       "            4.5305e-02, -1.6953e-02, -8.6002e-03],\n",
       "          [ 5.0610e-02,  3.3153e-02,  7.1011e-04,  9.3553e-02,  2.5956e-01,\n",
       "            5.9181e-03,  1.2432e-02,  4.6154e-02]],\n",
       "\n",
       "         [[-1.3585e-01,  9.2872e-02, -1.4793e-01, -1.1512e-01,  1.5590e-01,\n",
       "            2.2489e-01, -3.7400e-02,  8.9671e-03],\n",
       "          [ 1.2229e-02,  5.6446e-02, -7.8934e-02, -1.8395e-02, -8.4341e-03,\n",
       "           -7.6971e-02, -1.3930e-01, -6.7892e-02],\n",
       "          [-1.1147e-01, -4.8629e-02, -2.1664e-01, -3.3911e-01,  4.3205e-02,\n",
       "            2.1142e-01,  1.5720e-01, -9.5574e-02],\n",
       "          [-8.9254e-02,  8.2551e-02, -1.4078e-01, -1.0149e-02, -1.1512e-01,\n",
       "            1.5735e-02, -1.5732e-02, -2.5725e-02],\n",
       "          [-1.6366e-01, -1.5913e-01,  1.0488e-01,  1.8312e-02, -1.1536e-01,\n",
       "           -9.7325e-02, -1.0079e-02,  4.9891e-02],\n",
       "          [ 1.0770e-01, -5.7575e-03, -5.3437e-02,  1.2482e-01, -2.3788e-03,\n",
       "           -1.1852e-01,  7.6648e-03,  1.7264e-01],\n",
       "          [-1.1454e-01,  6.6031e-02,  3.3561e-02,  1.1550e-01,  3.3194e-02,\n",
       "           -5.3352e-02,  7.9536e-02, -3.4172e-02],\n",
       "          [ 2.3970e-02,  7.6088e-02,  2.3769e-02,  7.7430e-02,  4.1850e-02,\n",
       "           -5.7268e-02, -2.0471e-01, -4.6616e-02],\n",
       "          [ 2.5361e-01,  7.3356e-02,  3.3439e-02,  1.3950e-01,  1.2645e-01,\n",
       "           -9.6047e-02, -2.0069e-01, -7.2836e-02],\n",
       "          [-2.2446e-01,  1.6392e-01, -3.4341e-02,  4.7037e-02, -1.3359e-01,\n",
       "           -9.1107e-02,  1.7364e-01,  1.1519e-02],\n",
       "          [-5.6372e-02,  1.1312e-02, -2.8172e-02,  4.5846e-02, -1.2361e-01,\n",
       "           -4.5051e-02,  4.3062e-02, -3.8127e-02]],\n",
       "\n",
       "         [[-1.4309e-01, -6.5963e-02,  7.0733e-02,  7.1321e-02, -1.0483e-02,\n",
       "           -7.5946e-02,  6.3015e-02, -1.2175e-01],\n",
       "          [-6.0215e-02, -1.2964e-01, -4.1215e-02,  6.9457e-02,  7.5664e-02,\n",
       "           -1.0304e-01, -2.9700e-02,  6.0717e-02],\n",
       "          [ 5.9984e-03, -1.3327e-01,  9.6241e-02, -1.0734e-01, -2.0875e-03,\n",
       "           -7.6233e-02,  9.7367e-02,  6.4693e-03],\n",
       "          [-1.7633e-02,  1.6854e-01, -8.6208e-02, -4.6795e-03, -1.0864e-01,\n",
       "            2.3052e-01,  1.0588e-01,  1.1213e-01],\n",
       "          [-3.3662e-02, -1.6736e-01,  1.4612e-02, -6.4703e-02, -2.2264e-02,\n",
       "           -1.2749e-01,  7.3656e-02,  1.8216e-01],\n",
       "          [-1.5582e-01,  1.2711e-01,  1.5169e-01,  1.2948e-01, -1.1523e-01,\n",
       "            9.0820e-02,  2.6602e-02, -2.6166e-01],\n",
       "          [ 6.1286e-02,  3.6829e-02, -4.6362e-02, -2.6925e-02,  6.0222e-02,\n",
       "            1.0195e-01, -5.2741e-02, -2.9422e-02],\n",
       "          [-1.0795e-01,  6.0505e-02, -1.1798e-01,  1.4692e-02, -1.1468e-01,\n",
       "           -1.9919e-02, -1.6373e-01, -6.1603e-02],\n",
       "          [-2.9146e-02, -4.4111e-02, -9.3631e-02,  5.2231e-02,  2.5405e-02,\n",
       "            6.3922e-03, -5.5303e-02, -1.3124e-02],\n",
       "          [ 6.8878e-02,  6.0870e-02,  4.8069e-02, -3.7918e-02, -2.2233e-01,\n",
       "            1.5291e-01, -1.5083e-02,  2.0566e-01],\n",
       "          [ 1.1992e-01,  2.9154e-01, -2.5652e-02, -7.5681e-02, -8.5944e-02,\n",
       "            1.4765e-01,  4.1195e-02,  1.1788e-03]],\n",
       "\n",
       "         [[ 2.4231e-01,  7.0299e-02, -1.1467e-01, -9.4125e-02,  8.1528e-02,\n",
       "           -7.9781e-02, -2.3202e-01, -1.0171e-06],\n",
       "          [ 2.4695e-02, -9.5687e-02, -4.6277e-02, -1.1383e-01, -5.0090e-02,\n",
       "            8.2741e-02, -8.0750e-02, -8.1632e-02],\n",
       "          [ 1.5215e-01, -9.1245e-03,  9.1994e-02,  3.8657e-02,  5.7950e-02,\n",
       "           -3.7011e-02, -1.1481e-01, -2.1823e-02],\n",
       "          [ 3.7893e-02,  3.5049e-03, -2.7877e-02, -3.8457e-02,  4.7491e-02,\n",
       "           -1.5452e-02,  1.4595e-01,  1.2903e-02],\n",
       "          [-1.0468e-01, -7.3844e-02, -8.7396e-02,  8.3684e-02, -5.3852e-02,\n",
       "           -2.7035e-02, -9.1436e-02,  2.5312e-01],\n",
       "          [ 9.6731e-03, -8.8589e-02, -2.3525e-01, -1.3662e-01, -1.1410e-01,\n",
       "           -1.6017e-02,  1.0795e-01,  9.9317e-02],\n",
       "          [-2.2697e-02,  8.7938e-02,  1.5342e-01,  9.5591e-02,  1.2513e-01,\n",
       "           -1.0107e-01, -8.8984e-04, -8.0815e-02],\n",
       "          [-9.8328e-02, -2.1991e-01, -2.7023e-01, -7.4256e-02, -2.2132e-01,\n",
       "            1.0652e-01, -8.7405e-03, -1.7384e-02],\n",
       "          [ 4.3775e-02, -4.4323e-02, -1.1244e-01, -1.0052e-01, -3.7993e-02,\n",
       "            1.1670e-01,  9.4239e-03, -1.7900e-01],\n",
       "          [ 1.1771e-01,  5.6775e-03,  1.5242e-02, -1.0361e-01, -5.2973e-02,\n",
       "           -1.0709e-01,  2.4308e-02,  2.4566e-01],\n",
       "          [ 7.1337e-02, -3.7211e-02,  1.5309e-01,  1.0676e-01, -6.5610e-02,\n",
       "           -7.5583e-02,  1.9758e-01,  4.9031e-02]]]],\n",
       "       grad_fn=<PermuteBackward0>)), (tensor([[[[ 1.9655e-02, -2.4642e-02,  1.8728e-01, -9.1637e-02, -1.2734e-01,\n",
       "           -1.2457e-01, -7.2839e-02,  1.5625e-01],\n",
       "          [-1.2426e-02, -2.7234e-02,  2.2014e-01,  1.2746e-02,  1.3320e-01,\n",
       "           -4.9042e-02,  3.6293e-02,  1.9725e-01],\n",
       "          [ 4.6031e-02,  1.3231e-01,  1.4088e-01, -1.8674e-01, -2.8778e-01,\n",
       "           -5.3535e-02, -6.6287e-02, -6.2437e-02],\n",
       "          [ 1.5779e-01,  1.5958e-01,  1.0051e-01,  2.6623e-01, -4.8648e-02,\n",
       "           -2.3269e-02,  8.6545e-05,  1.1476e-01],\n",
       "          [-7.2616e-02,  6.8641e-02,  7.3206e-02, -5.5930e-02,  9.9780e-02,\n",
       "            7.8260e-02, -4.9725e-02, -2.5309e-01],\n",
       "          [ 2.1762e-01, -6.0847e-02, -4.9701e-02, -1.2506e-03,  5.2562e-02,\n",
       "            3.4226e-02, -5.0600e-02, -7.7295e-02],\n",
       "          [-1.8604e-02, -3.4299e-02, -3.0165e-02,  4.8597e-02,  8.5176e-02,\n",
       "            1.6664e-01,  1.4418e-01,  1.1460e-01],\n",
       "          [ 3.4455e-02, -2.0964e-02,  1.6196e-01,  2.9834e-02,  5.3945e-02,\n",
       "            3.1867e-02, -7.9612e-02,  6.6296e-02],\n",
       "          [-3.5265e-02, -1.5020e-02, -3.1830e-02,  1.1211e-01,  1.5470e-01,\n",
       "            1.1392e-01,  2.3657e-01, -3.9926e-02],\n",
       "          [-1.3334e-01,  6.6091e-02,  1.3074e-01, -6.7963e-02,  1.2241e-01,\n",
       "            1.1816e-01,  1.0546e-01, -1.2022e-01],\n",
       "          [ 9.0939e-03,  3.3160e-03, -2.0166e-01, -1.5864e-01, -1.8309e-01,\n",
       "           -3.5827e-02, -3.8732e-02,  1.9021e-02]],\n",
       "\n",
       "         [[ 1.9639e-01,  1.0707e-01, -1.1620e-01,  5.4050e-02,  1.9319e-02,\n",
       "            4.2543e-02,  8.5563e-02,  1.3634e-01],\n",
       "          [-1.3204e-01,  1.6163e-01, -1.1370e-01, -8.0647e-03, -1.2946e-02,\n",
       "           -1.1829e-02,  1.7688e-02, -1.5363e-02],\n",
       "          [-1.6050e-01,  2.3084e-02, -2.0597e-02, -2.8701e-02, -4.6081e-02,\n",
       "            2.5772e-02,  2.5943e-02,  7.3866e-02],\n",
       "          [-1.2260e-01, -1.5409e-01, -1.2878e-01, -5.9696e-02,  9.0048e-02,\n",
       "            1.0201e-01,  1.2388e-02, -1.1689e-01],\n",
       "          [ 9.5306e-03,  3.3247e-02,  7.9809e-02,  9.3499e-02, -8.1418e-02,\n",
       "           -1.1903e-01, -4.5950e-02, -4.5668e-02],\n",
       "          [-9.2077e-02, -1.2523e-01, -6.4214e-02,  4.0589e-02,  3.7718e-02,\n",
       "           -2.0620e-02, -2.5000e-02,  1.5549e-01],\n",
       "          [ 2.1661e-01,  1.5763e-02, -4.4520e-02, -3.6442e-02,  7.8612e-02,\n",
       "           -9.2636e-02, -1.3206e-01,  5.6052e-02],\n",
       "          [-3.0321e-02,  2.4525e-02, -6.6186e-02,  7.7509e-02,  5.7207e-02,\n",
       "           -1.6381e-02,  3.9088e-02, -2.1171e-01],\n",
       "          [ 1.6130e-01, -6.6012e-02,  1.1205e-01, -1.2505e-01, -1.3446e-01,\n",
       "           -4.8481e-02,  2.3952e-02, -1.4967e-01],\n",
       "          [-6.0789e-02, -3.7571e-03, -9.8991e-03,  1.0264e-01, -3.5205e-02,\n",
       "           -2.3197e-01, -1.6050e-01, -7.1569e-02],\n",
       "          [-6.5232e-02, -1.2558e-02,  1.5042e-01, -2.0402e-02,  6.6490e-02,\n",
       "            2.2346e-02,  3.3374e-03,  4.8406e-02]],\n",
       "\n",
       "         [[ 8.2086e-02,  3.5274e-02,  7.3406e-02,  9.3053e-03, -1.0546e-01,\n",
       "            4.6164e-02,  5.6165e-03,  6.1189e-02],\n",
       "          [-6.9414e-02,  1.7776e-01,  2.2195e-01,  6.9627e-02, -8.2285e-02,\n",
       "            1.5516e-01,  1.6888e-01,  2.4197e-02],\n",
       "          [ 9.1560e-02,  1.4651e-01, -1.6141e-02, -1.1683e-02,  4.3219e-02,\n",
       "            1.1061e-01,  4.0337e-02, -1.2162e-01],\n",
       "          [-5.6599e-02, -8.6605e-02,  1.7361e-01,  1.2226e-01, -5.5775e-02,\n",
       "           -2.3146e-02,  5.0126e-02, -3.9308e-02],\n",
       "          [-5.7818e-02, -5.3923e-02,  1.6513e-02,  7.7530e-02, -1.8508e-01,\n",
       "           -1.1011e-02,  1.4829e-01, -9.5729e-02],\n",
       "          [ 9.0889e-02,  5.1728e-02,  8.4021e-02,  1.3281e-02, -2.3699e-01,\n",
       "           -3.3448e-02,  6.5765e-02, -1.2337e-01],\n",
       "          [-4.1188e-02, -1.7031e-01, -6.5521e-03,  6.5001e-03,  6.6523e-02,\n",
       "            1.7786e-01, -1.2541e-01, -5.7872e-02],\n",
       "          [-2.3541e-02,  1.8824e-01,  3.5680e-02,  9.2370e-02, -2.7706e-02,\n",
       "           -1.4654e-01,  2.5202e-02, -5.3819e-02],\n",
       "          [ 2.4551e-02,  5.8172e-02,  1.1910e-01,  1.3029e-01,  2.7476e-02,\n",
       "           -4.8523e-02,  3.8957e-02,  4.7793e-02],\n",
       "          [ 8.4055e-02,  2.9050e-01,  5.4331e-02,  7.4200e-02, -4.6192e-02,\n",
       "           -5.0081e-02, -5.2392e-02,  1.3739e-01],\n",
       "          [ 7.6812e-02,  2.0679e-01, -2.0018e-03, -1.3556e-01,  5.8772e-02,\n",
       "            2.1566e-02, -1.7549e-01,  3.7373e-02]],\n",
       "\n",
       "         [[ 4.0806e-02,  2.9989e-02,  1.7039e-02,  1.9101e-01,  3.7112e-02,\n",
       "            1.1763e-02, -1.1353e-01,  3.9608e-02],\n",
       "          [-1.8892e-02,  1.5414e-01,  3.5271e-02, -5.2720e-02,  8.4263e-03,\n",
       "           -1.9947e-02, -1.1940e-01,  7.2674e-02],\n",
       "          [-6.2370e-02, -6.1413e-02,  9.2647e-02,  1.6310e-01,  2.1392e-01,\n",
       "           -2.4011e-02, -1.9225e-02,  1.6863e-02],\n",
       "          [-4.0353e-03, -1.4690e-01, -1.1734e-02,  1.7036e-01,  1.0216e-01,\n",
       "           -7.3719e-03, -1.8250e-01, -6.3432e-02],\n",
       "          [ 2.2244e-02, -5.9447e-03,  1.3890e-01, -1.5938e-01,  1.3940e-01,\n",
       "            1.6872e-01, -7.8527e-02, -9.1950e-02],\n",
       "          [ 1.5317e-01,  3.9529e-02, -1.8213e-01, -8.3438e-02, -1.7920e-01,\n",
       "            5.2646e-02,  8.4369e-02,  7.1301e-02],\n",
       "          [ 2.0617e-02, -1.8132e-02, -5.2361e-02,  7.8225e-02,  2.2359e-01,\n",
       "            1.2208e-01,  1.6898e-01,  1.3759e-01],\n",
       "          [-6.4600e-02,  5.6242e-02,  1.8285e-02, -2.3752e-01, -3.4003e-02,\n",
       "           -1.0937e-01,  2.4103e-02, -1.5990e-02],\n",
       "          [ 3.4076e-02,  9.9755e-02,  7.9096e-03, -1.1022e-01, -1.7177e-01,\n",
       "            2.4276e-02, -1.2102e-01,  7.8489e-02],\n",
       "          [-1.9307e-03, -8.0679e-02,  1.0159e-01,  4.7116e-02,  1.8073e-01,\n",
       "            6.3498e-02,  6.3722e-02, -5.4936e-02],\n",
       "          [ 9.3102e-02, -4.9144e-02,  4.8781e-02,  1.0025e-01,  1.2909e-01,\n",
       "           -4.4738e-02,  1.3244e-01, -6.3545e-02]]]],\n",
       "       grad_fn=<PermuteBackward0>), tensor([[[[ 9.9032e-02,  4.1740e-02,  1.5137e-03, -8.7085e-02,  1.6324e-01,\n",
       "            1.3771e-01, -1.3653e-01, -1.0451e-01],\n",
       "          [ 3.8464e-02,  8.6386e-03,  1.2872e-01, -2.3765e-02,  2.3515e-02,\n",
       "            7.6035e-02, -1.3121e-01, -7.8289e-02],\n",
       "          [ 4.4193e-02, -9.5581e-02, -3.9893e-02, -1.1753e-01,  1.4666e-01,\n",
       "            7.2409e-02,  9.4959e-02, -1.2695e-01],\n",
       "          [ 2.4971e-01, -5.9866e-02,  4.3893e-02, -1.2016e-01,  2.6755e-02,\n",
       "            3.6584e-02, -5.1379e-02, -7.1203e-02],\n",
       "          [ 2.7379e-02, -4.2359e-02,  4.4374e-03, -1.3568e-01, -2.8623e-02,\n",
       "           -7.2670e-02, -1.0302e-01, -6.1403e-02],\n",
       "          [ 8.5159e-02,  5.8995e-02, -2.0629e-02, -1.2778e-01,  1.9559e-01,\n",
       "            8.0959e-02,  1.3983e-01, -1.3420e-01],\n",
       "          [ 1.5191e-01, -3.8522e-02, -5.3166e-02,  1.8894e-04,  2.3615e-01,\n",
       "           -2.2810e-03,  4.3652e-01,  6.9595e-02],\n",
       "          [-8.1983e-02,  1.0643e-01, -1.0023e-01, -1.3195e-02,  2.1255e-02,\n",
       "            1.6430e-01, -1.1716e-01, -2.3520e-02],\n",
       "          [ 8.0074e-02, -3.1517e-02,  1.9510e-02,  7.3419e-02, -5.2067e-03,\n",
       "           -7.9187e-02, -9.8985e-02, -6.2973e-02],\n",
       "          [-5.6735e-04, -3.2650e-02,  8.6439e-03, -2.1681e-01,  5.6426e-03,\n",
       "            3.9780e-02,  8.4042e-02,  1.3966e-02],\n",
       "          [ 2.5436e-02, -1.1487e-01,  6.5033e-03, -1.0607e-01,  4.6462e-02,\n",
       "            2.4905e-02,  2.6626e-01,  1.1086e-01]],\n",
       "\n",
       "         [[-1.0555e-01, -1.2887e-02,  1.2291e-02, -5.3082e-02,  2.2348e-02,\n",
       "            5.9204e-02,  2.1026e-01, -1.6742e-01],\n",
       "          [-1.8683e-01, -1.2952e-02,  2.4284e-01, -7.8194e-02, -1.8810e-01,\n",
       "           -7.5121e-02, -8.5918e-02,  9.5019e-03],\n",
       "          [-9.5719e-02,  6.9600e-03, -4.3338e-02,  1.5033e-01, -1.7734e-02,\n",
       "            1.0953e-01,  1.9630e-02, -6.6335e-03],\n",
       "          [-2.0075e-01, -9.8067e-02,  1.2917e-01,  5.6765e-02, -6.2099e-02,\n",
       "           -1.5663e-01, -1.0616e-02, -4.0402e-02],\n",
       "          [ 3.4684e-02,  9.6705e-02, -1.1074e-01,  1.2187e-01, -1.2148e-01,\n",
       "            2.3092e-01, -1.5364e-01, -3.8845e-02],\n",
       "          [ 2.5648e-02,  8.2934e-03, -1.9029e-01,  6.7544e-02,  2.2404e-02,\n",
       "            4.3698e-02,  1.2273e-01, -1.6464e-01],\n",
       "          [-2.0822e-01,  1.0565e-01,  1.9915e-01, -1.9430e-02, -4.7516e-02,\n",
       "           -1.1661e-01, -4.6671e-03,  1.4013e-02],\n",
       "          [ 7.6114e-02, -4.8852e-02,  2.0995e-02,  5.2781e-02, -5.8036e-02,\n",
       "           -1.3448e-01,  5.0088e-02,  2.1813e-02],\n",
       "          [ 9.1532e-02, -2.4961e-02,  1.4504e-01,  3.0001e-02,  1.5310e-01,\n",
       "           -9.1881e-02,  1.8212e-02,  4.2688e-03],\n",
       "          [ 4.9360e-02,  1.2510e-01,  1.1646e-01,  2.2108e-01,  4.1346e-02,\n",
       "            8.5560e-02, -8.8586e-02, -6.0387e-02],\n",
       "          [-7.3908e-02,  3.7706e-02, -1.1553e-01,  3.4413e-02,  8.6175e-02,\n",
       "           -1.1582e-01, -3.4227e-02,  7.6075e-03]],\n",
       "\n",
       "         [[-8.7126e-02, -8.8801e-02,  1.1988e-01,  1.5013e-01, -6.2006e-02,\n",
       "           -9.2672e-02, -1.2345e-01, -5.6560e-02],\n",
       "          [-6.7205e-02,  1.2469e-01,  8.3322e-02,  2.0651e-01,  1.4747e-01,\n",
       "           -9.6375e-02, -1.0783e-01, -1.1012e-01],\n",
       "          [-2.3478e-02, -3.9385e-02,  1.0016e-01,  3.9462e-01, -1.5659e-01,\n",
       "            1.3625e-01,  1.8711e-01, -1.3581e-01],\n",
       "          [-4.4960e-02,  3.8860e-02,  1.2267e-01,  1.4251e-01,  6.4248e-02,\n",
       "           -6.3679e-02, -3.6149e-02,  8.5167e-02],\n",
       "          [ 7.3423e-02, -7.1827e-02, -2.0474e-01,  8.9260e-03,  1.0813e-01,\n",
       "            2.2044e-02,  6.9923e-02, -8.0114e-02],\n",
       "          [ 5.1815e-03,  5.4253e-02, -8.1787e-02, -1.0041e-01, -4.1680e-02,\n",
       "            9.0407e-02, -1.2065e-01, -7.8159e-02],\n",
       "          [-3.4005e-02, -4.2539e-02,  1.4403e-01, -2.7901e-02, -2.4315e-01,\n",
       "            2.2523e-02, -7.0114e-02,  9.7347e-02],\n",
       "          [-4.3129e-02,  5.0323e-03, -1.3601e-01, -1.7534e-01,  1.7477e-01,\n",
       "           -1.0960e-01, -9.1020e-02, -2.4842e-02],\n",
       "          [-2.2213e-02,  9.7766e-02,  5.0506e-02, -2.3035e-01,  2.0718e-01,\n",
       "           -5.0757e-02, -1.4392e-01,  1.0872e-01],\n",
       "          [ 6.4022e-02, -1.0234e-01, -7.4405e-02, -1.0629e-01,  8.3598e-03,\n",
       "           -3.7097e-02, -1.2277e-01, -1.2038e-01],\n",
       "          [ 2.2284e-02, -5.0673e-02,  1.1640e-01,  4.6825e-02, -2.8513e-01,\n",
       "            4.4499e-02, -3.1267e-02,  1.9829e-01]],\n",
       "\n",
       "         [[ 2.6177e-01,  2.2374e-02, -7.5136e-02, -1.9844e-01,  4.8853e-02,\n",
       "           -2.2309e-02, -1.2206e-02,  1.5843e-01],\n",
       "          [ 1.6435e-01, -2.8376e-02, -2.7972e-02, -1.9006e-03, -3.6882e-03,\n",
       "           -5.1417e-02, -4.5317e-02,  3.1916e-02],\n",
       "          [ 1.4870e-01,  3.5078e-02, -1.8038e-01, -7.6885e-02,  6.7130e-02,\n",
       "            1.4694e-01, -1.2716e-01,  1.5415e-01],\n",
       "          [ 1.5896e-01,  6.9464e-02, -5.4122e-02, -3.6350e-02, -6.0540e-03,\n",
       "           -8.0298e-02, -1.3473e-01,  7.0172e-02],\n",
       "          [-1.8615e-01,  1.4613e-02, -1.7715e-01, -8.7881e-02,  1.3024e-01,\n",
       "            1.2003e-01,  9.3095e-02, -9.5240e-03],\n",
       "          [ 9.7599e-02, -5.8033e-02, -1.6084e-01, -9.1192e-02,  1.8343e-01,\n",
       "           -7.9973e-02,  9.0956e-02, -9.8354e-03],\n",
       "          [ 3.3164e-02, -7.5779e-02,  5.8592e-02,  1.2202e-01, -1.2316e-01,\n",
       "           -5.4116e-02, -5.4304e-02,  1.6176e-01],\n",
       "          [ 7.0699e-02, -1.4702e-01, -1.2549e-02, -9.8604e-02,  8.6450e-02,\n",
       "           -7.9816e-02,  2.0978e-02, -1.2707e-01],\n",
       "          [ 2.0168e-01, -7.7093e-02,  1.0733e-01,  5.1440e-04, -4.8066e-02,\n",
       "           -4.0888e-02,  4.5951e-02,  1.9826e-02],\n",
       "          [-1.0952e-01, -1.8534e-01, -1.9524e-01, -3.7981e-04, -1.2374e-03,\n",
       "            1.5631e-01,  3.3384e-02, -2.1535e-02],\n",
       "          [ 7.6081e-02,  3.5729e-02, -5.4869e-02,  1.1278e-01, -1.5018e-01,\n",
       "            2.0313e-01, -2.1306e-02,  6.8187e-02]]]],\n",
       "       grad_fn=<PermuteBackward0>)), (tensor([[[[ 0.0789, -0.0315,  0.0073, -0.1308,  0.2274, -0.0503,  0.0012,\n",
       "           -0.1898],\n",
       "          [ 0.0111,  0.0131,  0.0702, -0.0177,  0.0786,  0.2196,  0.0137,\n",
       "           -0.3226],\n",
       "          [-0.0804,  0.0353,  0.0608, -0.0830,  0.0483, -0.1477,  0.0629,\n",
       "           -0.0927],\n",
       "          [-0.0387, -0.0390,  0.0530, -0.1052,  0.1263,  0.1171, -0.1525,\n",
       "           -0.3175],\n",
       "          [ 0.1436, -0.0722,  0.0418, -0.3048, -0.0157, -0.1963,  0.0194,\n",
       "            0.1212],\n",
       "          [-0.0672,  0.0157,  0.0552,  0.0411,  0.0428,  0.1558,  0.0508,\n",
       "            0.1349],\n",
       "          [ 0.0362, -0.0223,  0.0463, -0.0021, -0.1072,  0.0187, -0.0817,\n",
       "           -0.2645],\n",
       "          [-0.0501,  0.0708,  0.1469,  0.0208, -0.0645,  0.0322, -0.0511,\n",
       "           -0.0438],\n",
       "          [ 0.1055, -0.0636, -0.0702,  0.1260,  0.1986,  0.0863, -0.1005,\n",
       "            0.2849],\n",
       "          [ 0.0727, -0.0328,  0.1243, -0.0623,  0.0430, -0.0847,  0.0482,\n",
       "            0.0714],\n",
       "          [-0.0159, -0.0331,  0.0335, -0.0770, -0.0708, -0.1098, -0.0823,\n",
       "           -0.2182]],\n",
       "\n",
       "         [[-0.1066,  0.0788,  0.0529,  0.1142, -0.2532, -0.0964,  0.2413,\n",
       "            0.0886],\n",
       "          [-0.1569, -0.3346,  0.0335, -0.0293, -0.2092, -0.0614, -0.0042,\n",
       "           -0.0305],\n",
       "          [-0.0250,  0.0260, -0.0152,  0.0648, -0.1413, -0.0534,  0.1389,\n",
       "            0.1227],\n",
       "          [ 0.2255, -0.3083, -0.1405, -0.1555, -0.1358,  0.0562, -0.0378,\n",
       "           -0.0199],\n",
       "          [-0.0989,  0.1612,  0.0283, -0.0523,  0.0148,  0.0022,  0.1364,\n",
       "            0.1053],\n",
       "          [-0.1797, -0.0884,  0.0587, -0.0568,  0.1237, -0.0995,  0.1032,\n",
       "            0.0690],\n",
       "          [-0.1455,  0.1750, -0.0352, -0.0436, -0.0597,  0.0513, -0.1452,\n",
       "           -0.0675],\n",
       "          [ 0.2231, -0.1802,  0.0882,  0.0384,  0.1023,  0.0461,  0.0667,\n",
       "            0.0357],\n",
       "          [ 0.1912,  0.0016, -0.0204,  0.1956,  0.0082,  0.0264,  0.0565,\n",
       "            0.0122],\n",
       "          [ 0.0648,  0.1584, -0.0571, -0.0361, -0.0308,  0.2001,  0.0449,\n",
       "           -0.0287],\n",
       "          [ 0.1422, -0.1871, -0.1182, -0.1653, -0.1429,  0.1878, -0.2605,\n",
       "            0.0228]],\n",
       "\n",
       "         [[-0.1493, -0.1135,  0.0467, -0.0374, -0.1172, -0.1857,  0.2850,\n",
       "           -0.0655],\n",
       "          [ 0.0284, -0.2083,  0.2059, -0.0029,  0.0436,  0.0036,  0.1864,\n",
       "           -0.0642],\n",
       "          [-0.0192,  0.0848,  0.0865, -0.0140,  0.0839, -0.0338,  0.1532,\n",
       "           -0.0769],\n",
       "          [ 0.2062,  0.0718,  0.0817, -0.1274, -0.1627, -0.0295,  0.2813,\n",
       "           -0.1615],\n",
       "          [ 0.0842, -0.1773,  0.2241, -0.0154,  0.1243,  0.0581, -0.1204,\n",
       "           -0.1016],\n",
       "          [-0.0877, -0.0286,  0.2767,  0.0102,  0.0705,  0.0880, -0.1452,\n",
       "           -0.2636],\n",
       "          [-0.1932, -0.0968, -0.0042,  0.1085, -0.0329, -0.0267,  0.0133,\n",
       "           -0.0006],\n",
       "          [-0.1478,  0.1153,  0.1933, -0.0670,  0.0488,  0.0467, -0.0655,\n",
       "           -0.1972],\n",
       "          [ 0.0224,  0.1133, -0.0774, -0.0880, -0.0687, -0.0291,  0.0826,\n",
       "            0.1649],\n",
       "          [-0.0059, -0.0971,  0.1952,  0.0413,  0.0413, -0.0207, -0.0091,\n",
       "           -0.2048],\n",
       "          [ 0.0274, -0.1600, -0.1487, -0.0487,  0.0491,  0.1128,  0.1749,\n",
       "            0.0820]],\n",
       "\n",
       "         [[-0.0078,  0.1440, -0.1019, -0.0810, -0.0535, -0.1130, -0.0268,\n",
       "           -0.0279],\n",
       "          [-0.1513,  0.0256, -0.0742, -0.0126,  0.0186, -0.0041, -0.2163,\n",
       "            0.1798],\n",
       "          [-0.0802, -0.1184,  0.0015, -0.0304,  0.0482, -0.0972,  0.0220,\n",
       "            0.0519],\n",
       "          [ 0.0845, -0.0240, -0.1224, -0.0481,  0.0512, -0.1258, -0.0899,\n",
       "            0.0422],\n",
       "          [ 0.0895,  0.1013,  0.2137, -0.0777,  0.0320,  0.1207, -0.0899,\n",
       "           -0.0654],\n",
       "          [ 0.0312,  0.1118,  0.2246,  0.1323, -0.0283,  0.1225, -0.0621,\n",
       "            0.0439],\n",
       "          [-0.0143, -0.0586, -0.1215,  0.0149,  0.1123, -0.0789,  0.0567,\n",
       "            0.0010],\n",
       "          [ 0.0380,  0.0292,  0.1320,  0.0152, -0.0308,  0.0864, -0.1101,\n",
       "            0.0379],\n",
       "          [-0.0485,  0.1677, -0.0879,  0.1369, -0.2540, -0.0202,  0.0193,\n",
       "            0.1196],\n",
       "          [ 0.0274,  0.1291,  0.1875,  0.0209,  0.0917,  0.0410, -0.2093,\n",
       "           -0.2830],\n",
       "          [-0.0122,  0.0614,  0.0241, -0.0041,  0.1011, -0.1029, -0.1492,\n",
       "           -0.0897]]]], grad_fn=<PermuteBackward0>), tensor([[[[ 2.3384e-02,  9.2885e-02,  1.3068e-01, -6.0840e-02, -1.5481e-03,\n",
       "           -1.3479e-01,  2.3832e-02, -3.7459e-02],\n",
       "          [-1.4214e-01,  2.4353e-02,  3.5257e-02, -5.9815e-02,  1.6627e-02,\n",
       "           -1.1131e-01,  1.0527e-01,  6.2969e-02],\n",
       "          [ 1.7400e-01,  1.8215e-01,  1.7192e-01, -1.7595e-02,  3.5042e-02,\n",
       "           -1.1548e-01, -7.5399e-03, -1.2612e-01],\n",
       "          [-1.3611e-01, -1.6011e-01,  3.1976e-02,  3.0935e-03,  1.9359e-01,\n",
       "            1.2228e-01,  8.3502e-02, -2.8117e-02],\n",
       "          [-1.8220e-03,  9.4856e-02,  1.5615e-01, -8.5434e-02,  5.9525e-02,\n",
       "           -1.4262e-01,  1.0755e-02,  2.1404e-01],\n",
       "          [-8.4769e-02,  1.0178e-01,  1.2519e-01, -8.9394e-02,  1.3628e-01,\n",
       "           -2.3231e-01,  9.1484e-02,  1.5219e-01],\n",
       "          [-9.3323e-04, -1.6557e-01, -2.3530e-04,  3.1363e-02,  9.0434e-02,\n",
       "           -1.0335e-01,  5.3325e-02, -6.5917e-02],\n",
       "          [-7.9407e-02, -7.9144e-02,  7.5024e-02, -1.4309e-01,  2.0796e-01,\n",
       "           -7.7410e-02,  1.0105e-01,  7.7585e-02],\n",
       "          [-1.4954e-01, -1.3136e-01, -1.3126e-01, -3.6914e-01,  2.2625e-03,\n",
       "           -6.0245e-02,  1.1681e-01, -1.9317e-02],\n",
       "          [ 3.7453e-02, -7.3411e-02,  2.5789e-01, -2.0085e-02,  9.5114e-02,\n",
       "           -1.8056e-01,  4.5789e-02,  2.5883e-01],\n",
       "          [-2.1208e-02, -3.6884e-02, -6.0428e-02,  1.9773e-01,  4.0376e-03,\n",
       "            1.1532e-01, -1.7716e-01, -1.5036e-01]],\n",
       "\n",
       "         [[ 5.2417e-02, -3.8332e-02,  1.1810e-02, -1.7944e-02,  5.2526e-02,\n",
       "           -2.1337e-02,  2.1738e-02,  7.6040e-02],\n",
       "          [-4.5773e-02,  3.5146e-02, -1.4416e-01,  2.6179e-02, -1.0470e-01,\n",
       "           -1.8235e-01,  1.3752e-01, -6.6689e-02],\n",
       "          [ 9.2809e-02, -1.0024e-02,  5.4300e-02,  3.0276e-02,  2.3301e-01,\n",
       "            4.2226e-02, -1.2922e-01, -5.6953e-02],\n",
       "          [ 1.7981e-03, -1.7529e-02, -1.0003e-01, -1.7569e-03,  9.4218e-03,\n",
       "           -1.4606e-01, -1.8619e-03,  4.0835e-02],\n",
       "          [-9.2895e-02,  1.5277e-02, -5.1288e-02,  1.4492e-01,  4.3240e-02,\n",
       "            5.7235e-02, -1.4045e-01,  1.2746e-01],\n",
       "          [-2.8094e-02,  8.5391e-02,  7.1290e-02,  4.9775e-02,  1.0569e-01,\n",
       "            1.0350e-01,  2.7978e-02,  3.8782e-02],\n",
       "          [ 2.5036e-01,  4.1061e-02, -1.4539e-01, -6.6588e-02,  5.6943e-03,\n",
       "            3.0978e-02,  5.5062e-02, -1.2290e-01],\n",
       "          [-1.9431e-01,  3.3272e-02, -6.0884e-02, -4.8014e-02, -1.5260e-01,\n",
       "           -1.3062e-01, -4.9695e-02,  4.4275e-02],\n",
       "          [-1.8801e-01, -5.7460e-02, -9.2782e-02,  5.0529e-03, -5.6377e-02,\n",
       "           -5.2096e-03,  1.5035e-01, -6.4247e-02],\n",
       "          [-2.0274e-02,  6.0987e-03, -1.3042e-01,  1.0019e-01,  1.3143e-02,\n",
       "           -1.7180e-03, -2.4116e-02, -1.1524e-01],\n",
       "          [ 9.0273e-02, -1.7867e-02,  3.4977e-02, -9.3038e-02,  1.4381e-01,\n",
       "            5.4287e-02, -1.8917e-01, -1.3824e-01]],\n",
       "\n",
       "         [[ 2.5329e-03, -1.6505e-01,  7.7166e-02, -1.9969e-01, -1.1121e-01,\n",
       "            8.6306e-03,  9.4753e-03, -1.7211e-01],\n",
       "          [ 7.8047e-02, -1.2871e-03,  1.6985e-01, -2.8302e-03, -1.5118e-01,\n",
       "            1.8316e-02,  2.1825e-01, -1.3595e-01],\n",
       "          [-1.7943e-01, -1.2880e-01,  2.2764e-02, -8.3387e-02, -6.7746e-02,\n",
       "            1.0388e-01, -1.2417e-01, -8.1406e-02],\n",
       "          [-5.4940e-02, -9.9823e-02,  1.0042e-01,  4.1569e-02, -1.2237e-01,\n",
       "           -2.6802e-02,  1.6353e-01,  4.6727e-04],\n",
       "          [-2.9223e-02,  1.9130e-01,  1.8711e-01,  3.3973e-02, -2.9151e-02,\n",
       "            8.3993e-02, -1.3204e-02,  2.0055e-01],\n",
       "          [ 1.0180e-01,  1.5170e-01,  2.6571e-01, -8.3612e-02, -8.3937e-02,\n",
       "            1.3777e-01,  1.2560e-01,  9.7758e-02],\n",
       "          [ 5.6542e-02, -1.3196e-01,  1.0526e-01, -4.4215e-02,  1.0845e-02,\n",
       "           -7.4554e-02,  1.5680e-01, -1.7622e-01],\n",
       "          [-9.1865e-02,  1.8179e-02,  3.8782e-02,  1.7921e-02, -6.3565e-02,\n",
       "            2.4926e-03, -9.3908e-02,  1.9779e-01],\n",
       "          [ 1.0865e-02, -2.3078e-01,  5.6473e-02,  9.4025e-02,  7.7013e-02,\n",
       "           -1.4509e-01, -1.1307e-01,  2.2329e-02],\n",
       "          [-2.7165e-02,  7.0780e-02,  4.3306e-02,  3.0932e-02, -4.7496e-02,\n",
       "           -4.4352e-02,  1.2694e-01,  1.9101e-01],\n",
       "          [-1.4383e-01,  3.4772e-02,  5.9353e-02,  1.7976e-01, -1.7058e-02,\n",
       "            2.8812e-03,  1.3908e-01,  1.5649e-02]],\n",
       "\n",
       "         [[ 8.8022e-03, -8.6258e-02, -3.1180e-03,  1.9796e-01, -1.5301e-01,\n",
       "            1.8072e-02, -7.1127e-02,  1.2356e-02],\n",
       "          [ 1.3933e-02,  7.1461e-02, -1.9109e-02, -6.8790e-02, -8.6775e-02,\n",
       "           -5.7629e-02,  5.8091e-02,  5.0743e-04],\n",
       "          [ 6.1598e-02, -9.9178e-03,  2.9462e-02,  8.7770e-02, -7.5184e-02,\n",
       "            2.4771e-01, -1.9332e-01,  2.7385e-02],\n",
       "          [-1.1379e-01,  2.6807e-02, -9.5568e-02, -9.3584e-02, -3.9451e-02,\n",
       "           -9.1629e-02,  9.6630e-02, -1.5002e-02],\n",
       "          [-8.0424e-02, -4.5056e-02,  2.3193e-02,  8.9428e-02,  2.6714e-01,\n",
       "            8.6036e-03, -7.6186e-02,  1.4770e-01],\n",
       "          [ 7.5312e-02, -1.1219e-01,  1.4954e-01, -1.0052e-01,  2.9929e-01,\n",
       "            6.1515e-02,  1.6437e-02,  4.4761e-02],\n",
       "          [ 2.8682e-02,  2.1608e-01,  5.1571e-02, -2.4578e-01, -2.4308e-01,\n",
       "           -1.4850e-01,  1.0818e-01,  2.9724e-02],\n",
       "          [ 5.0297e-02, -2.5777e-02,  6.7495e-02,  1.2934e-01,  4.8873e-02,\n",
       "           -1.9097e-02,  7.0088e-02, -1.8980e-02],\n",
       "          [ 8.7021e-02,  1.6651e-01,  8.3532e-02,  1.0434e-01,  5.8631e-02,\n",
       "           -1.6246e-01,  1.7867e-01, -3.8121e-02],\n",
       "          [-7.3261e-02, -3.3204e-02,  6.1836e-02,  1.3329e-01,  1.5628e-01,\n",
       "            1.2839e-01,  1.6348e-01,  1.7304e-02],\n",
       "          [-7.7600e-02,  8.5979e-02, -1.0299e-01, -2.9868e-01,  3.6148e-02,\n",
       "            6.7420e-02,  1.2646e-02, -1.2175e-01]]]],\n",
       "       grad_fn=<PermuteBackward0>)), (tensor([[[[ 1.2858e-01,  1.1984e-01,  6.6029e-02, -7.5276e-02, -2.7536e-02,\n",
       "           -5.4618e-02, -1.6377e-01, -2.9655e-02],\n",
       "          [-5.2732e-02,  9.2695e-02, -4.7456e-02, -1.9066e-01,  9.8209e-02,\n",
       "           -1.5055e-02, -8.2431e-03,  8.8056e-02],\n",
       "          [-1.6997e-01,  1.3423e-01,  1.1620e-01, -1.1275e-01, -3.8406e-02,\n",
       "           -1.2237e-01, -5.2265e-02,  7.9076e-02],\n",
       "          [ 5.0432e-02, -2.5281e-01,  5.1177e-02, -1.5151e-01, -8.5305e-02,\n",
       "            8.5228e-02,  2.0542e-01, -4.3989e-02],\n",
       "          [-1.7566e-01, -6.4898e-02, -1.1319e-01,  8.7899e-02, -4.3240e-02,\n",
       "           -1.4577e-01,  3.2651e-02,  1.0214e-01],\n",
       "          [-1.3524e-02, -1.6968e-01,  1.5206e-02,  3.4159e-02,  6.7501e-02,\n",
       "            1.4005e-01, -1.1168e-01,  9.7896e-02],\n",
       "          [ 4.8480e-02,  2.9866e-01,  1.0454e-01, -1.0095e-01, -1.6568e-01,\n",
       "           -3.1927e-02,  1.4285e-01, -1.3082e-01],\n",
       "          [ 1.0654e-01, -2.4379e-01, -1.5485e-01, -6.3671e-02,  8.2090e-02,\n",
       "            3.0399e-01,  9.3592e-02,  9.4891e-02],\n",
       "          [-4.1719e-02, -1.0417e-01, -1.7878e-01,  1.3156e-02, -5.9285e-02,\n",
       "            8.0192e-02,  6.8304e-02, -5.5697e-02],\n",
       "          [-6.7145e-02, -7.7859e-04, -9.5860e-02, -1.4698e-02, -7.7406e-02,\n",
       "            1.0456e-01,  3.4279e-02, -2.4992e-01],\n",
       "          [ 1.6399e-01, -1.3973e-03,  1.1845e-01, -1.4218e-01, -1.6462e-01,\n",
       "            6.1310e-02,  5.1940e-02, -2.0739e-01]],\n",
       "\n",
       "         [[-2.1601e-02, -5.2978e-02,  1.6212e-01,  1.3393e-01,  2.0923e-01,\n",
       "           -2.4222e-01, -2.8633e-01, -4.0422e-02],\n",
       "          [-2.2103e-02, -3.1270e-02,  8.2945e-02,  2.5675e-02,  7.3487e-02,\n",
       "            1.4379e-02, -2.6110e-02, -1.6034e-01],\n",
       "          [-3.2577e-02, -6.0673e-03,  5.3722e-02,  7.0543e-02, -1.7912e-02,\n",
       "           -2.9936e-02, -2.2165e-01, -1.2781e-01],\n",
       "          [ 9.8445e-02, -1.3925e-01,  2.0155e-02,  8.5008e-02,  6.3007e-02,\n",
       "            1.1519e-02,  1.2570e-02, -2.4605e-01],\n",
       "          [-2.7243e-02,  1.9778e-01, -6.5577e-02, -1.4582e-02, -4.2038e-02,\n",
       "           -2.1114e-01, -2.4028e-02, -2.3559e-02],\n",
       "          [-2.7138e-01, -1.0064e-01,  9.5027e-03, -2.7875e-04,  4.8092e-02,\n",
       "           -1.8564e-02,  7.8304e-02,  6.3346e-02],\n",
       "          [ 1.2533e-01,  3.2025e-01, -1.9407e-01,  1.4734e-01, -1.2841e-01,\n",
       "            1.5048e-01, -2.1157e-02, -2.5598e-01],\n",
       "          [ 5.9067e-02, -1.5953e-01, -6.8774e-02, -1.6862e-01, -5.4490e-02,\n",
       "           -4.0879e-02,  1.5183e-01,  7.9057e-02],\n",
       "          [ 6.4550e-02,  5.3939e-03, -1.0209e-01, -3.2740e-02, -1.3218e-02,\n",
       "            4.1155e-02,  1.1301e-01,  1.6193e-01],\n",
       "          [ 1.5557e-01, -5.1758e-02, -6.4025e-02, -3.7499e-03, -1.9197e-01,\n",
       "           -4.1311e-02,  7.8808e-02, -8.4232e-02],\n",
       "          [ 1.7008e-01, -1.9892e-01, -2.1832e-02,  8.4326e-02, -1.0553e-01,\n",
       "            3.0568e-01,  6.5787e-02, -1.7281e-01]],\n",
       "\n",
       "         [[ 6.9492e-02,  1.4528e-01,  5.0072e-02,  1.9101e-01, -2.2971e-02,\n",
       "           -2.3652e-01,  1.0256e-01,  8.7021e-02],\n",
       "          [-1.1356e-01, -5.8675e-02, -9.3490e-02,  2.4797e-01, -5.0108e-02,\n",
       "           -1.9730e-01, -8.0034e-04, -2.7417e-02],\n",
       "          [-1.6826e-01,  9.3941e-02,  1.3239e-01,  1.2527e-01, -4.0619e-03,\n",
       "           -1.5599e-01,  1.6504e-01,  1.8774e-01],\n",
       "          [-1.0731e-01,  1.1630e-01, -3.0051e-02,  1.6495e-01, -1.3451e-01,\n",
       "           -1.0371e-01,  5.9367e-02, -7.4712e-02],\n",
       "          [-4.3805e-02, -6.1534e-02,  5.3343e-02,  2.1614e-01,  1.7700e-01,\n",
       "           -4.0353e-01, -6.1143e-02,  8.3135e-03],\n",
       "          [ 1.6026e-01,  5.0113e-02,  5.7298e-02,  3.1017e-01,  3.0226e-02,\n",
       "           -6.0681e-02, -1.6259e-01,  7.2107e-03],\n",
       "          [-4.3469e-02, -2.7040e-01, -8.6763e-02, -1.0127e-01, -8.8406e-02,\n",
       "            5.0998e-02, -1.4050e-01,  6.5418e-02],\n",
       "          [-9.6488e-02,  1.8223e-02,  1.1881e-01,  1.4600e-01,  1.5983e-01,\n",
       "           -2.4330e-02,  4.6287e-02, -9.8879e-02],\n",
       "          [ 1.9580e-01,  1.3635e-01, -6.3793e-02, -2.5425e-01, -1.9534e-01,\n",
       "            1.1968e-01, -2.5650e-02,  1.2672e-01],\n",
       "          [-5.8767e-02,  7.3806e-02, -7.3501e-02,  1.3338e-01,  1.0428e-01,\n",
       "           -2.3101e-01, -2.6572e-02,  7.0615e-02],\n",
       "          [-9.4203e-02,  2.0882e-02,  1.5294e-01, -1.6537e-02,  2.2973e-02,\n",
       "            3.3658e-02, -8.1173e-03,  9.0982e-02]],\n",
       "\n",
       "         [[-9.8602e-03,  1.8170e-01,  1.7826e-02,  5.3488e-02, -9.7733e-02,\n",
       "            6.8940e-02, -1.2038e-02,  3.0401e-02],\n",
       "          [-2.7452e-01,  2.9584e-02, -7.6557e-02, -6.3323e-02, -9.8479e-02,\n",
       "            3.7125e-02,  6.7338e-02, -6.9969e-02],\n",
       "          [ 2.6955e-02, -1.1879e-01, -3.5988e-02,  1.0416e-01,  1.0672e-01,\n",
       "            5.0066e-02,  5.8730e-03, -4.9886e-02],\n",
       "          [ 4.2630e-02,  5.1829e-02, -1.0862e-01,  5.9922e-02, -6.6969e-02,\n",
       "            7.7712e-02, -6.0497e-02, -9.5925e-02],\n",
       "          [ 1.0584e-01,  1.7355e-01, -5.2074e-02,  2.4218e-02,  1.1155e-01,\n",
       "           -3.7191e-02, -5.8011e-02, -6.6720e-03],\n",
       "          [ 3.8620e-03,  1.8951e-01,  4.0889e-02, -1.2659e-01, -6.0712e-02,\n",
       "            7.0293e-04, -1.0604e-02,  8.4837e-03],\n",
       "          [ 1.6175e-01,  1.7795e-01,  1.6407e-01,  1.7422e-01,  1.0391e-01,\n",
       "            1.7621e-01, -1.2539e-01, -1.1640e-03],\n",
       "          [ 5.1932e-02, -2.3054e-01,  2.4432e-03, -2.8563e-02, -1.0906e-01,\n",
       "           -1.5839e-01, -7.0359e-02, -8.1816e-02],\n",
       "          [-6.9289e-02,  1.2784e-01,  2.5661e-01, -4.3254e-03,  1.8403e-01,\n",
       "           -1.1828e-01,  5.2361e-02, -1.5141e-01],\n",
       "          [ 2.5024e-02, -5.3251e-02,  1.0377e-01,  3.9815e-01,  1.3118e-01,\n",
       "            7.4110e-02, -1.6502e-01, -2.2230e-02],\n",
       "          [-6.6506e-02,  2.0383e-02, -1.5198e-01,  2.8661e-01, -4.1080e-02,\n",
       "            1.4475e-01, -1.1667e-01, -1.0166e-01]]]],\n",
       "       grad_fn=<PermuteBackward0>), tensor([[[[-3.5452e-02, -6.3840e-02,  5.6672e-02, -7.2753e-02,  1.6733e-01,\n",
       "            1.2111e-01, -5.2990e-02,  1.4205e-01],\n",
       "          [ 3.9098e-02,  4.7236e-02,  1.4865e-02,  8.5665e-03,  9.8619e-02,\n",
       "            1.0355e-01, -1.3229e-01,  4.5022e-02],\n",
       "          [-1.2629e-01,  2.1482e-03, -3.0415e-02, -4.4326e-02,  1.0810e-01,\n",
       "           -5.6810e-02, -1.1802e-02,  3.7142e-02],\n",
       "          [ 8.6931e-02, -1.6105e-04,  1.3789e-01, -4.5277e-02,  2.3405e-01,\n",
       "           -9.1347e-02,  4.3685e-02,  3.6180e-01],\n",
       "          [ 5.0351e-03, -1.1583e-01,  1.2419e-01,  1.9135e-02,  1.1787e-01,\n",
       "           -5.7920e-02,  4.3264e-02, -2.5683e-01],\n",
       "          [ 7.8741e-02,  1.6267e-04,  1.8484e-02,  5.8360e-02,  2.0532e-01,\n",
       "            5.2099e-02,  1.1430e-02, -8.4857e-02],\n",
       "          [-6.6120e-02, -2.4558e-02, -1.0162e-01, -6.5536e-02, -4.7477e-02,\n",
       "           -5.1094e-02, -6.9180e-02,  8.0662e-02],\n",
       "          [ 5.6638e-02,  7.8647e-02,  7.8568e-02,  8.5098e-02, -5.2378e-02,\n",
       "           -1.5315e-01,  4.2837e-02, -9.2520e-02],\n",
       "          [ 2.9874e-03, -1.4401e-01, -2.9811e-01, -9.2094e-02, -7.2185e-02,\n",
       "           -4.0290e-02,  3.1772e-02, -3.1214e-02],\n",
       "          [ 1.2638e-01, -2.2698e-01,  4.5229e-02, -1.4185e-01,  1.6085e-01,\n",
       "           -5.4947e-02,  1.0810e-02, -1.6871e-01],\n",
       "          [-8.6899e-02,  3.6934e-02,  1.9846e-01, -5.6491e-02,  1.2937e-01,\n",
       "           -1.1986e-01,  6.9518e-02,  2.0664e-01]],\n",
       "\n",
       "         [[-5.1632e-03, -8.1571e-02, -6.4701e-02,  4.7694e-02, -1.4425e-01,\n",
       "           -2.8585e-02,  1.6814e-01,  1.8678e-01],\n",
       "          [-1.2438e-02, -1.2520e-02,  8.6765e-02,  6.0278e-03, -1.4918e-01,\n",
       "            4.0464e-03,  8.7573e-02, -6.2574e-02],\n",
       "          [ 6.1409e-02, -4.9798e-02,  2.1368e-01, -1.0160e-01, -1.7225e-01,\n",
       "           -5.7113e-02,  6.3746e-02,  1.8745e-01],\n",
       "          [ 4.7860e-02, -8.4755e-03,  1.0519e-01,  3.7161e-02, -2.0188e-01,\n",
       "            1.7996e-03, -1.1970e-01, -8.8566e-02],\n",
       "          [-2.8573e-02, -3.7878e-02, -2.9129e-02,  1.1077e-01,  1.4531e-01,\n",
       "           -1.2043e-01, -5.4396e-02, -3.4429e-02],\n",
       "          [-5.0770e-02, -7.7860e-03,  2.8281e-01,  1.9501e-01, -3.9411e-02,\n",
       "           -2.5497e-01, -1.4273e-01, -1.2240e-01],\n",
       "          [ 7.1816e-02, -3.0740e-02,  1.5741e-01,  1.2722e-01, -1.6716e-01,\n",
       "           -1.8307e-02,  1.4012e-01, -2.2484e-02],\n",
       "          [-6.5426e-02, -9.7876e-02,  9.9526e-03,  6.1557e-02,  5.6201e-02,\n",
       "           -3.5371e-02, -3.2048e-01, -1.7031e-01],\n",
       "          [-7.0304e-02, -2.6407e-02,  5.9823e-02,  2.2310e-01,  1.7354e-01,\n",
       "            2.4387e-01,  1.2685e-01, -6.0056e-02],\n",
       "          [ 9.5331e-02,  9.6428e-03,  1.4051e-01,  2.9384e-01, -9.2060e-02,\n",
       "           -9.3743e-02, -2.3395e-01,  1.3737e-02],\n",
       "          [ 2.4964e-01, -1.9468e-02,  5.6112e-02,  7.6484e-02, -2.1504e-01,\n",
       "           -3.0786e-01, -9.7997e-02,  4.7186e-02]],\n",
       "\n",
       "         [[-4.9422e-02,  4.7917e-02,  2.7651e-01,  1.6269e-01,  1.6113e-01,\n",
       "            2.9664e-02, -9.0836e-02,  1.3666e-01],\n",
       "          [ 4.9850e-02, -1.1401e-01,  1.0357e-01, -1.5447e-02,  1.3686e-01,\n",
       "           -2.6969e-03,  2.8626e-02, -8.9857e-02],\n",
       "          [-5.3851e-02,  2.1767e-02,  1.7689e-01,  1.2114e-01,  8.9640e-02,\n",
       "           -5.2108e-02, -2.0406e-01,  1.6324e-01],\n",
       "          [ 9.9097e-02, -5.3795e-02,  7.7166e-02,  3.4629e-02, -8.3227e-02,\n",
       "            1.2264e-01,  1.3715e-02,  2.3473e-04],\n",
       "          [-6.4916e-02, -2.0872e-01,  3.8787e-03,  2.0842e-04,  4.8013e-02,\n",
       "            1.7901e-01,  1.9114e-01, -9.5437e-02],\n",
       "          [-3.3216e-02, -1.0760e-01, -6.2724e-03, -1.7340e-01,  2.5901e-01,\n",
       "            8.7884e-03,  1.0523e-01, -2.4907e-01],\n",
       "          [ 1.5155e-01,  2.1714e-01, -2.7464e-02, -5.1933e-03, -2.3723e-01,\n",
       "            2.7501e-01, -2.0751e-01,  1.1427e-01],\n",
       "          [-1.4871e-03, -1.5210e-01, -6.9549e-02, -4.8385e-02,  1.6326e-01,\n",
       "           -7.0812e-02,  3.5802e-01, -2.0996e-01],\n",
       "          [ 1.9610e-01,  3.3402e-02, -6.7770e-02,  5.8869e-02,  2.7682e-02,\n",
       "            5.0124e-02,  3.1182e-02, -2.1388e-01],\n",
       "          [ 1.1534e-01, -1.7669e-01,  5.2504e-02, -6.4837e-02, -1.1535e-01,\n",
       "            3.7504e-01,  4.3276e-02, -2.4928e-02],\n",
       "          [-1.1347e-02, -5.4120e-03, -1.5766e-02, -1.0424e-01, -2.5390e-01,\n",
       "            2.3573e-01, -1.3775e-01,  7.7676e-02]],\n",
       "\n",
       "         [[ 6.6630e-02, -7.5008e-02, -2.1022e-01, -1.3957e-01,  2.9735e-02,\n",
       "            1.6495e-01,  1.5988e-02,  2.7726e-02],\n",
       "          [ 5.0526e-02, -1.4892e-01, -2.6768e-01, -6.0828e-02,  8.6699e-02,\n",
       "            3.7392e-02,  1.1173e-01,  1.9125e-02],\n",
       "          [-5.2845e-02, -8.1246e-02,  3.6418e-02, -8.8897e-02, -3.3754e-02,\n",
       "            1.8378e-01, -1.3392e-01, -4.3666e-02],\n",
       "          [-8.6305e-02, -2.3519e-01, -3.4326e-01,  1.4170e-01,  1.3125e-01,\n",
       "            1.0956e-01, -4.0580e-02,  3.2982e-02],\n",
       "          [ 2.9662e-02,  2.3126e-01,  6.9330e-02, -1.4391e-02, -8.5452e-02,\n",
       "            9.2602e-02,  3.2784e-02, -7.8550e-02],\n",
       "          [-1.2386e-01, -1.2204e-01,  1.9000e-01, -2.1597e-01, -1.6595e-01,\n",
       "            1.2766e-01,  1.9888e-01, -8.9324e-02],\n",
       "          [ 6.2624e-02, -2.3271e-01, -1.5368e-01,  3.4824e-02,  1.2490e-01,\n",
       "           -4.8806e-02,  4.1712e-02, -4.0323e-02],\n",
       "          [-1.8245e-02,  9.8453e-02,  2.5691e-02,  4.5112e-02, -6.5638e-02,\n",
       "           -8.3949e-02,  1.3276e-01,  4.2020e-02],\n",
       "          [ 1.9348e-01,  2.6972e-02,  8.6487e-02, -1.4176e-01,  7.7233e-02,\n",
       "           -2.0010e-02,  1.3143e-01,  2.1659e-02],\n",
       "          [ 1.0938e-01,  8.1011e-02,  4.9193e-02, -1.2657e-01, -1.0454e-01,\n",
       "           -5.0619e-02,  1.9542e-01, -5.9556e-02],\n",
       "          [-4.4117e-02, -2.0269e-01, -6.3551e-03,  1.0668e-01,  1.4880e-01,\n",
       "            7.9268e-02, -1.1834e-01,  6.5369e-02]]]],\n",
       "       grad_fn=<PermuteBackward0>))), hidden_states=None, attentions=(tensor([[[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000],\n",
       "          [0.5052, 0.4948, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000],\n",
       "          [0.3291, 0.3354, 0.3355, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000],\n",
       "          [0.2503, 0.2484, 0.2514, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000],\n",
       "          [0.2001, 0.1942, 0.2061, 0.1956, 0.2041, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000],\n",
       "          [0.1654, 0.1677, 0.1673, 0.1686, 0.1664, 0.1646, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000],\n",
       "          [0.1424, 0.1439, 0.1421, 0.1424, 0.1426, 0.1426, 0.1440, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000],\n",
       "          [0.1258, 0.1243, 0.1296, 0.1213, 0.1253, 0.1263, 0.1232, 0.1242,\n",
       "           0.0000, 0.0000, 0.0000],\n",
       "          [0.1095, 0.1128, 0.1073, 0.1136, 0.1110, 0.1104, 0.1113, 0.1134,\n",
       "           0.1107, 0.0000, 0.0000],\n",
       "          [0.1017, 0.0974, 0.1024, 0.0986, 0.1016, 0.1037, 0.0983, 0.0981,\n",
       "           0.0941, 0.1041, 0.0000],\n",
       "          [0.0906, 0.0925, 0.0891, 0.0919, 0.0916, 0.0918, 0.0898, 0.0924,\n",
       "           0.0886, 0.0906, 0.0912]],\n",
       "\n",
       "         [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000],\n",
       "          [0.4967, 0.5033, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000],\n",
       "          [0.3282, 0.3375, 0.3343, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000],\n",
       "          [0.2495, 0.2526, 0.2481, 0.2498, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000],\n",
       "          [0.2022, 0.1970, 0.1996, 0.2007, 0.2005, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000],\n",
       "          [0.1690, 0.1643, 0.1670, 0.1667, 0.1637, 0.1693, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000],\n",
       "          [0.1422, 0.1436, 0.1439, 0.1425, 0.1422, 0.1421, 0.1434, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000],\n",
       "          [0.1252, 0.1264, 0.1245, 0.1243, 0.1239, 0.1273, 0.1216, 0.1267,\n",
       "           0.0000, 0.0000, 0.0000],\n",
       "          [0.1130, 0.1095, 0.1125, 0.1107, 0.1114, 0.1116, 0.1102, 0.1111,\n",
       "           0.1102, 0.0000, 0.0000],\n",
       "          [0.1004, 0.0994, 0.0992, 0.0996, 0.0991, 0.1018, 0.0974, 0.1021,\n",
       "           0.1008, 0.1003, 0.0000],\n",
       "          [0.0892, 0.0930, 0.0890, 0.0904, 0.0900, 0.0899, 0.0939, 0.0892,\n",
       "           0.0928, 0.0911, 0.0915]],\n",
       "\n",
       "         [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000],\n",
       "          [0.4998, 0.5002, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000],\n",
       "          [0.3325, 0.3342, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000],\n",
       "          [0.2515, 0.2502, 0.2510, 0.2474, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000],\n",
       "          [0.2008, 0.1996, 0.2003, 0.2001, 0.1992, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000],\n",
       "          [0.1664, 0.1653, 0.1676, 0.1660, 0.1681, 0.1666, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000],\n",
       "          [0.1436, 0.1431, 0.1426, 0.1406, 0.1429, 0.1460, 0.1413, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000],\n",
       "          [0.1257, 0.1258, 0.1255, 0.1234, 0.1241, 0.1304, 0.1248, 0.1203,\n",
       "           0.0000, 0.0000, 0.0000],\n",
       "          [0.1114, 0.1116, 0.1119, 0.1097, 0.1108, 0.1132, 0.1136, 0.1074,\n",
       "           0.1103, 0.0000, 0.0000],\n",
       "          [0.1004, 0.1014, 0.1001, 0.0990, 0.0978, 0.1004, 0.0980, 0.1007,\n",
       "           0.0989, 0.1033, 0.0000],\n",
       "          [0.0914, 0.0915, 0.0910, 0.0900, 0.0907, 0.0913, 0.0901, 0.0913,\n",
       "           0.0903, 0.0922, 0.0902]],\n",
       "\n",
       "         [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000],\n",
       "          [0.5008, 0.4992, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000],\n",
       "          [0.3454, 0.3315, 0.3231, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000],\n",
       "          [0.2512, 0.2503, 0.2482, 0.2503, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000],\n",
       "          [0.1974, 0.2013, 0.1997, 0.1989, 0.2027, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000],\n",
       "          [0.1625, 0.1695, 0.1666, 0.1655, 0.1707, 0.1652, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000],\n",
       "          [0.1457, 0.1415, 0.1430, 0.1410, 0.1464, 0.1406, 0.1417, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000],\n",
       "          [0.1220, 0.1260, 0.1292, 0.1236, 0.1268, 0.1207, 0.1251, 0.1265,\n",
       "           0.0000, 0.0000, 0.0000],\n",
       "          [0.1098, 0.1106, 0.1169, 0.1115, 0.1141, 0.1059, 0.1111, 0.1115,\n",
       "           0.1087, 0.0000, 0.0000],\n",
       "          [0.0994, 0.1005, 0.0996, 0.0991, 0.1018, 0.1014, 0.0991, 0.0989,\n",
       "           0.0986, 0.1016, 0.0000],\n",
       "          [0.0913, 0.0917, 0.0896, 0.0911, 0.0907, 0.0908, 0.0907, 0.0914,\n",
       "           0.0917, 0.0910, 0.0899]]]], grad_fn=<SoftmaxBackward0>), tensor([[[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000],\n",
       "          [0.5096, 0.4904, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000],\n",
       "          [0.3329, 0.3347, 0.3324, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000],\n",
       "          [0.2527, 0.2494, 0.2441, 0.2538, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000],\n",
       "          [0.1984, 0.2010, 0.1994, 0.2000, 0.2011, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000],\n",
       "          [0.1654, 0.1667, 0.1683, 0.1669, 0.1665, 0.1662, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000],\n",
       "          [0.1431, 0.1417, 0.1453, 0.1423, 0.1437, 0.1430, 0.1408, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000],\n",
       "          [0.1270, 0.1250, 0.1232, 0.1264, 0.1248, 0.1251, 0.1267, 0.1218,\n",
       "           0.0000, 0.0000, 0.0000],\n",
       "          [0.1113, 0.1108, 0.1127, 0.1112, 0.1125, 0.1108, 0.1113, 0.1100,\n",
       "           0.1094, 0.0000, 0.0000],\n",
       "          [0.1005, 0.1002, 0.0985, 0.1016, 0.0999, 0.1001, 0.1008, 0.0988,\n",
       "           0.0988, 0.1009, 0.0000],\n",
       "          [0.0884, 0.0914, 0.0895, 0.0911, 0.0915, 0.0904, 0.0926, 0.0930,\n",
       "           0.0912, 0.0913, 0.0898]],\n",
       "\n",
       "         [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000],\n",
       "          [0.5004, 0.4996, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000],\n",
       "          [0.3285, 0.3341, 0.3374, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000],\n",
       "          [0.2524, 0.2475, 0.2500, 0.2501, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000],\n",
       "          [0.2019, 0.2044, 0.1987, 0.1959, 0.1990, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000],\n",
       "          [0.1672, 0.1674, 0.1672, 0.1673, 0.1648, 0.1660, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000],\n",
       "          [0.1423, 0.1417, 0.1432, 0.1428, 0.1422, 0.1443, 0.1436, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000],\n",
       "          [0.1250, 0.1233, 0.1267, 0.1256, 0.1255, 0.1251, 0.1256, 0.1233,\n",
       "           0.0000, 0.0000, 0.0000],\n",
       "          [0.1108, 0.1117, 0.1094, 0.1131, 0.1108, 0.1101, 0.1118, 0.1117,\n",
       "           0.1105, 0.0000, 0.0000],\n",
       "          [0.1007, 0.0997, 0.1004, 0.0987, 0.1003, 0.0996, 0.0984, 0.1008,\n",
       "           0.0995, 0.1018, 0.0000],\n",
       "          [0.0915, 0.0899, 0.0918, 0.0907, 0.0899, 0.0912, 0.0901, 0.0906,\n",
       "           0.0906, 0.0910, 0.0928]],\n",
       "\n",
       "         [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000],\n",
       "          [0.5032, 0.4968, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000],\n",
       "          [0.3327, 0.3292, 0.3381, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000],\n",
       "          [0.2548, 0.2498, 0.2481, 0.2473, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000],\n",
       "          [0.2008, 0.1981, 0.2001, 0.2004, 0.2007, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000],\n",
       "          [0.1648, 0.1687, 0.1671, 0.1663, 0.1674, 0.1659, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000],\n",
       "          [0.1393, 0.1431, 0.1446, 0.1430, 0.1440, 0.1427, 0.1434, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000],\n",
       "          [0.1252, 0.1264, 0.1214, 0.1247, 0.1271, 0.1243, 0.1253, 0.1256,\n",
       "           0.0000, 0.0000, 0.0000],\n",
       "          [0.1147, 0.1108, 0.1087, 0.1104, 0.1112, 0.1098, 0.1112, 0.1122,\n",
       "           0.1111, 0.0000, 0.0000],\n",
       "          [0.0998, 0.1001, 0.1004, 0.1003, 0.1007, 0.0998, 0.1002, 0.0991,\n",
       "           0.0985, 0.1012, 0.0000],\n",
       "          [0.0906, 0.0917, 0.0914, 0.0916, 0.0927, 0.0902, 0.0913, 0.0898,\n",
       "           0.0880, 0.0928, 0.0897]],\n",
       "\n",
       "         [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000],\n",
       "          [0.4982, 0.5018, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000],\n",
       "          [0.3277, 0.3359, 0.3364, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000],\n",
       "          [0.2519, 0.2521, 0.2446, 0.2514, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000],\n",
       "          [0.2016, 0.1997, 0.1993, 0.1997, 0.1997, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000],\n",
       "          [0.1700, 0.1665, 0.1662, 0.1668, 0.1662, 0.1642, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000],\n",
       "          [0.1412, 0.1437, 0.1408, 0.1434, 0.1431, 0.1443, 0.1435, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000],\n",
       "          [0.1288, 0.1250, 0.1263, 0.1247, 0.1262, 0.1221, 0.1241, 0.1228,\n",
       "           0.0000, 0.0000, 0.0000],\n",
       "          [0.1121, 0.1116, 0.1116, 0.1103, 0.1122, 0.1097, 0.1111, 0.1096,\n",
       "           0.1118, 0.0000, 0.0000],\n",
       "          [0.1011, 0.0989, 0.1001, 0.0996, 0.1001, 0.0995, 0.1007, 0.0993,\n",
       "           0.0996, 0.1011, 0.0000],\n",
       "          [0.0913, 0.0909, 0.0910, 0.0911, 0.0906, 0.0906, 0.0918, 0.0913,\n",
       "           0.0907, 0.0911, 0.0895]]]], grad_fn=<SoftmaxBackward0>), tensor([[[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000],\n",
       "          [0.5045, 0.4955, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000],\n",
       "          [0.3345, 0.3283, 0.3372, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000],\n",
       "          [0.2518, 0.2490, 0.2514, 0.2478, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000],\n",
       "          [0.1995, 0.2002, 0.2000, 0.1994, 0.2008, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000],\n",
       "          [0.1680, 0.1674, 0.1657, 0.1680, 0.1645, 0.1664, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000],\n",
       "          [0.1468, 0.1430, 0.1461, 0.1413, 0.1407, 0.1431, 0.1391, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000],\n",
       "          [0.1247, 0.1248, 0.1236, 0.1279, 0.1231, 0.1255, 0.1249, 0.1255,\n",
       "           0.0000, 0.0000, 0.0000],\n",
       "          [0.1106, 0.1109, 0.1109, 0.1121, 0.1100, 0.1108, 0.1122, 0.1106,\n",
       "           0.1117, 0.0000, 0.0000],\n",
       "          [0.1000, 0.0997, 0.1013, 0.1009, 0.1006, 0.0992, 0.0991, 0.0997,\n",
       "           0.0992, 0.1004, 0.0000],\n",
       "          [0.0896, 0.0890, 0.0918, 0.0892, 0.0932, 0.0911, 0.0904, 0.0901,\n",
       "           0.0912, 0.0922, 0.0920]],\n",
       "\n",
       "         [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000],\n",
       "          [0.4984, 0.5016, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000],\n",
       "          [0.3337, 0.3346, 0.3317, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000],\n",
       "          [0.2491, 0.2517, 0.2511, 0.2481, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000],\n",
       "          [0.2013, 0.2024, 0.1986, 0.1969, 0.2007, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000],\n",
       "          [0.1662, 0.1639, 0.1658, 0.1672, 0.1668, 0.1701, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000],\n",
       "          [0.1467, 0.1422, 0.1416, 0.1395, 0.1428, 0.1418, 0.1453, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000],\n",
       "          [0.1220, 0.1247, 0.1256, 0.1256, 0.1258, 0.1266, 0.1248, 0.1249,\n",
       "           0.0000, 0.0000, 0.0000],\n",
       "          [0.1137, 0.1110, 0.1104, 0.1104, 0.1101, 0.1104, 0.1119, 0.1110,\n",
       "           0.1110, 0.0000, 0.0000],\n",
       "          [0.1007, 0.1015, 0.1005, 0.0998, 0.0995, 0.1005, 0.0998, 0.1000,\n",
       "           0.0974, 0.1004, 0.0000],\n",
       "          [0.0912, 0.0912, 0.0906, 0.0908, 0.0909, 0.0904, 0.0911, 0.0911,\n",
       "           0.0918, 0.0912, 0.0896]],\n",
       "\n",
       "         [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000],\n",
       "          [0.5030, 0.4970, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000],\n",
       "          [0.3340, 0.3380, 0.3280, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000],\n",
       "          [0.2523, 0.2450, 0.2524, 0.2503, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000],\n",
       "          [0.1979, 0.2014, 0.1967, 0.2019, 0.2021, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000],\n",
       "          [0.1667, 0.1658, 0.1684, 0.1656, 0.1661, 0.1674, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000],\n",
       "          [0.1434, 0.1430, 0.1469, 0.1394, 0.1403, 0.1425, 0.1445, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000],\n",
       "          [0.1256, 0.1248, 0.1248, 0.1247, 0.1249, 0.1255, 0.1236, 0.1260,\n",
       "           0.0000, 0.0000, 0.0000],\n",
       "          [0.1117, 0.1103, 0.1103, 0.1111, 0.1110, 0.1111, 0.1120, 0.1117,\n",
       "           0.1109, 0.0000, 0.0000],\n",
       "          [0.0994, 0.0996, 0.1005, 0.1006, 0.1015, 0.1009, 0.1008, 0.0997,\n",
       "           0.0993, 0.0976, 0.0000],\n",
       "          [0.0903, 0.0928, 0.0927, 0.0905, 0.0908, 0.0907, 0.0913, 0.0903,\n",
       "           0.0904, 0.0897, 0.0904]],\n",
       "\n",
       "         [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000],\n",
       "          [0.4978, 0.5022, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000],\n",
       "          [0.3332, 0.3332, 0.3336, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000],\n",
       "          [0.2489, 0.2461, 0.2497, 0.2553, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000],\n",
       "          [0.1988, 0.2017, 0.1982, 0.1990, 0.2023, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000],\n",
       "          [0.1665, 0.1687, 0.1656, 0.1647, 0.1651, 0.1693, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000],\n",
       "          [0.1437, 0.1432, 0.1439, 0.1436, 0.1421, 0.1411, 0.1424, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000],\n",
       "          [0.1243, 0.1251, 0.1227, 0.1234, 0.1264, 0.1278, 0.1252, 0.1251,\n",
       "           0.0000, 0.0000, 0.0000],\n",
       "          [0.1117, 0.1098, 0.1111, 0.1137, 0.1116, 0.1114, 0.1104, 0.1095,\n",
       "           0.1108, 0.0000, 0.0000],\n",
       "          [0.0995, 0.1014, 0.0982, 0.0993, 0.1030, 0.0995, 0.0974, 0.1005,\n",
       "           0.1021, 0.0991, 0.0000],\n",
       "          [0.0904, 0.0915, 0.0912, 0.0905, 0.0911, 0.0900, 0.0907, 0.0921,\n",
       "           0.0907, 0.0909, 0.0908]]]], grad_fn=<SoftmaxBackward0>), tensor([[[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000],\n",
       "          [0.4980, 0.5020, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000],\n",
       "          [0.3322, 0.3329, 0.3350, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000],\n",
       "          [0.2475, 0.2523, 0.2481, 0.2521, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000],\n",
       "          [0.1997, 0.1994, 0.2002, 0.2013, 0.1995, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000],\n",
       "          [0.1676, 0.1689, 0.1640, 0.1695, 0.1635, 0.1666, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000],\n",
       "          [0.1427, 0.1420, 0.1445, 0.1412, 0.1436, 0.1425, 0.1434, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000],\n",
       "          [0.1241, 0.1293, 0.1216, 0.1283, 0.1201, 0.1257, 0.1260, 0.1249,\n",
       "           0.0000, 0.0000, 0.0000],\n",
       "          [0.1098, 0.1101, 0.1110, 0.1085, 0.1132, 0.1124, 0.1113, 0.1117,\n",
       "           0.1122, 0.0000, 0.0000],\n",
       "          [0.0986, 0.0997, 0.0989, 0.1002, 0.0988, 0.1008, 0.1005, 0.1011,\n",
       "           0.1019, 0.0996, 0.0000],\n",
       "          [0.0902, 0.0893, 0.0904, 0.0893, 0.0930, 0.0900, 0.0917, 0.0908,\n",
       "           0.0913, 0.0923, 0.0916]],\n",
       "\n",
       "         [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000],\n",
       "          [0.5007, 0.4993, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000],\n",
       "          [0.3291, 0.3380, 0.3329, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000],\n",
       "          [0.2498, 0.2515, 0.2492, 0.2496, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000],\n",
       "          [0.2023, 0.1996, 0.2007, 0.2001, 0.1973, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000],\n",
       "          [0.1680, 0.1680, 0.1661, 0.1674, 0.1645, 0.1661, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000],\n",
       "          [0.1401, 0.1436, 0.1412, 0.1415, 0.1433, 0.1450, 0.1454, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000],\n",
       "          [0.1308, 0.1246, 0.1273, 0.1222, 0.1256, 0.1233, 0.1236, 0.1226,\n",
       "           0.0000, 0.0000, 0.0000],\n",
       "          [0.1092, 0.1105, 0.1099, 0.1114, 0.1108, 0.1122, 0.1113, 0.1128,\n",
       "           0.1120, 0.0000, 0.0000],\n",
       "          [0.1016, 0.0992, 0.1009, 0.0996, 0.1001, 0.0989, 0.0992, 0.0996,\n",
       "           0.1007, 0.1002, 0.0000],\n",
       "          [0.0915, 0.0910, 0.0909, 0.0888, 0.0917, 0.0918, 0.0922, 0.0904,\n",
       "           0.0908, 0.0911, 0.0897]],\n",
       "\n",
       "         [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000],\n",
       "          [0.5001, 0.4999, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000],\n",
       "          [0.3311, 0.3367, 0.3322, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000],\n",
       "          [0.2545, 0.2504, 0.2484, 0.2467, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000],\n",
       "          [0.1971, 0.1984, 0.2022, 0.2012, 0.2012, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000],\n",
       "          [0.1636, 0.1689, 0.1668, 0.1633, 0.1700, 0.1674, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000],\n",
       "          [0.1447, 0.1417, 0.1440, 0.1439, 0.1405, 0.1417, 0.1435, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000],\n",
       "          [0.1241, 0.1244, 0.1262, 0.1266, 0.1251, 0.1243, 0.1236, 0.1256,\n",
       "           0.0000, 0.0000, 0.0000],\n",
       "          [0.1131, 0.1112, 0.1115, 0.1127, 0.1088, 0.1085, 0.1115, 0.1098,\n",
       "           0.1128, 0.0000, 0.0000],\n",
       "          [0.0992, 0.0997, 0.1000, 0.1012, 0.1005, 0.1011, 0.0985, 0.1006,\n",
       "           0.0984, 0.1009, 0.0000],\n",
       "          [0.0903, 0.0905, 0.0897, 0.0916, 0.0919, 0.0916, 0.0905, 0.0910,\n",
       "           0.0911, 0.0912, 0.0906]],\n",
       "\n",
       "         [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000],\n",
       "          [0.5025, 0.4975, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000],\n",
       "          [0.3358, 0.3318, 0.3324, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000],\n",
       "          [0.2513, 0.2533, 0.2486, 0.2468, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000],\n",
       "          [0.2000, 0.2021, 0.1977, 0.2000, 0.2002, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000],\n",
       "          [0.1667, 0.1659, 0.1645, 0.1649, 0.1685, 0.1695, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000],\n",
       "          [0.1438, 0.1406, 0.1430, 0.1441, 0.1427, 0.1416, 0.1443, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000],\n",
       "          [0.1251, 0.1265, 0.1236, 0.1248, 0.1253, 0.1253, 0.1239, 0.1255,\n",
       "           0.0000, 0.0000, 0.0000],\n",
       "          [0.1099, 0.1099, 0.1117, 0.1124, 0.1115, 0.1115, 0.1123, 0.1115,\n",
       "           0.1094, 0.0000, 0.0000],\n",
       "          [0.0995, 0.1006, 0.1000, 0.1011, 0.1002, 0.1003, 0.0999, 0.1005,\n",
       "           0.0996, 0.0983, 0.0000],\n",
       "          [0.0911, 0.0931, 0.0926, 0.0916, 0.0899, 0.0897, 0.0908, 0.0906,\n",
       "           0.0902, 0.0891, 0.0913]]]], grad_fn=<SoftmaxBackward0>), tensor([[[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000],\n",
       "          [0.4997, 0.5003, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000],\n",
       "          [0.3366, 0.3329, 0.3305, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000],\n",
       "          [0.2466, 0.2508, 0.2502, 0.2524, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000],\n",
       "          [0.2010, 0.2011, 0.2002, 0.1986, 0.1991, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000],\n",
       "          [0.1652, 0.1683, 0.1677, 0.1649, 0.1673, 0.1666, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000],\n",
       "          [0.1436, 0.1423, 0.1434, 0.1414, 0.1430, 0.1418, 0.1444, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000],\n",
       "          [0.1224, 0.1243, 0.1235, 0.1280, 0.1243, 0.1244, 0.1251, 0.1280,\n",
       "           0.0000, 0.0000, 0.0000],\n",
       "          [0.1113, 0.1114, 0.1106, 0.1117, 0.1093, 0.1117, 0.1116, 0.1125,\n",
       "           0.1098, 0.0000, 0.0000],\n",
       "          [0.1010, 0.0998, 0.1004, 0.0997, 0.1002, 0.1002, 0.1007, 0.0992,\n",
       "           0.0994, 0.0994, 0.0000],\n",
       "          [0.0915, 0.0893, 0.0896, 0.0915, 0.0915, 0.0895, 0.0922, 0.0897,\n",
       "           0.0914, 0.0914, 0.0925]],\n",
       "\n",
       "         [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000],\n",
       "          [0.5001, 0.4999, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000],\n",
       "          [0.3337, 0.3324, 0.3339, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000],\n",
       "          [0.2566, 0.2475, 0.2498, 0.2461, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000],\n",
       "          [0.1954, 0.2017, 0.2016, 0.2041, 0.1972, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000],\n",
       "          [0.1655, 0.1674, 0.1665, 0.1669, 0.1674, 0.1663, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000],\n",
       "          [0.1470, 0.1420, 0.1426, 0.1414, 0.1456, 0.1425, 0.1390, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000],\n",
       "          [0.1244, 0.1253, 0.1258, 0.1255, 0.1235, 0.1244, 0.1252, 0.1259,\n",
       "           0.0000, 0.0000, 0.0000],\n",
       "          [0.1069, 0.1113, 0.1107, 0.1115, 0.1100, 0.1114, 0.1121, 0.1137,\n",
       "           0.1123, 0.0000, 0.0000],\n",
       "          [0.1009, 0.0998, 0.0999, 0.0995, 0.1006, 0.1010, 0.0989, 0.1000,\n",
       "           0.1000, 0.0994, 0.0000],\n",
       "          [0.0964, 0.0907, 0.0925, 0.0901, 0.0932, 0.0921, 0.0891, 0.0893,\n",
       "           0.0902, 0.0897, 0.0866]],\n",
       "\n",
       "         [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000],\n",
       "          [0.5057, 0.4943, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000],\n",
       "          [0.3342, 0.3349, 0.3309, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000],\n",
       "          [0.2521, 0.2463, 0.2508, 0.2507, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000],\n",
       "          [0.1993, 0.1995, 0.2009, 0.1984, 0.2019, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000],\n",
       "          [0.1679, 0.1658, 0.1660, 0.1666, 0.1662, 0.1674, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000],\n",
       "          [0.1424, 0.1445, 0.1437, 0.1456, 0.1410, 0.1418, 0.1410, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000],\n",
       "          [0.1257, 0.1228, 0.1244, 0.1254, 0.1234, 0.1253, 0.1268, 0.1262,\n",
       "           0.0000, 0.0000, 0.0000],\n",
       "          [0.1122, 0.1117, 0.1126, 0.1116, 0.1114, 0.1106, 0.1091, 0.1124,\n",
       "           0.1085, 0.0000, 0.0000],\n",
       "          [0.1006, 0.0998, 0.1010, 0.1005, 0.0992, 0.0980, 0.1010, 0.0986,\n",
       "           0.1021, 0.0991, 0.0000],\n",
       "          [0.0910, 0.0904, 0.0891, 0.0921, 0.0882, 0.0919, 0.0915, 0.0891,\n",
       "           0.0958, 0.0904, 0.0904]],\n",
       "\n",
       "         [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000],\n",
       "          [0.4997, 0.5003, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000],\n",
       "          [0.3335, 0.3352, 0.3313, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000],\n",
       "          [0.2493, 0.2510, 0.2486, 0.2510, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000],\n",
       "          [0.2009, 0.1983, 0.1992, 0.2005, 0.2010, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000],\n",
       "          [0.1672, 0.1667, 0.1646, 0.1659, 0.1674, 0.1682, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000],\n",
       "          [0.1425, 0.1406, 0.1426, 0.1428, 0.1436, 0.1432, 0.1447, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000],\n",
       "          [0.1252, 0.1257, 0.1251, 0.1254, 0.1233, 0.1247, 0.1248, 0.1259,\n",
       "           0.0000, 0.0000, 0.0000],\n",
       "          [0.1115, 0.1133, 0.1115, 0.1134, 0.1096, 0.1102, 0.1097, 0.1117,\n",
       "           0.1091, 0.0000, 0.0000],\n",
       "          [0.1010, 0.1003, 0.0990, 0.1013, 0.1002, 0.1007, 0.1003, 0.0997,\n",
       "           0.0981, 0.0992, 0.0000],\n",
       "          [0.0913, 0.0911, 0.0904, 0.0910, 0.0912, 0.0902, 0.0906, 0.0897,\n",
       "           0.0894, 0.0922, 0.0929]]]], grad_fn=<SoftmaxBackward0>)))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = model(**inputs, output_attentions=True)\n",
    "# output_attentions=True is required, otherwise that field is not visible\n",
    "\n",
    "# using output_hidden_states=True we can also get the \n",
    "# Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-03T10:25:11.093371Z",
     "start_time": "2022-07-03T10:25:11.089925Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The attention field contains 5 items\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 11, 11])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_weights = outputs[\"attentions\"]\n",
    "# it returns a matrix n_tokens x n_tokens (because it is self-attention) x n_head\n",
    "# for each layer in the model\n",
    "\n",
    "# check model parameters (e.g. n_head = 4): \n",
    "# https://huggingface.co/hf-internal-testing/tiny-random-gptj/blob/main/config.json\n",
    "\n",
    "print(f\"The attention field contains {len(attention_weights)} items\")  # 5 items, one for each layer\n",
    "first_layer = attention_weights[0]\n",
    "first_layer.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-03T10:26:02.455903Z",
     "start_time": "2022-07-03T10:26:02.450819Z"
    }
   },
   "outputs": [],
   "source": [
    "# ONLY FOR GPTJModel\n",
    "#last_hidden_states = outputs.last_hidden_state\n",
    "#last_hidden_states  # Sequence of hidden-states at the output of the last layer of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try to generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-03T10:26:06.218373Z",
     "start_time": "2022-07-03T10:26:05.967245Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello, my dog is cute AfterJ����������������������������llHqere :keits wair'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "greedy_output = model.generate(input_ids, max_length=50)\n",
    "tokenizer.decode(greedy_output[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": "20",
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
